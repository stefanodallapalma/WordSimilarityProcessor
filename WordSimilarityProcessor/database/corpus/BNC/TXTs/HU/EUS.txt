Introduction
The Classic Von Neumann Computer
THE TYPICAL programmer sees a modern computer in terms of one or more high-level languages, together with a command language which he must use to communicate with the operating system.
His only contact with its hardware may be with an interactive terminal (if he is lucky), or with a series of error messages couched in terms of the internal structure of the computer (if he is unlucky).
Even the assembler-language programmer is protected from some of the worst idiosyncrasies of the computer's hardware by the ministrations of the operating system (typically in the control of peripheral devices).
The aim of this book, however, is to look beneath this superstructure, and to describe computers and their variation at a basic hardware level, below the operating system but above the detailed electronics; we discuss further the nature of this level in § 1.3.
We first consider a very simple computer, with which we can introduce the basic concepts and terminology required for the remainder of the book.
We choose to do this in terms of a simplified version of the computer described in the paper ‘Preliminary discussion of the logical design of an electronic computing instrument’ by Burks, Goldstine, and Von Neumann (1946), for reasons which will be mentioned later.
This computer, shown in Figure 1.1, consists of four sections: the store; the data manipulation unit; the input and output units; and the control unit.
We review the salient points of these sections in turn.
(a) The store
The store is a collection of store locations or words , into each of which the computer can place a piece of information, to be retained for later extraction and use.
A word is a group of electronic components, each of which can be set into either of two states.
Thus a word can store any information that can be coded in the form of an appropriate number of binary bits: such a piece of information might be a numeric value, a group of one or more characters, or (as we shall see later) an instruction for controlling the computer.
The number of bits of information which a store word can hold is its   word length , and is the same for all the words in the store.
The word length for the store of a particular computer is chosen to hold a maximal numeric value or number of characters appropriate to the application area and cost to which the computer is designed.
Word lengths vary from 12 or less to 64 or more bits, typical lengths being 16 and 32 bits.
In order to store or retrieve information we must have some means by which the computer can refer to any location.
Since a store is constructed as a vector or linear sequence of words, we specify each one by its position in the sequence.
Thus if we have a set of N words or store locations, each has an associated numeric value in the range zero to N -1, the store address , by which it is uniquely specified.
Such a store address is a new type of information that we might want to deposit as the contents of a word.
Two special storage devices or registers form part of the store, the store address register (SAR) and the store data register (SDR), or store buffer register.
In order to store a piece of information, it is placed in the SDR; the address at which it is to be stored is placed in the SAR, and the control unit of the computer sends a ‘write’ signal to  the store.
After some delay depending on the technology involved, the operation is complete; the specified location contains the new information, its previous contents having been overwritten and lost.
In order to retrieve a piece of information, its address is placed in the SAR and a ‘read’ signal sent to the store; after some delay, the information is available in the SDR, for routing to some other part of the computer.
Note that the store location accessed still contains a copy of the information read.
We assume that the store is randomly accessible; that is, the time taken to access a store location in order to store or retrieve information is constant and (in particular ) is independent of the particular location being accessed and of the location previously accessed.
Notice that we avoid the (anthropomorphic) term memory, and the term core store, which presupposes a particular store technology (although it is often used generically); when we wish to distinguish among several levels of storage on a computer, we will refer to this basic level as main or primary store.
Furthermore the term register is sometimes used to refer to any device which holds a group of one or more bits of information, and which is capable of being accessed at electronic speeds: in this book we use it only for storage devices provided for some special purpose (such as the SAR), and do not apply it to a general store location.
(b) The data manipulation unit
The data manipulation unit is capable of performing any of a fixed set of operations as signalled by the control unit.
A typical operation requires two pieces of data or operands upon which to operate; one is extracted from an appropriate location in store, and one is found in a special storage device or register within the data manipulation unit, the accumulator.
The result of the operation is placed in the accumulator, destroying its previous contents.
In a computer oriented towards numerical calculation, the operations must include provision for addition and subtraction, and probably multiplication and division as well; on a computer oriented towards character manipulation there would be various operations for moving and scanning character strings.
We have chosen a name for this unit which does not presuppose numerical calculation; other terms for this unit are the arithmetic unit, the arithmetic and logical unit (ALU), and the mill (a term introduced by Babbage, and used in some British computers).
(c) Input and output units
Any computer has a number of input/output or peripheral devices attached to it, both for communication with the outside world (card readers, line printers, terminals, etc.) and for augmenting the computer's information storage capacity (magnetic discs, magnetic tape, etc.).
As with the data manipulation unit, these units are able to carry out any one of an appropriate set of operations when signalled by the control unit; for example, to read a digit punched on paper tape and store it in a specified store location or to retrieve a digit from a specified store location and print it on an electric typewriter.
At present we assume that the computer does only one thing at a time.
If a read or write operation is started on a peripheral device, the computer awaits the completion of this operation before continuing with the next.
When we consider that a typical input/output operation is thousands or millions of times slower than a typical internal operation (such as the addition of two numbers), and that there may be other tasks to which the computer could turn its attention while such an operation is going on, then we can see scope for redesigning the computer in this area.
We introduce the term transput from the Algol 68 language to cover both input and output: thus for instance we will talk of a transput device instead of a peripheral device, and a transput operation instead of an input/output operation.
(d) The control unit
The control unit supervises the operation of the other three sections of the computer discussed above.
It does this by initiating the transfer of data between units, and by sending appropriate control signals, in accordance with a schedule or program of instructions.
Each instruction is encoded in such a way that it can be held in a store location, so that the store contains both the data being operated on and the program of instructions specifying the operations to be performed.
The control unit contains a special storage device or register, the program counter, which contains the address of the store location holding the next instruction to be executed or obeyed.
Other names in common use for this register are the instruction counter, next instruction address register (or NIAR), sequence register, and current order register.
While the computer is running, the control unit is executing a basic cycle divided into two phases, fetch and execute.
The fetch phase involves extracting the contents of the store location referred to by the  program counter, and decoding it into an operation code portion, specifying an operation to be carried out, and an operand portion, specifying a store address (whose numeric value we will indicate as The program counter is then incremented by one, to point to the next store location in sequence, since this normally holds the next instruction to be executed.
The control unit then enters the execute phase, to carry out the operation decoded in the fetch phase.
There are three cases.
(a) An operation of the data manipulation Unit is called for; for example, transfer the numeric value held at store address X to the data manipulation Unit, and add it to the value in the accumulator, or transfer the value in the accumulator to the store and deposit it at address X.
(b) An input/output operation is called for; for example, transfer the character at store address X to the typewriter and print it, or read the next character punched on a piece of paper tape and transfer it to the store, to be deposited at address X.
(c) The sequence of instructions being executed is to be changed, so that the next instruction to be executed is not the one stored immediately after the instruction currently being executed.
The store address X extracted from the current instruction is placed by the control unit in the program counter (destroying its previous contents), so that the next fetch extracts an instruction from store address X (and then from store addresses X+ 1, X+2, etc., until a further such "jump" instruction is encountered).
We require two types of jump instruction; one (the unconditional jump) which always changes the program counter, and one (the conditional jump) which changes the program counter only if a certain condition is true (such as that the accumulator contains a non-negative value); if the condition is false, the next instruction to be executed is the one stored immediately after the jump instruction.
Thus the computer selects a course of action dependent on the data values it encounters.
To illustrate these concepts, Figure 1.2 gives a section of program to compare the contents of locations 100 and 101, and store the larger value in location 102.
The stored program concept
One of the consequences of holding instructions in store locations is that they can be treated as data, and manipulated as such by the computer.
This allows us to write programs which incorporate instruction modification.
Suppose, for example, that we wish to write a program in which the computer has to sum N values held in successive store locations, perhaps with store addresses 100,101,102, and so on .
N may be too large for there to be room for that number of add instructions to be held economically in the store, or the value of N may not be known when the program is being prepared (for example it might be read in as a piece of data).
On the simple computer described above we would have to do something like the following:
We write an add instruction which initially refers to store address 99, and arrange to have it executed N times by appropriate use of a conditional jump instruction.
Then, before each execution of the add instruction, we arrange to manipulate it as data in such a way that one is added to the address portion of the instruction; thus the add instruction refers successively to store addresses 100,101,102, and so on.
A skeleton program for this is shown in Figure 1.3.
There are major difficulties with the use of this technique, since it is likely to result in programs whose structure is extremely opaque, and which are therefore very difficult to understand and debug.
Furthermore the lack of any rigid demarcation between the (unchanging) program  and the (changing) data means that the computer hardware cannot be used to protect the program from corrupting itself, nor can a single copy of a program be shared simultaneously by several users.
For these reasons the basic form of instruction modification described above is no longer used; instead its effect is achieved by the use of index registers, as described in the next section.
The stored program concept, the realization that instructions can be encoded and held in the store of the computer together with the data being operated on, is an important one for two reasons.
First, as mentioned above, it introduced the idea of instruction modification, for which better mechanisms could then be found.
Second, a program as a whole could be treated as data by a supervisory program; a loader, an assembler or compiler, or an operating system.
Since this is such a vital concept, all the computers discussed in this book are stored program computers.
We are thus ignoring two classes of externally programmed device, common in the early days of computing:(a) Computers whose programs were held on such external media as paper tape or punched cards.
Examples of this class are the Harvard Mark I (Aiken and Hopper 1946) and Babbage's analytical engine (Bowden 1953, Appendix 1).
(b) Devices where the program resides in a plugboard, such as the early ENIAC computer (Goldstine and Goldstine 1946).
We could envisage computers with two main stores, one for the program and one for data, but these would be very inflexible.
Instead we assume that each store location in main store can hold either an instruction or a piece of data; this does not, of course, preclude the possibility of distinguishing these two cases at any particular time, either by segregating instruction areas from data areas, or by marking store locations in some way.
The general-purpose electronic digital computer
Let us now try and define rather more precisely the type of device that we are considering.
We have introduced the term computer, and by this we mean a device which can process substantial quantities of data without detailed human intervention.
We have limited the field to stored-program computers, and we further limit the field to general-purpose computers; that is, to computers  with an instruction set rich enough to perform a wide variety of tasks.
This concept is rather vague because it can be shown that a computer with a very basic instruction set can simulate a Turing machine and therefore can, in theory, perform any "computable" task, so that such a computer is (again in theory) completely general-purpose (see for example Minsky 1967).
We will not normally discuss the technology out of which the architectures that we describe are to be implemented.
However, we will from time to time assume that we are discussing electronic computers, implemented by the routing and gating of electrical signals.
This does not preclude the theoretical possibility of implementation in other technologies, such as fluidics (see for example Gluskin, Jacoby, and Reader 1964), although historically this has not been the case.
Finally, we will be considering digital computers, where data is stored and manipulated by devices that can assume one of only a finite number of states, rather than by devices which can assume any state in a prescribed continuous range (as in analog computers).
Von Neumann's computer
There are many possible computers which could have been used to exemplify the terms we have introduced.
For example we could have used a simple computer such as the DEC PDP-8, or a suitable hypothetical computer such as that presented for didactic purposes by Knuth (1968).
However, these have assimilated some of the developments in computer design which we will discuss later.
Instead, we choose to use the Von Neumann, Princeton, or LAS (Institute of Advanced Studies) computer introduced in Von Neumann's paper.
We take this paper to mark the beginning of the era of modern computing, since it was the "first widely circulated document about high speed computers" (Knuth 1970).
The paper proposed a computer with a store of 409640-bit words stored on the faces of a number of "Selectrons" or electrostatic storage tubes, this being a device able to provide random-access storage before the introduction of the ferrite core store (first used on the Whirlwind computer at MIT in 1953).
Although such a device matches the modern conception of main store, most early computers (such as EDVAC (Knuth 1970) and EDSAC (Wiles and Renwick 1949)) had a serial store, implemented as a magnetic drum or set of delay lines (as discussed in 5.6).
The word length of 40 bits was chosen to give suitable accuracy for  the type of fixed-point binary calculations for which the computer was designed (hardware floating-point arithmetic having been rejected).
Instructions of such a length would have been wasteful of storage, so instructions were 20 bits long (6 for an operation code, 12 for a store address, and 2 unused) and held two to a store word, as shown in Figure 1.4.
The control unit executed first the left-hand instruction, then the right-instruction, of each word in the program.
Pairs of jump instructions were provided to transfer control to the left- or right-hand instruction of a specified store location.
The internal operations proposed in this paper are shown in Figure 1.5.
Notice the "partial substitution" orders (18 and 19), provided for instruction modification and subsequently altered to provide certain shifting facilities as well.
Apart from the storage of two instructions to a word, the only extension to our rudimentary computer is in the provision of a second storage device or register in the data manipulation unit, the arithmetic register AR, used in conjunction with the accumulator for multiplication and division; the need for such a register is discussed in 2.1.
Figure 1.6 shows how the section of program in Figure 1.2 would appear encoded for the Von Neumann computer, to occupy locations 50 to 53.
Input/output operations were not specified in detail in this paper, but three devices were proposed: an electric typewriter for the transfer of small quantities of data, a display unit for graphical presentation of results, and several magnetic wire or tape units to provide a secondary storage medium and for all normal input and output.
It was expected that input data would be transcribed to the magnetic wire by a process which did not involve the computer, and similarly for output.
Word-oriented single-address binary computers
Some basic features of the Von Neumann computer do not carry over to all the other designs we will study, for it can be characterized as a word-oriented, single-address binary computer.
(a) To say a computer is word-oriented means that the basic unit of storage is a word, of from twelve to sixty-four or more bits, able to hold a complete numeric value or (usually) at least one instruction; in 2.5 we will meet the character- or byte-oriented computer, where the basic unit of storage is 6 to 8 bits long.
(b) A single-address or one-address computer holds only one store address per instruction.
The other common instruction format is the two-address format, where each instruction holds the addresses of two operands, and one of these addresses is further used to hold the result.
These and other possible arrangements are discussed in 3.2.
(c) Finally, this computer is oriented towards the performance of arithmetic on values held in a binary format (that is, to base two).
In 2.5 we discuss the possibility of performing arithmetic directly on values held in decimal format (that is, to base ten).
Strictly speaking all modern digital computers are binary, in the sense that the basic electronic units out of which they are built can assume either of exactly two stable values (thus representing one binary bit).
Devices where the units can assume any one of three or ten states have been proposed but rarely implemented: for the ternary (3-state) case see Knuth (1969, pp. 173–5).
We thus use the term binary for computers which manipulate numeric values expressed directly in a binary format, and decimal for those where binary patterns are interpreted as decimal digits.
1.2.
The Development of the Computer
Since the introduction of the stored program concept, there have been many developments in the design of general-purpose digital computers, but only two can be described as vital omissions from the computer described in the previous section.
The index register
The first of these is the index register, introduced on the Manchester University Mark I computer in the late 19405 (Kilburn, Toothill, Edwards, and Pollard 1953).
Let us consider again the problem of adding together N values held in store locations 100, 101, 102, and so on.
It would be convenient to be able to write an instruction to add into the accumulator the contents of store address 99, and to have some automatic means for modifying  the instruction just prior to its execution, so that it refers successively to addresses 100, 101, 102, and so on.
This can be done by providing a storage device in the control unit, called the index register (or B-line on the Manchester computer).
We then arrange for the contents of this register to be added to the store address in each instruction before it is used to access a particular location in the store.
Our "add" instruction now has the effect add into the accumulator the contents of the store location with address (99 + contents of index register).
Thus when the index register holds a value (say) 4, the "add" instruction will have the effect "add into the accumulator the contents of store location (99+4) or 103" .
By controlling the value held in the index register, we can make the add instruction refer to a different element each time it is executed.
Notice, however, that the instruction itself, coded as a binary pattern in a particular store location, does not change between executions.
We now need instructions to insert a value into the index register, or to modify the value held (for example, by adding one).
In this example we initialize the index register to zero and, on each iteration of the loop, we add one to the value held by the index register before executing the add instruction.
A skeleton program is given in Figure 1.7.
Sometimes we will wish to use the add instruction without modification by the index register, so we need an extra bit set aside in all such instructions, to specify whether or not the modification of the operand field by means of the index register is to take place.
A common extension to this technique is to provided several such index registers.
Now a larger field is required in the instruction to specify, not only that indexing is to be performed, but also which index register is to be used.
We may also have further instructions to  manipulate the index register or registers.
Index registers are discussed further in 4.2.
Interrupts
The second major development in computer design is the interrupt facility, which seems to have appeared first on the Univac 1103 computer (Bell and Newell 1971, p. 48) in the early 1950s.
In early computers, as mentioned above, input/output (or transput) operations were treated in the same way as internal operations (such as addition); once the operation had been started, the control unit awaited its completion, before continuing with the extraction, decoding, and execution of the next instruction.
This was an inefficient system, since there would often be other processing that the computer could be doing while a transput operation was being completed.
We therefore modify each transput instruction so that it merely initiates an operation (such as to print a digit on the electric typewriter), and allow the control unit immediately to continue with the next and subsequent instructions while the transput operation is being carried out.
The program must have some way of establishing when a transput operation is complete, so that a further operation can be initiated or (in the input case) so that the input character can be processed.
To this end each transput device has an associated one-bit register, which is cleared as an operation starts and is set to one when it terminates; an instruction is provided to read this register, and thus the programmer can establish when a transput operation is complete.
Such a one-bit register holding information as to the status of a transput unit is called a flag; more generally, we use the term flag to refer to any one-bit field holding status information (whether in a special register or in a portion of a store location).
Such a solution has the disadvantage that, if the transput units are to be driven at anything like their full speed, the programmer must scatter test instructions through his program.
A better solution (common to all modern computers) is to allow the control unit to continue with subsequent instructions as before, but to allow any transput unit to interrupt the control unit when an operation is complete.
This interruption, or interrupt, takes the form of automatically switching the control unit from its current sequence of instructions to a separate sequence of instructions, whose task is to deal with the completion of the transput operation (and perhaps to initiate a further such operation).
There must of course be provision for retaining information about the interrupted sequence of instructions, so that processing can continue from the point at which the interruption took place.
A simple interrupt system might operate as follows.
Suppose an interrupt signal is sent to the control unit by a transput device while the instruction at location n is being executed.
The execution of the current instruction is completed, the current contents of the program counter (n+ I) are stored in some fixed location (perhaps zero), and the program counter is loaded with a fixed address (say one).
Thus the control unit proceeds to execute instructions from locations 1, 2, 3,…-etc., and these constitute the interrupt service routine to deal with the interrupt.
When this routine has completed its operation, it returns to the interrupted program by using the old program counter contents stored in location zero.
The interrupted program therefore proceeds with the execution of the instructions at locations n+1, n+2,…, etc. as if nothing had happened, until the next interrupt signal is received.
As we shall see in 6.3, the interrupt facility can be expanded to switch instruction sequences on the occurrence of many different events; further we will see the need for a means of ensuring that at critical times the computer is able to ignore all but a subset of these events.
The effects of changing technology
It is traditional in the computer field (insofar as the term "traditional" has a meaning for a subject only thirty years old) to refer to four generations of computers, according to the electronic technology out of which the control and data manipulation units are constructed.
This is shown in the first three columns of Figure 1.8.
The most obvious result of the evolving technology is increased speed, from a few thousand instructions per second to several millions or tens of millions of instructions per second.
Other important aspects, however, are the reduced cost per functional unit, reduced power requirement, and increased reliability and flexibility in use.
Main storage technologies have similarly evolved, as shown in the fourth column of Figure 1.8.
Again we have an increasing speed but, equally important, a decreasing cost per bit, and therefore a capacity increase from a few tens of words to several million characters of main storage.
We tend nowadays to see these generations not merely in terms of the underlying electronic technology, but also in terms of the organization of the hardware and software involved in a computer system; thus the transition to the fourth generation can be seen more as a change in the way that computer systems are organized (with the use of such techniques as virtual storage and distributed intelligence), than as a change in the underlying technology.
A major problem with early computers was the question of reliability; pessimistic forecasts were made of the mean time to failure of assemblies of several thousand valves, and a prime consideration in designing a computer was to make it as simple as possible.
With the improving reliability and flexibility and reducing cost of electronic components, it is possible to build more complex hardware systems to simplify the requirements on the computer programs.
Thus we have already seen that the provision of (hardware) index registers provides a facility (instruction modification) which previously had to be provided by software, and the interrupt facility is the hardware equivalent of scattering transput test instructions through a program.
Throughout this book we will see examples of situations where a facility (for example stacks in 3.2, store protection in 5.1, and sophisticated transput operations in 6.2) could be provided by means of a piece of software.
but is nowadays provided by extra hardware.
It is interesting to note that certain problems which have been solved by technological improvements reappear as the technology continues to evolve.
Thus the problem of splitting a program between two storage media, one fast but small and the other large but slow, has reappeared at different times and at different storage levels.
As with the history of the automobile, several radically different early designs were tried out before the main stream of development settled on the Von Neumann  model; some of these ideas have reappeared as specialized computers or as variations on the basic model (for example parallel operation of components of the data manipulation unit).
Because of this we will not attempt in this book to trace a historical sequence from early, simple computers to later, complex ones.
Instead we will discuss together all design variations in each particular field; within a field it should not be assumed that the more complex facilities are the later ones, since evolution has often been towards simplifying systems from the programmer's viewpoint.
The history of computers is discussed in Serrell, Astrahan, Patterson, and Pyne (1962), Rosen (1969), and Randell(1975).
Application areas
In the early development of computers there was a split between computers designed for scientific calculations and computers designed for commercial or business data processing.
Scientific computers would carry out calculations on numeric values held (on the earliest computers) in binary fixed-point format or (more typically) in binary floating-point format; the transput facilities, particularly in the first generation of computers, would be rudimentary.
Computers for business use, on the other hand, were oriented towards operations on strings of characters and would require much more extensive transput facilities; arithmetic would normally be in decimal rather than binary, to avoid the time taken in conversion.
Nowadays it is clear that this distinction is illusory; scientific applications of computers often require character string manipulation and need many of the transput facilities of business data-processing, while business applications increasingly need sophisticated arithmetic calculation.
Thus many third and fourth generation computers provide a range of facilities (such as several data-types) in an attempt to cover the whole spectrum of applications in one design.
Another important application area for computers is in real-time control.
Once reliability was adequate, computers could be used to control chemical plants, scientific experiments, missiles in flight, and the like.
Such computers have, of course, to stand up to more extreme environmental conditions than in the application areas described above.
Architecturally they are likely to have a short-word length (most commonly 16 bits), to perform arithmetic in fixed-point binary format, and to be provided with sophisticated transput and interrupt facilities.
Recently such minicomputers have become more similar to computers in the mainstream, with extensive data-types and with larger addressing ranges, and are entering the fields of more conventional scientific and business computing (see for example the early DEC PDP-8, and the more recent PDP-11, Data General Nova, and GEC 4000 series).
We see, therefore, that a number of specializations have appeared since computers were introduced, but typically these have been reabsorbed into the mainstream of computing.
Microprocessors
With each generation of computer technology, the physical space taken up by the electronic components has become smaller and smaller, until with the introduction of large-scale integration (LSI) it became possible to fit thousands of components onto a silicon chip less than a quarter of an inch square.
This allowed a complete processor, consisting of data manipulation unit and control unit, to be fitted onto a chip.
The first such microprocessor was the Intel 4004, which used a basic 4-bit unit of data and appeared in 1971.
A wide range of 8-bit and 16-bit microprocessors is now available.
If we add some storage on a second chip, we obtain a complete microcomputer at an extremely low cost and small physical size, and this can economically replace hardwired logic.
We can use these microcomputers to build "intelligence" into appropriate parts of a computer system (such as the transput devices), but they are increasingly finding applications outside conventional computing, for example in controlling washing machines and the ignition systems of cars.
The physical implementation and application areas of microcomputers are thus different from mainstream computers, but at the instruction set level which we discuss in this book they can be treated as variations on the central Von Neumann model.
Microprocessors are discussed in more detail in Aspinall and Dagless (1977).
As an example of microprocessor architecture, we briefly discuss the Intel 8080, a well-known microprocessor which has been widely copied.
Physically the Intel 8080 is a package approximately two inches long by half an inch wide by a fifth of an inch deep.
It has 40 pins for communication with other chips and the outside world, 8 to pass an 8-bit data word or byte, 16 to pass an address, 4 for power supply, and 12 for control signals (6 for input to the 8080 chip, 6 for output from it).
In order to complete the processor, a chip containing a clock, and some electronics to provide the power supply, are necessary.
Store chips  come in two forms; normal read/write store (usually called RAM, for random-access memory) and read-only store or ROM (for read-only memory), where the contents of individual words can be read but not written.
Since a microcomputer is usually dedicated to controlling some fixed task, the program of instructions can be permanently built into the ROM so that it cannot be corrupted.
A typical microcomputer system therefore consists of a microprocessor chip, one or more ROM chips, one or more RAM chips (to hold variable data), and a further chip to decode the top few bits of the 16-bit address coming from the microprocessor, in order to direct it to the appropriate store chip.
Further chips provide standard types of communication path between the microprocessor and transput devices.
The basic unit of data in the Intel 8080 is the 8-bit byte, and instructions are provided to perform fixed-point binary arithmetic between 8-bit values.
Because of the low precision of 8-bit arithmetic, facilities are provided for propagating a carry bit from one arithmetic operation to the next, so that multiple-precision arithmetic operations can be programmed.
Because of the expected application areas of this microprocessor (including pocket calculators), facilities are also provided for performing arithmetic on bytes interpreted as two 4-bit decimal digits.
As on the Von Neumann computer, the Intel 8080 has an (8-bit) accumulator, on which most arithmetic operations take place, and a (16-bit) program counter, to hold the address of the next instruction to be executed.
There are also six 8-bit registers called B, C, D, E, H, and L, which can be used individually or in pairs (BC, DE, and HL) to hold intermediate values during a computation.
A register pair can also be used to hold an address; in fact, one form of addressing mode used by instructions expects to find the operand address in the HL register pair.
There is also a 16-bit stack pointer register, which holds the address of the top of a stack or last-in-first-out list of store locations (see Chapter 3).
Three instruction formats are used; a 1-byte format with just an operation code (for example "clear accumulator" ), a 2-byte format with an operation code and a 1-byte operand field (for example  "load into accumulator the value in the second byte of the instruction" ), and a 3-byte format with an operation code and a 2-byte operand field (for example  "load into accumulator the contents of the byte whose address is given by the second and third bytes of the instruction" ).
Ranges of computers and optional features
When we discuss computer design variations in the ensuing chapters, we will tend to make two assumptions which it is as well to clarify; first, that each computer is designed independently for a particular application area and to purely technical criteria; and second, that a computer design is implemented as a fixed set of facilities for all customers.
In fact a computer manufacturer normally designs a number of compatible models rather than a single computer, and these cover a range of speeds, capacities, and costs.
The models will be architecturally very similar, usually with all (or most of) an instruction set in common; further, transput devices and main storage may be transferable between models.
This makes it easier for the customer to move to a larger model in the range as his requirements increase, and eases the manufacturer's problem of providing software for a number of different computers.
Examples of this are the ICL 1900 and 2900 ranges of computer.
However, this is at the expense of a new problem of economically providing, on the lowest model in the range, all the hardware facilities needed on models at the upper end of the range.
Furthermore, manufacturers wish to transfer their customers to their new ranges of computers from earlier models, and therefore want to minimize the problems of transition.
This can be done by making the new range of computers architecturally similar to earlier models, and by making improvements only in the technological implementation of this architecture; alternatively we can use microprogrammed emulation, as discussed in 3.6.
To the technical criteria for designing a particular computer will therefore be added criteria of compatibility with earlier models or with other models in the same range.
This is, of course, part of the technological solution of the computer manufacturer's general marketing problem of capturing and retaining a customer base.
First generation computers were designed with a fixed instruction set and store size, and with a fixed complement of transput devices.
It soon became apparent that customers had different requirements for the number and type of transput devices to be attached to their computer.
It is usual today therefore to provide a number of general electronic interfaces to which a range of transput devices can be attached, and a set of general instructions which can be used to control any transput device available at present or in the future, from the computer manufacturer or elsewhere.
When larger quantities of main store became available, customers would not always want the maximum quantity addressable by the control unit.
So most computer models are now made available with a range of different storage sizes.
Finally, we mentioned above that many third generation computers have a range of facilities to cover a number of application areas; if some of these facilities are optional, we may particularize Our computer for an application area by selecting from these options.
For example, models in the IBM 370 range have a basic instruction set for general data manipulation and transput control, an optional floating-point instruction set for scientific calculation, and an optional decimal digit string instruction set for business data-processing.
Thus a computer model is tailored for a customer by a choice of optional features, main store capacity, and number and type of transput devices.
1.3.
Computer architecture
In this book we are concerned with those internal design aspects of a computer which directly affect the way in which it processes data; we therefore exclude such external aspects as the packaging of electronic components and units, the supply of power, or the conditions under which the computer is capable of operating, important though these are.
As we suggested earlier, most users (and even most programmers) see the computer at a level remote from the hardware, shielded as it is behind layers of operating system and compiler software.
As illustrated in Figure 1.9, we can make out a number of levels more basic than this; the electronic circuits themselves, the logical functions (such as gates and flipflops), and the functional units (such as adders and registers).
In this book we will be looking at the level at which facilities are provided in electronic hardware, as they might be seen by a system programmer about to implement the most basic software on the computer.
We choose to consider a system programmer's view rather than that of an application programmer (even at the assembler language level), because the latter would expect to use facilities provided by supervisory software to mask the hardware treatment of such things as transput control.
We term this the level of computer architecture, at which a set of logically integrated hardware functions are programmed to carry out   the processing of data.
Is to be distinguished from the implementation, in which electronic circuits are designed to realize an architecture.
We could implement the same architecture in a number of ways, and some ranges of computers (such as the IBM 370 range) are designed to a common architecture but with a radically different physical implementation for each model.
Books which describe this architectural level, and which are recommended for further reading, are Bell and Newell (1971), Stone (1975), Tanenbaum (1976), and Foster (1976a).
In practice certain aspects of an implementation may be discernible at an architectural level, most noticeably in coping with hardware faults and in the possibilities for enhancing program performance.
We drop below the level of computer architecture to discuss these in Chapter 8.
Furthermore, we discuss certain concepts in computer design which could be treated as implementational, their use being transparent to the architecture; the cache stores of 5.4 usually fall into this class.
There are a number of important implementational design decisions  which we mention here only briefly, as below the level of our current interest:
(a) Synchronous and asynchronous operation.
If control and data signals are propagated through the circuits of a computer only at time instants controlled by a master "clock" or pulse generator, then we have a synchronous computer.
If signals are propagated through the circuits at a rate governed only by the delays in the circuits themselves, then we have an asynchronous computer.
Because of the greater simplicity of design, the major units in a computer are generally synchronous.
Thus the control and data manipulation units might be timed by one clock, the store by a second, and each transput device by its own clock.
Thus each unit is internally synchronous, but is asynchronous with respect to other units; we therefore require synchronizing hardware at the interface between each pair of units.
(b) Serial and parallel data manipulation.
In data manipulation on a parallel computer all bits of a word are processed at the same time, while on a serial computer the bits of a word are processed one after another.
Thus if addition takes t time units per bit position, a serial computer with n bits per word will take nt time units to add two words, whereas a parallel computer will take t time units (plus some time for the carry propagation).
A serial computer requires less logic in the data manipulation unit than a parallel computer, but the control unit may be more complicated.
Many early computers were serial, since they were built round serial storage media such as delay lines.
Nowadays store technologies are inherently parallel, and the majority of computers are parallel in operation.
The choice of a serial implement tat ion for a computer would affect the architecture to the extent that we must choose a representation for negative numbers that can economically be processed in a serial manner.
(c) Improved data manipulation algorithms.
Finally, the speed at which arithmetic operations can be carried out is one of the limiting factors to the internal processing speed of a computer.
We are led to consider ways of speeding up these operations, for a given fixed technology and for a given fixed representation for numbers (which determines the results of these operations at an architectural level).
Thus we are led to consider such topics as carry-lookahead for addition, Booth's algorithm for multiplication, and non-restoring methods for division.
Details of such topics can be found in books on computer hardware design such as Hill and Peterson (1973), Hellerman (1973), or Townsend (1975).
In early computers, in particular those of the first generation, little distinction was made between architecture and implementation; the architecture was whatever could reliably and economically be implemented in hardware.
Today there is a growing tendency to define a computer architecture some way above the basic hardware level, to be implemented in a mixture of hardware and software, whose proportions may vary with the evolution of computer technology and with different models in a range.
When an architecture is implemented partly in hardware and partly in software we require such concepts as extracodes and microprogramming, and these are discussed in Chapter 3.
We will thus discuss features above the hardware architecture level from time to time, to complete the picture of a facility implemented only partly in hardware.
Plan of the book
In Figure 1.1 we showed a typical computer divided into four sections: the store; the data manipulation unit; the control unit; and a number of transput units.
In practice we may find it difficult to separate the hardware of the data manipulation unit from that of the control unit.
We therefore treat these two sections as one, the central processing unit or processor, and we redraw our typical computer as shown in Figure 1.10 with three sections: the store; the processor; and the transput units.
The processor contains circuitry to request the next instruction from the store, and to receive, decode, and execute it; the latter may require further read or write accesses to the store, or interaction with a transput unit.
It contains a number of processor storage registers, among them the accumulator, the program counter, and one or more index registers.
Starting from Figure 1.10 we consider design variations in each of the three sections.
In chapters land 3 we consider design variations in the processor.
In Chapter 2 we consider the formats in which data might be held for manipulation by the computer, and the types of operations appropriate to each format.
In Chapter 3 we consider other   aspects of processor design; the layout of instructions and how operands may be accessed by an instruction, control and other instructions not dealt with in Chapter 2, provision for supervisory software, and the concept of microprogramming.
Chapters 4 and 5 discuss design aspects of the store.
Chapter 4 considers addressing modes; that is, the range of ways (commencing with index registers) in which an instruction can specify a location in the store.
Chapter 5 considers other aspects of the store; protection of store areas from being inadvertently overwritten, the use of particular store locations for special purposes, means by which the store's apparent capacity and speed can be increased, and the use of associative and cyclic stores.
When we turn to the third section of Figure 1.10, we see a wide range of devices for receiving information from the outside world, for transmitting information to the outside world, and for expanding the storage capacity of the computer.
Because of this wide range, most  computer designs are not tied to the details of individual devices, but provide generalized transput control features and instructions, to which any selected input, output, or backing storage device may be interfaced.
Chapter 6 therefore begins with a description of such general transput systems.
Because of the mismatch between processor and transput device speeds, we are led to consider uncoupling the devices from direct processor control and providing them with some degree of autonomy.
Chapter 6 therefore deals with the possible degrees of autonomy provided for transput control systems.
This begins with the concept of interrupts introduced in 1.2, and leads on to the idea of interconnecting a number of processors.
All the computers we consider are general-purpose, at least in theory, although they may be oriented towards particular application areas.
In Chapter 7 we consider computers whose orientation has resulted in an architecture radically different from those in the rest of the book, although they have a sufficiently general instruction set to lay claim to universality.
Finally, in Chapter 8 we consider some topics on the border between architecture and implementation; dealing with faults in the hardware of the computer, ways of speeding up the processor, and the interface between computer and operator.
There is a problem in analysing computer designs in this way, in that a computer is an integrated system, and decisions in the design of one section of the computer will have effects in other sections.
We must therefore expect certain aspects of computer design to reappear throughout the book, to be seen from a slightly different angle each time.
One particularly important design decision is the choice of word length, since this affects how instructions are stored, the number of bits which are used to specify operation codes and addressing modes, and what data formats can or must be provided (for example, double-length working may be a necessity, if a short word length is chosen).
If the word length is short, consideration must be given to methods for achieving an adequate address range with a short operand field.
In order to provide specific illustrations of the ideas and techniques discussed in the book, we use a number of well-known conventional computers as running examples throughout the text.
These are as follows:(a) The DEC PDP-8 (Bell and Newell 1971, pp. 120–36; DEC 1972a), an early minicomputer with a 12-bit word, appearing in 1965.
(b) The DEC PDP-11 (Bell, Cady, McFarland, Delagi, O'Laughlin, Noonan, and Wulf 1970; DEC 1972b) and Data General Nova (Data General 1971; Townsend 1975, Chapter 8), more recent 16-bit mini-computers, both appearing in the period 1970–1.
(c) The IBM 360 and 370 ranges of medium and large computers, based on a common "byte-orientated" architecture (see 2.5).
For most purposes the architecture of models in the IBM 360 range (appearing in 1964–5; Amdahl, Blaauw, and Brooks 1964; Blaauw and Brooks 1964; IBM 1964; Bell and Newell 1971, pp. 561–87) is the same as that in the IBM 370 range (appearing in 1970–1; IBM 1970; Katzan 1971a; Case and Padegs 1978).
However, we usually refer to the latter in the text, as it has a number of additional features of interest, such as paging and segmentation.
(d) The DEC PDP-10 (appearing in the late 1960s: DEC 1972c; Bell, Kotok, Hastings, and Hill 1978), a typical medium-sized, word-oriented computer.
(e) The Burroughs range of medium and large computers comprising the B5000 (appearing in about 1961), the B5500, B6500, and B7500 (appearing in the late 1960s), and the more recent B5700, B6700, and B7700.
These all use an architecture based on stacks and segmentation, and oriented towards high-level language usage; its fully-developed form is described in Hauck and Dent (1968), Creech (1970), and Organick (1973).
In the text we refer to this architecture as the "B6700 computer" for brevity.
(f) The CDC 6600 "super-computer" (appearing in 1964; Thornton 1964, 1970; CDC 1966), an architecture oriented to very high speed operation on scientific problems.
The CDC 7600 has a similar but extended architecture.
(g) The ICL 2900 range of medium and large computers (announced in 1974; Huxtable and Pinkerton 1977; Buckle 1978) and the University of Manchester MU5 (Kilburn, Morris, Rohl, and Sumner 1968; Sumner 1974; Ibbett and Capon 1978) on which it is based, as examples of more recent architectural practice.
The instruction set
We will usually be treating individual instructions or groups of instructions in terms of the facilities that they control, so it is worth saying a few words here about the instruction set as a whole.
It has been shown that a single instruction is sufficient (in theory) for a general-purpose computer (Van der Poel 1956); however, instead of a theoretically sufficient instruction set, what we require is an instruction set which provides a practical set of operations in which applications can be programmed.
The DEC PDP-8 computer has the instructions shown in Figure 1.11, and this must be near the lower practical limit.
The IBM 370 range of computers has more than 180 different instructions, but some of these provide the same operation (such as "add" ) on different data types; we could reduce this number by using a tagged architecture, as described in 2.6.
Furthermore, an instruction set could provide an arbitrary   collection of operations which seem likely to be convenient, or we can attempt to group the operations and provide some structure in the set of instructions provided; the instruction set of the Pegasus (Elliott, Owen, Devonald, and Maudsley 1956) is a good example of the latter.
Describing the Architecture
The architecture of a computer is usually defined in terms of the format and effect of machine-level instructions.
This description is given, partly verbally and partly in a more formal way (by tables and diagrams), in the computer system reference manual, or in a programmer's guide to the computer's assembler language; the latter tends to subsume the effect of some of the computer's more basic software in the description.
There have been a number of attempts to define languages at the assembler level which would be portable from one computer to another; the compatible computer ranges of the previous section are, of course, one such attempt.
Conway (1958) suggested a Universal Computer-Oriented Language or UNCOL, which would be a common assembler language for all computers.
However, a sufficiently general language would have all the disadvantages of not being able to deal with the idiosyncrasies of a particular computer, with none of the advantages of a high-level language.
Another solution is the so-called "high-level assembler language" , or "machine-oriented high-level language" (Van der Poel and Maarssen 1974), for example BCPL (Richards 1969) and PL360 (Wirth 1968) which provides such high-level facilities as block structure and loop control, but with some concessions to the type of manipulations required at the machine level.
Alternatively we can start at the level of registers and transfers between registers, and describe our architecture in terms of these primitives; a microinstruction set would be an example here.
Iverson (1962) designed the APL language to describe hardware at this level, although the language is general enough for use in other areas; Falkoff, Iverson, and Sussenguth (1964) give a description of the IBM 360 architecture using this language.
Bell and Newell (1971, Chapter 2) suggest a pair of notations for describing computers; ISP (Instruction Set Processor) for descriptions at the instruction level, and PMS (Processor-Memory-Switch) for an over-all view of hardware components making up a complete computer system.
The ISP notation has been used to describe the appropriate parts of the DEC PDP- Il computer (DEC 1972b).
In this book we will be using verbal, informal descriptions rather than such a formal language for two reasons;(a) these languages define more detail than we usually wish to consider; and (b) they describe complete computers, whereas we wish to look at any stage at a particular aspect of a number of computers.
One further means of describing a computer should be mentioned for completeness; this is the use of some numeric value or values to indicate computer performance.
In early computers the add or store cycle time would be quoted; even today raw computer power may be expressed in terms of the number of instructions executed per second, using a weighted average over some standard instruction mix.
This of course says more about the technology than the architecture of a computer.
A more realistic performance measurement, which takes into account the effectiveness of an architecture, is the resources (including time) required for processing a set of standard benchmark programs, which cover the expected areas and types of use of the computer.
For a survey of this area see Lucas (1971) and Stone (1975, Chapter Il).
Problems
1.1.
Describe in detail the sequence of steps carried out by the store, data manipulation unit, and control unit of the Von Neumann computer when executing the sequence of instructions given in Figure 1.6.
1.2.
How does the Von Neumann computer distinguish between a location holding a pair of instructions and a location holding a numeric value?
1.3.
A set of fifty numbers is held in the store of a Von Neumann computer, say in locations 100 to 149.
Write a program, using the instruction set given in Figure 1.5, to find the largest number and put it in location 150.
Comment on any difficulties caused by (a) the lack of index registers,(b) the presence of two instructions per word, and (c) the provision of only a single conditional jump instruction.
1.4.
Two index registers are to be added to the Von Neumann computer.
How would you modify the instruction format and add to the instruction set?
Reprogram problem 1.3 using this new facility.
1.5.
Using your extensions to the Von Neumann computer for problem 1.4, write a program to sort into ascending order 100 numbers starting at, say, location 200.
1.6 Suppose the Von Neumann computer had n input devices, the ith device having a device flag register DF; and a device buffer register DBi.
We add the following instructions: Start Input Device i: set DF; to 0, and start reading the next character into DB; on completion, set DF1 to 1.
Read j: transfer into the Accumulator the contents of DBi, and set DF to O. Test i: if DFi is set to 1, skip the following instruction.
Interrupt On: switch the interrupt system on.
Interrupt Off: switch the interrupt system off.
When the interrupt system is on, and any device flag register is set to 1 then the computer finishes the current instruction, switches the interrupt system off, stores the contents of the program counter in the left-most twelve bits of store location 0, and continues to execute instructions starting at location 1.
Write an interrupt routine to service input device one.
1.7.
Extend your answer to problem 1.6 so that it deals with any number of input devices.
Why should the "Interrupt On" instruction have a delay of one instruction before taking effect?
1.8.
Suggest instructions to control a set of output devices, and extend your interrupt service routine accordingly.
What are the problems of allowing a high-priority device to interrupt the servicing of a low-priority device?
1.9.
Obtain a manual describing the instruction set of a medium or large computer.
Attempt to place the features described in the following categories:(a) Features provided by the assembler program.
(b) Features provided by the supervisory software.
(c) Features provided by the instructions at the architectural level.
(d) Features caused by the implementation.
(e) Features of particular transput devices.
Do the same for a mini- or micro-computer.
Are the categories more or less clearly distinguished?
1.10.
Write a program to simulate the architecture of a computer of your choice (such as the Von Neumann Computer).
Do you have to make assumptions about details of the implementation of the computer in order to complete your program?
Is there any way to distinguish the running of a program on your simulator from its running on the original computer (apart from speed)?
1.11.
Write a program, for a computer to which you have access, to  clear all of main store (including that occupied by the program).
If this is not possible on your computer, what is the minimum number of uncleared locations?
2 Data-types and operations
2.1.
Fixed-point binary arithmetic
IN THIS chapter we describe various formats in which data may be stored and manipulated within the computer.
Historically the computer has been seen as a device for performing calculations (as evidenced by the term "computer" ), and for this reason we commence our discussion of data-types with the fixed-point binary format.
Not only is this format the simplest and earliest means of storing numeric data in the computer, it is also used for store addresses and forms the basis for encoding the individual subportions of more complicated arithmetic data-types, such as the fraction and exponent of the floating-point format described in 2.2.
For further reading on computer arithmetic see Richards (1955), Flores (1963), Knuth (1969), or Stein and Munro (1971).
It should be noted that, despite the historical primacy of arithmetic calculation as the raison d'etre for computers, we ought perhaps to see them instead as symbol-processing devices.
This would lead us to begin with the logical and data transfer operations of 2.3 and 2.4.
We would then treat arithmetic as a set of manipulations which (in theory) could be built up from these more primitive operations, but which in practice are supplied as basic computer instructions for efficiency.
A computer word holds a fixed number of bits, in a range from about 12 to 64, and we number these bit positions consecutively, starting at zero at the right-hand or least-significant end.
Then, in the fixed-point binary format, the presence or absence of a bit at the ith bit position indicates the presence or absence of the ith power of two in the numeric value held.
Thus in a 16-bit word, the fixed-point binary value 0010101101110001 represents , or 11, 121 in decimal.
Here we assume that the right-hand bit position represents 2 or one, so that numeric values lie in the range zero to 2 n -1, where n is the word length.
We could instead scale all numeric values held in the computer by agreeing that this bit position shall represent some other positive or negative power of two.
Indeed, we can use different scaling factors for different pieces of data, as long as we align the binary points appropriately  when performing arithmetic.
The position of the binary point affects the basic arithmetic operations only when performing multiplication and division.
It is usual to consider fixed-point binary values either as integral (as described above), or as proper fractions with absolute values lying in the range zero to one; we use the former convention for the rest of this section.
Negative numbers
In some circumstances, the numeric values we wish to represent and manipulate take on only non-negative values.
This is usually the case when fixed-point binary format is used to represent store addresses, for example.
However, for normal fixed-point binary arithmetic, we wish to have a range of positive and negative values.
In order to represent signed numeric values, we divide the range of available binary patterns into two nearly-equal sections, and use one to represent the positive, and one the negative, values.
Conventionally, those patterns whose left-hand or most significant bit position contains a zero represent positive values, and those where it contains a one   represent negative values.
Thus this left-hand bit position is a sign bit, set to zero for positive, and one for negative, values.
A technique of representing signed numbers for which this is not the case, the excess-value representation, is described in 2.2.
There are three standard methods for representing signed numbers, as shown in Figure 2.1.
These vary in the way that the set of binary patterns is mapped onto the negative numbers; the positive numbers are represented in the same way in all three methods.
In the sign-and-magnitude method, the sign bit is used as described above, while the remainder of the word holds the absolute value of the number in the normal unsigned form.
In the one's complement method, any pattern whose sign bit is one represents the negative of a number obtained by replacing one by zero and zero by one in each bit position.
A difficulty with both these methods is that two different binary patterns represent zero, so that testing for zero becomes slightly more complicated.
In the third method, two's complement, any pattern whose sign bit is one represents the negative of a number obtained by inverting all bits and adding one.
There is now only one representation for zero, but there is now a negative number whose absolute value cannot be represented in the computer word.
All three methods have been widely used, with two's complement as the most common on current computers; we use this convention for the rest of the section, unless stated otherwise.
A basic set of operations
The most common computer organization for arithmetic operations is the one-address system introduced in the Von Neumann computer in Chapter 1.
Thus we have a processor register called the accumulator, the same length as a word of computer storage.
All binary arithmetic operations take place between a value in the accumulator and a value read from the store, and the result is left in the accumulator.
We discuss the various arithmetic operations required for this type of system; modifications for the other systems described in 3.2 should be obvious.
A comprehensive set of basic arithmetic operations to manipulate fixed-point binary numbers consists of the following:(a) Add contents of a store location to contents of the accumulator, leaving the result in the accumulator.
(b) Subtract contents of a store location from contents of the accumulator, leaving the result in the accumulator. (c) Load the contents of a store location into the accumulator, overwriting the accumulator's previous contents.
(d) Store the contents of the accumulator in a store location, overwriting the location's previous contents.
(e) Negate the contents of the accumulator in accordance with the appropriate method of representing negative numbers.
(f) Take the absolute value of the contents of the accumulator, leaving the result in the accumulator.
(g) Clear (i.e. set to zero) the accumulator.
We can, of course, manage with a much smaller set of operations.
The basic instruction set of the DEC PDP-8 computer (Figure 1.11) provides TAD, DCA, CIA, and CLA which are (a),(d),(e), and (g), with (d) in a form which clears the accumulator after storing its contents.
To obtain the effect of the load operation (c), we add into an accumulator already cleared either explicitly by CLA or implicitly by DCA.
The normal store operation is obtained by following DCA by a TAD from the same location.
The subtract operation (A-B) can be obtained by  and the absolute value of A can be obtained by:
Conversely, we may combine two or more of these operations in one more powerful instruction.
The Von Neumann computer, for example, has a range of load instructions to load the accumulator with the contents, negated contents, absolute value of the contents, or negated absolute value of the contents of a store location.
A similar set of variations is provided for add and subtract.
An instruction to clear a store location is occasionally provided, but more general manipulation of store locations (such as "negate contents  of store location" ) is rare.
A number of early computers had an instruction to extract a square root, but nowadays this operation is achieved by software.
Shifting
Since we are considering a binary computer, multiplication and division of the contents of the accumulator by a power of two can be accomplished by shifting the contents to the left or right an appropriate number of positions.
A store address is not required by a shift instruction, so it is normal to use this portion of the instruction to specify the number of bit positions to be shifted.
In a shift operation, a decision has to be taken as to what is to happen at each end of the shifted pattern.
Consider Figure 2.2 where we illustrate the shifting by one bit position of a six-bit binary number using two's complement representation.
Rows (b) and (c) show the simplest form of shift, where zeros are shifted into the vacated end of the pattern, and bits shifted out of the pattern are lost.
This is adequate for left shift, at least as long as the bits shifted out are the same as the sign bit.
However, this form of shift does not work for right shift, and we need arithmetic right shift, shown in row (d).
In this form of shift, we "propagate the sign bit" ; that is, the value shifted into the left-hand bit position is a copy of the sign bit.
Notice that the right shift truncates the result; some computers provide a version of arithmetic right shift which adds to the right-hand bit position of the result the value of the last bit shifted out.
This is shown in row (e) of Figure 2.2; it will be seen that this gives a rounded value as the result of a division by two.
Whatever the method of representing negative numbers, most computers provide the appropriate arithmetic shifts to preserve the numerical validity of the shifted results.
In one's complement format, the basic left shift of Figure 2.2 is inadequate; instead a copy of the sign bit must be shifted into the right-hand bit position (why?).
In sign-and-magnitude format, the basic left and right shifts must be used, but with the sign bit not participating in the shift.
Multiplication and division
There are several possible approaches to multiplication and division in fixed-point binary, all of which have been used at some time.
We could provide no hardware for these operations, so that they must be programmed when required out of the basic steps of addition, subtraction, and shifting.
Alternatively, we could provide multiplication and division only of non-negative numbers, or multiplication but not division, or multiplication together with the operation of taking the reciprocal (as on the Cray-1 super-computer).
However, we will consider the provision of hardware for both operations, the usual situation on modern medium and large computers.
For definiteness we consider multiplication and division of integral numbers held in two's complement format in an n-bit word (so that the numeric value of a word lies in the range -.
The hardware to perform multiplication requires two n-bit registers to hold the multiplier and multiplicand, and a 2n-bit register to hold the product, since it may have up to(2n-1) significant bits.
The basic algorithm shown in Figure 2.3 is the normal pencil-and-paper method, with modifications to deal with negative numbers held in two's complement form.
Initially the partial product is zero.
The multiplier is then scanned from right to left, and each multiplier bit is examined in turn.
If the examined bit is a one, the multiplicand is added to the partial product.
The partial product is then shifted to the right, the sign bit of the multiplicand is copied into the vacated position, and the next bit of the multiplier is examined.
Finally, a correction is required if the multiplier is negative; in this case the multiplicand is subtracted from the partial product to form the final product.
It will be seen from Figure 2.3 that separate registers for partial product and multiplier are unnecessary.
Instead they can occupy the left- and right-hand portions of a single 2n-bit register After a multiplier bit has been examined, it is dropped by shifting the partial product and multiplier together one position to the right.
Notice also how all the additions take place in the left-hand half of this 2n-bit register.
For this reason the 2n-bit register has in the past usually been implemented as an n-bit accumulator extended on the right by a special n-bit register for multiplication (and division, as we shall see).
On the Von Neumann computer this was called the arithmetic register (AR); but it is more commonly referred to as the multiplier-quotient (MQ) or quotient (Q) register.
To multiply two numbers, say A and B, the following sequence of instructions would be performed: Move A to MQ register (load MQ with multiplier) Clear Accumulator (set first partial product, if not done automatically by "multiply" ) Multiply by B The final instruction brings B to a multiplicand register and performs the multiplication, leaving the left-hand portion of the product in the accumulator and the right-hand portion in the MQ register.
It will be seen that extra instructions are now required to load and store the MQ  register, either directly from or to a store location, or via the accumulator; see for example instructions 9 and 10 on the Von Neumann computer (Figure 1.5).
As discussed in 3.2, many modern computers have an array of accumulators.
In this situation the multiply instruction finds the multiplier in one accumulator, the multiplicand in a second accumulator or in a store location, and the resultant product occupies a pair of adjacent accumulators.
The full power of the instruction set is now available for manipulating each half of the product.
Suppose that, in multiplying A by B, the initial partial product is some non-zero value C. Then the final result of using the above multiplication algorithm is AXB+C.
So the same hardware can provide both a "multiply" and a "multiply-and-add" instruction, depending upon whether or not the initial partial product is zero.
The programmer usually requires an n-bit result after multiplication, so an appropriate portion of the product must be extracted for further processing; with the above (integral) convention, the right-hand n bits would be taken.
Some computers have a variation on the multiply instruction which delivers an n-bit result for operands limited in size, or which rounds a 2n-bit result to n bits.
In division we have a dividend N and a divisor D, and require a quotient Q and remainder R such that The divisor, quotient, and remainder are n bits long, and the dividend is 2n bits long.
It will be seen that this is a form of inverse to the "multiply-and-add" operation.
We consider for the moment only the case where His non-negative and D is positive, and assume they are of such a size that overflow does not occur (see problem 2.4).
Then a basic algorithm for division is shown in Figure 2.4, again derived from the normal pencil-and paper method.
Initially N is taken as the first partial remainder.
Then there are n stages, each of which generates one quotient bit and a new partial remainder.
For each stage, the partial remainder is shifted left, and the divisor is subtracted from it to form a new tentative remainder.
If the result is positive, it is the new partial remainder and the new quotient bit is one; otherwise the quotient bit is zero, and we "restore" the previous remainder by adding the divisor to the tentative remainder.
Figure 2.4 has been drawn to show that, as the partial remainder is shifted left, the vacated positions can be used to hold the bits of the quotient.
Thus the 2n-bit register initially holding the dividend will, after division, hold R in the left-hand n bits and Q in the right-hand n bits.
This dividend register will be the register which holds the product in multiplication, made up of the accumulator extended on the right by an MQ register (on a computer with a single accumulator) or a pair of adjacent accumulators (on a computer with an array of accumulators).
A second divide instruction may be provided which automatically rounds the quotient according to the value of the remainder, or accepts an n-bit (instead of a 2n-bit) dividend.
Arithmetic tests
So far we have discussed instructions for manipulating numeric  values to obtain a new value.
A further requirement is to be able to select for execution one out of two or more sequences of instructions depending on a previously calculated numeric value.
On the Von Neumann computer the only conditional jump instruction was one which tested for a non-negative value in the accumulator.
It is difficult to program naturally with such a restricted set of conditional jumps, so it is usual to provide the ability to jump on any of four accumulator conditions zero, non-zero, positive (including zero), and negative — and perhaps on combinations of these conditions.
The only other conditional jump instructions based on arithmetic tests that are at all common are (a) Jump if the contents of a specified store location is zero, usually forming part of an "increment and jump if zero" instruction (see 3.3).
(b) Tests on the equality or otherwise of two values, held either in two store locations, or in an accumulator and a store location.
Overflow
Overflow is the condition in arithmetic calculation when the result of an operation cannot be correctly represented within the computer, without changing the form of representation.
It occurs under the following   lowing conditions for two's complement arithmetic, illustrated in Figure 2.5.
(a) For addition and subtraction, when the carry into the sign bit and the carry out of the sign bit are different.
(b) On negation of the maximum negative number (— 2n- 1 in an n-bit word).
(c) For left arithmetic shift, when the sign bit is changed; rounding errors in right arithmetic shift are not usually considered to be a case of overflow.
(d) Overflow cannot occur on multiplication if the values are assumed to be integral, but multiplication of two operands both of which are the maximum negative number causes overflow under the fractional convention.
(e) For division, when  being the dividend and D the divisor (see problem 2.4).
This includes the case of division by zero.
For sign-and-magnitude and one's complement, there are equivalent conditions for overflow to (a) and (c), condition (e) is the same, and conditions (b) and (d) do not cause overflow.
On some computers, no automatic tests are made for overflow, and the programmer has to incorporate into his program explicit checks for overflow as required.
On early computers which included automatic tests for overflow, a detected overflow condition caused a halt.
On more recent machines, overflow causes a special overflow flag to be set (for later testing by the program) or generates an interrupt, causing entry to an overflow recovery routine.
Multiple-length arithmetic
We have already seen how the operations of multiplication and division introduce the use of double-length operands.
Often the programmer rounds these to single-length values, but in order to retain precision he may wish to manipulate double-length operands (particularly if the word-length is short).
For this reason, some computers have further instructions to deal with such operands, held either in a "double-length" accumulator (the combined accumulator and MQ register, or a pair of adjacent accumulators) or in two consecutive store locations.
Typical double-length instructions are to load and store the double-length accumulator, addition and subtraction; less common are double-length  compare and negate.
Double.length shifts are usually provided, where bits shifted out of one register of the double.length accumulator are shifted into the other.
Double.length multiplication and division are rarely provided, as these would involve quadruple.
length operands.
There may be a requirement for precision greater than that obtained from double.length arithmetic, or double.length arithmetic may not be provided, and in this situation it is necessary to program the basic multiple.length arithmetic operations out of simpler operations.
A multiple.length value is held in a number of consecutive store locations; the left-most bit of all words but the first (which holds the sign bit) may either be used arithmetically or may be ignored.
The hardware designer can simplify the programmer's job in several ways.
The basic requirement is for a carry flag, a one-bit processor register which is set according to whether or not carry occurs out of the accumulator during an arithmetic operation; the carry is out of the left-hand or next to the left-hand bit position, depending on whether the left-hand bit Position participates in multiple.length arithmetic or not.
Notice the difference between overflow and carry; overflow indicates that the result of an arithmetic operation is invalid, while carry always occurs (for example) when adding two negative numbers together.
Given a conditional jump instruction which can test the value of the carry flag, the processing of multiple.length operands can be programmed.
Triple.length addition is illustrated in Figure 2.6.
2.2.
Floating-point arithmetic
In the previous section we mentioned that the programmer using fixed-point binary format could consider the binary point to be at an appropriate position in the word for each item of data.
Operands must then be aligned for arithmetic calculation (where necessary) by shifting, If the programmer stores, with each item of data, a scaling factor indicating the position of its binary point, then an automatic system could perform this alignment.
On early computers sets of subroutines to perform these manipulations were provided, but the task was soon taken over by hardware.
Thus, on most medium and large modern computers, hardware is available (perhaps as an optional extra) to manipulate arithmetic quantities consisting of a numeric value together with a scaling factor, and this is known as floating-point arithmetic.
Floating-point format, then, represents arithmetic quantities in the form , where b is a fixed number known as the base or radix; it is often two, but other values have been used, such as 8 and 16 (for example on the Burroughs B6700 and IBM 370 range respectively).
The numeric value f is called the fraction or mantissa, and the scaling factor e is called the exponent.
To hold a number in this form, the computer word is divided into two unequal portions, the larger to hold the fraction and the smaller to hold the exponent.
These two values are represented in fixed-point binary format, the exponent as an integer and the fraction with the binary point to the left of its left-hand or most significant bit (although, as we shall see, the latter convention is not invariable).
As an example, Figure 2.7 shows the value 6.25 held in floating-point format with base 2 in a 16-bit word, divided into 10 bits for the   fraction and 6 bits for the exponent.
Notice that there are several possible representations of the same value.
For definiteness, and to maximize the number of significant digits in the fraction, we require all non-zero floating-point numbers to be held in normalized form.
This means that the fraction and exponent are adjusted until the fraction (considered as a sequence of digits in the base used by the floating point system) has a non-zero left-hand or most-significant digit.
Thus, for a floating-point system with a base of two, the fraction must have a non-zero most-significant bit, and lies in the range .
If the base is 16, then the most significant hexadecimal digit must be non-zero; i.e. at least one of the four most-significant bits must be non-zero.
In general for a base b, the fraction of a normalized non-zero floating-point number is in the range 1/ b ≤ f <.
So far we have assumed that the fraction and exponent are positive.
However, both must be allowed to take on negative values, so that we can represent negative numbers and normalized numbers with absolute value less than 1 /b.
The fraction can be represented as a signed number by any of the methods discussed in the previous section.
The most common method of representing the exponent is the excess-value or biased representation, where a fixed value of 2k-1 (where the exponent field has k bits) is added to the exponent.
The result is a positive number (sometimes called the characteristic), which is then represented as an unsigned fixed-point binary number.
For example, for a 6-bit exponent, we use an "excess-32" representation; the exponent is in the range -32 to +31, and to obtain the characteristic we add 32 to give a number in the range 0 to 63.
Any floating.point number with a zero fraction represents zero.
However, the standard or normalized representation of zero (true zero) has a (Positive) zero fraction and the smallest possible exponent.
For an exponent held in excess-value representation, this means that true zero is represented by a word with all bits set to zero.
Figure 2.8 illustrates a complete (hypothetical) floating-point format with base 2 in a 16-bit word.
One bit holds the sign, 6 bits hold the exponent in excess-32 representation, and 9 bits hold the fraction in sign-and.magnitude representation.
For further discussion of floating-point arithmetic see Knuth (1969) and Stein and Munro (1971).
Floating-point operations
The instruction set for operating on floating-point numbers includes the four arithmetic operations of addition, subtraction, multiplication, and division (usually without remainder); negate and clear (set to zero) operations may also be provided.
If the floating-point accumulator is separate from the fixed-point accumulator, then there will be appropriate load and store instructions; otherwise the fixed-point load and store instructions are used.
In the former case there may, as with fixed-point arithmetic, be instructions to load the negated contents, absolute value of contents, or negated absolute value of contents of a store location into the accumulator.
We need to be able to test for floating.point zero, or for a positive or negative value, and change the instruction sequence accordingly.
There is a problem here of the multiplicity of representations of zero, so such a test is usually only for true zero.
It has been suggested (Knuth 1969, pp. 199–201) that testing for floating.point zero is not appropriate, and that a more suitable test would be for any value in a small range about zero, the size of which could be set by the programmer.
Similarly, if a floating.point compare instruction is provided, it should test not for equality but for an absolute difference lying within this range.
Standard floating.point instructions normalize the result of each arithmetic operation; this is known as post-normalization.
An additional set of floating.point instructions is sometimes provided which does not post-normalize the result of an operation.
Use of such a set of unnormalized operations may give a better indication of the accuracy of an arithmetic calculation.
Overflow and underflow
As with fixed-point arithmetic, the results of some floating-point operations cannot be represented in the standard format.
We define the two conditions of (exponent) overflow and underflow, when the exponent becomes respectively too large or too small to be represented in floating-point format.
These conditions usually result in a flag being set, or an interrupt to a recovery routine being taken.
In some computers underflow in an operation forces a true zero result with no error indication.
A separate floating-point error condition may be signalled on loss of significance (i.e. a zero fraction) in addition or subtraction.
On the CDC 6600 computer certain values of the floating-point exponent are set aside to represent zero, infinite, and "indefinite" operand values, and rules are given for arithmetic operations on these quantities (for example, the difference of two infinite operands is indefinite).
Variations on Floating-point format
We have assumed that the exponent and fraction are packed into one computer word.
In this case the programmer is forgoing some of the precision of single-length fixed-point arithmetic in exchange for an   increased range of values.
However, it is common (especially on computers with short word-lengths) for a pair of consecutive store locations to be allocated to each floating-point value, to allow increased Precision.
The floating-point accumulator must now be double-length, and operands are specified by the address of the first store location in the pair.
In some computers both a short and a long floating-point format and instruction set are provided, so that the programmer can trade precision against storage space.
Indeed the IBM 370 range of computers has three different formats for floating-point operands, as illustrated in Figure 2.9.
It is usual for the extra length in the long format to be used to extend the precision of the fraction, while the range of exponents is the same in all formats.
In all the above we have assumed that the numeric value f of a floating-point quantity is a fraction, in the range  when normalized.
This is by far the most common technique, but on some computers f is considered to be an integer with a binary point on the extreme right; in this case we use the term mantissa, rather than fraction, for f.
Perhaps the most interesting example of this is on the Burroughs B6700, illustrated in Figure 2.10.
On this computer there is no fixed-point arithmetic, and all arithmetic is done in short or long floating-point format.
In short floating-point format the mantissa f is integral; since both exponent and mantissa are held in sign-and-magnitude form, integers are simply short floating-point values with an exponent of zero.
In long floating-point format the binary point lies between the portion of the mantissa in the first word and the portion of the mantissa in the second word.
If floating-point hardware is not provided on a computer, then instructions may be provided to help with the programming of floating-point arithmetic subroutines.
This usually takes the form of a special left shift instruction to simplify normalization, sometimes called "shift and count" or (more unfortunately) "normalize" .
For example the extended arithmetic element on the DEC PDP-8 has a "normalize" instruction which shifts the accumulator and MQ register left (inserting zeros at the right-hand end), until the most and next most significant bits of the accumulator are different.
The number of shifts performed is placed in an accessible register called the step counter.
Figure 2.1 I illustrates the action of this instruction.
2.3.
Logical operations
Consider the concept, common in high-level languages, of the boolean variable, that is, a variable which can take only two values (true and false) and upon which the usual boolean functions — and, or, etc.ban be Performed.
There is an obvious correlation between a boolean variable and a single bit position in a computer word, with one representing true and zero representing false.
The boolean functions which can be applied to a single bit position can then be extended to what are called logical operations on the whole word.
A logical operation is one which treats all bits of the word similarly and independently; there is no carry from one bit to another, nor is there an underlying implied interpretation of the word as a numeric quantity.
The most common use of these logical operations is to extract, test, and alter selected parts of words of other data-types; for example, an integral fixed-point number may be tested for odd or even by extracting and testing the right-hand bit (at least in two's complement or sign-and-magnitude representation).
They can, of course, also be used to manipulate boolean variables.
Basic logical operations
It is most common to find the following three logical operations between the contents of the accumulator and the contents of a store location.
They are illustrated in Figure 2.12 for a 16-bit word.
(a) And or logical conjunction.
Here each result bit is set to one only if the corresponding bits in the two operands are both one.
This operation can be used for extracting fields of interest from a word by "masking" .
For example, Figure 2.12 (a") illustrates the extraction of the bottom four bits of a word for further processing.
This operation can also be used to clear a specified bit of a word to zero, as shown in Figure 2.12 (a").
(b) Inclusive Or or logical disjunction (sometimes referred to simply as Or).
Here each result bit is set to one if either (or both) of the corresponding bits in the two operands are one.
This operation can be used to set a specified bit of a word to one, as shown in Figure 2.12 (b').
(c) Exclusive Or or non-equivalence.
Here each result bit is set to one if either (but not both) of the corresponding bits in the two operands are one.
Exclusive Or can be used to invert a specified bit of a word, as shown in Figure 2.12 (c').
We also need the operation.of logical complement or Not, by which each zero bit becomes a one and each one a zero.
Notice that this is the same as negate or arithmetic complement under the one's complement representation.
Since logical values are manipulated in the accumulator used for fixed-point binary arithmetic, no separate load and store instructions are required.
There are 16 possible boolean functions of two variables.
As shown in Figure 2.13, these range from trivial ones such as merely copying the first operand to the more useful ones discussed above.
Sometimes a logical instruction is provided which allows specification (in a sub-operation field) of any of these 16 operations, to be performed on the two operands of the instruction.
However, it is only rarely that the more esoteric operations are ever used, particularly in programs compiled from high-level languages.
Masking
Masking is such a common operation that many computers provide at least one arithmetic or logical instruction which can specify a mask as well as the primary operands.
Only those bit positions corresponding to a one in the mask are involved in the primary operation.
The mask may be held in an implied processor register (for example accumulator zero in a computer with an array of accumulators) or in a store   location specified in the instruction.
The most common instructions in this area are load, store, and move under mask, where bits are copied from the source to the destination location only for those positions corresponding to a one in the mask.
The remaining bit positions in the destination are either all left unchanged or all set to zero (the latter being equivalent to logical "And" ).
The two possible forms of operation are shown in Figure 2.14.
Logical shifts
A common logical operation is logical shift left or right.
As with arithmetic shift, each bit is moved a number of positions to the left or right.
However, all bits participate equally, and there is no overflow testing or rounding correction.
There are two forms of logical shift, which differ in what happens at each end of the binary pattern.
In the basic form (shown in rows (b) and (c) of Figure 2.2), zeros are shifted in at one end while bits are lost at the other; in the second form (called rotate or circular shift) bits shifted out of one end of the pattern are shifted in at the other.
These are shown in Figure 2.15.
Some computers provide both logical and circular shifts, together with the appropriate arithmetic shifts.
Others provide only logical or only circular shifts, and arithmetic shifts must be constructed from these basic operations.
For example, the DEC PDP-8 computer provides only one- and two-bit circular shifts.
As before, further shift instructions may be provided for double-length operands.
Logical tests
Many computers have an instruction which is a variation on the operation of scanning the bit positions of a computer word.
In the simplest case an instruction tests for all, some or none of the bits being set to one.
This could be extended by the incorporation of a mask.
An example of such an instruction is TM (test under mask) on the IBM 370 range, which sets the condition code (see 3.3) to indicate the presence of none, some, or all bits set to one in a masked portion of a byte.
An alternative approach is for the instruction to scan from (usually) the most-significant to the least.significant end of the word, searching for the first bit set to one, and placing the number of that bit position in a suitable register, Notice the similarity between this operation and the "shift and count" instruction discussed in the last section.
A further possibility is an instruction to count the number of one bits in a computer word, and place the result in a suitable register; this is useful in the calculation of parity.
Strictly, the only possible logical comparison between two operands is for equality or Inequality.
However, some computers (such as the IBM 370 range) have what are called logical comparison instructions, which turn out to be unsigned fixed-point binary comparisons.
2.4.
Data transfer, part-word, and multiple-word operations
In the first three sections of this chapter we considered a word-oriented, single-address computer with one accumulator.
We studied the operations required by those dominant data-types which act on a  complete word: fixed and floating-point binary arithmetic, and logical values.
Among these operations we can distinguish several examples of data transfer operations, such as loading and storing the accumulator.
It is possible to envisage a general data transfer instruction (or set of instructions) which would move a word, a portion of a word, or several words from a source to a destination (either processor registers or store locations).
In fact, however, it is not economic to provide such generality, and the situation described in the earlier part of this chapter is typical.
A small number of data transfer instructions will be provided, integrated with the computer's dominant data-types, and making available only those transfer paths which are expected to be heavily used.
Other paths can be programmed out of sequences of those provided, together with logical operations where necessary for masking, etc.
In this section we briefly discuss the forms of data transfer instruction commonly found on word-oriented computers: first for whole-word transfers, then for the larger subdivisions of the word (such as the half-word), and finally for multiple-word transfers.
We omit discussion in this section of the subdivision of a word into the smaller units of those other dominant data-types, the character and the decimal digit.
Consideration of those data-types, in the next section, leads us to introduce a different form of computer architecture, the character or byte-oriented computer, where the basic storage unit is much shorter than the word (typically eight bits long) and all data-types are multiples of this unit.
Single word transfers
The instructions provided in this category depend heavily on the major data-types available and the addressing structure of the computer.
(a) On a single-address computer with one accumulator we find load and store instructions, perhaps with variations which interpret the value being transferred (for example, load absolute value).
(b) If the computer has more than one processor register, then instructions are necessary to transfer values to and from these registers, the source or destination of the value being either another processor register or a store location.
Instructions 9 and 10 on the Von Neumann computer  (Figure 1.5) are examples of such instructions.
Notice that, if a computer has an array of accumulators, as discussed in 3.2, then the two instructions: Load accumulator n from store location X and; Store accumulator n in store location X are logically sufficient.
But this means that data transfer between processor registers is slowed down to the speed of store access.
Hence an instruction of the form;
Load accumulator n from accumulator m is usually provided.
(c) To transfer a value from one store location to another in a one-address computer, the programmer uses a pair of load and store instructions, with an accumulator as an intermediate location.
On a two-address computer a "move" instruction which specified the address of both store locations would be available.
(d) Interchanging the contents of two words (whether processor registers or store locations) requires a temporary location 7, and the sequence of instructions;
Move A to 7 Move B to A Move T to B For this reason some computers provide an instruction to exchange the values, either in an accumulator and a store location (for example the DEC PDP- 10), or in two processor registers.
Part-word operations
We now consider how provision may be made for manipulating subdivisions of a word.
(a) First, notice that explicit part-word instructions are not logically  necessary.
The programmer can always extract the relevant portion of a computer word by masking, using either an "And" instruction or a "Load under Mask" instruction if available.
However, there are obviously time overheads for this packing and unpacking.
(b) Certain portions of a computer word have special significance for particular data-types: for example the exponent field of a floating-point number, and the operand field of an instruction.
A computer will occasionally have an instruction which extracts a value from, or inserts a value in, that part of a specified computer word.
An example of the latter is instructions 18 and 19 on the Von Neumann computer (Figure 1.5).
(c) On a computer with a long word-length, it is wasteful to store small fixed-point binary numbers one to a word.
Instead, the programmer can store two such numbers in a word, each in a half-word.
Instructions may therefore be provided to facilitate the manipulation of such a data-type.
Figure 2.16 lists the instructions provided on the IBM 370 range of computers to manipulate half-word values.
In each instruction but   STH, the source is a specified half-word (16 bits) in the store, considered as a fixed-point binary value.
This value is converted to a full-word (32 bits) by extending the sign bit to the left, since two's complement representation is used.
The resultant value is then manipulated in the appropriate way: for example, AH causes the 32-bit value to be added to the appropriate 32-bit accumulator.
For STH the least significant half of the specified accumulator is deposited in the specified half-word location in store.
Since the IBM 370 is a byte-oriented computer, the half-word store locations are specified by their byte address, as described in the next section.
Figure 2.17 illustrates the half-word data transfer operations on the DEC PDP- 10 computer, together with the variations which specify what action is to be performed on the other half of the destination word.
Each operation can be used with the source as a store location and the destination as an accumulator (i.e. as a load instruction) or the converse (i.e. as a store instruction).
Notice that, since the PDP-10 is a (36-bit) word-oriented computer, the operand field specifies a word, and the half-word is specified in the operation code.
On this computer the right-hand half of each instruction is occupied by a store address, so that these instructions provide a generalized version of the facility   mentioned in (b) above.
The PDP-10 also has an instruction which interchanges the two halves of a word during transmission, and one which adds one simultaneously to both halves of an accumulator.
(d) We could, of course, extend our subdivision of computer words even further.
On the Univac 1108 computer, for instance, a 36-bit word can be subdivided into halves, thirds, quarters, or sixths, and a selection field in the instruction specifies which portion of the store location is involved in any arithmetic operation.
The selected portion is aligned with the least-significant portion of the other operand (held in a processor register), and sign extension may or may not be applied (depending on the size of the portion and the value in the selection field).
On this computer simultaneous addition or subtraction may also be performed on corresponding halves or thirds of two words, one in a processor register and one in a store location.
A few computers have provided instructions to operate on any single bit, or on any contiguous group of bits, in a word.
A problem with such instructions is the number of bits required to select a word, a bit within the word, and (in the latter case) the length of the bit group.
For this reason, and because of the limited use for such general data-types, the only data-types smaller than a word which are at all commonly provided for in the instruction set are half-words, characters, and decimal digits.
Any other subdivisions can be accessed and manipulated by the logical operations of 2.3.
Multiple-word operations
Sometimes the programmer wishes to manipulate a group of contiguous words as a single unit.
The only common instructions provided to help are:(a) The double- or multiple-length arithmetic operations discussed in 2.1 and 2.2.
(b) The block transfer instruction.
This instruction specifies a number of words to be moved n, a source, and a destination address.
Then the n words beginning at the source address are transferred to an area beginning at the destination address.
This obviously allows speedier movement of blocks of data than a programmed loop containing a single-word move instruction (or a load and store pair).
In computers  fitted with mass store (see 5.4), such a block transfer instruction may be the only means of moving data between it and the main store.
(c) The multiple accumulator load and store instructions.
These load or store a set of processor registers (typically a sequence of accumulators in a computer with an array of accumulators) from or to a consecutive sequence of store locations.
They are used for rapid saving and restoring of the "process context" on entry to and exit from a subroutine, or in processor mode changes (see 3.5).
2.5.
Decimal Arithmetic and Character Handling
In 2.1 and 2.2 all arithmetic calculation is performed with operands in binary format; that is, to base two.
However, outside the computer, numerical data is generally held in decimal format, for obvious anatomical reasons (although there have been suggestions from time to time that manual calculation should be done in binary or octal, for example, Phillips 1936).
There are two ways of coping with this disparity.
Either the computer converts the initial data from decimal to binary and the final results from binary to decimal, performing all internal calculations in binary, or a data-type and instruction set is provided to perform arithmetic operations directly in decimal.
The first method is appropriate if large amounts of calculation are to be performed on the input data, since the cost of conversion is more than out-weighed by the increased speed and compactness of binary arithmetic.
The second method is appropriate when only a small number of arithmetic operations are to be performed on each of a large number of initial items of data.
Generally the first method is used for scientific, and the second for commercial, data-processing.
Word-oriented decimal arithmetic
A few (mainly early) computers provide word-oriented decimal arithmetic operations.
Each operand is a computer word (either in the accumulator or a store location), made up of a fixed number of decimal digits.
The basic hardware of such a computer is still binary, so that each computer word is, in fact, divided into a fixed number of equal length fields of bits, each representing one decimal digit.
There are a number of methods of encoding a field of bits to represent the ten decimal digits, four of which are shown in Figure 2.18; for   further discussion see Richards (1955).
The most common is the binary-coded decimal representation (BCD).
Here 10 of the 16 possible bit patterns of a 4 bit field are used to represent the decimal digits 0 to 9, the remaining 6 being unused.
Any of the 3 representations for negative numbers could be used, sign-and-magnitude being most common.
All the usual arithmetic operations of addition, subtraction, multiplication, division, and comparison are provided, as in 2.1.
Only decimal arithmetic may be provided, or separate instruction sets might treat each computer word as either a binary or a decimal value.
A few of these computers also have a decimal floating-point format.
Here the fraction is held as a series of decimal digits, with a floating.
point base of ten rather than a power of two; normally the exponent is still held in binary format.
Few modern large or medium.sized computers have such word-oriented decimal arithmetic.
It is more usual nowadays to provide decimal arithmetic (if at all) on a data.type derived from character string handling.
It is to such character string formats that we now turn.
Character-oriented computers and the IBM 1401
The word-oriented architecture derives from viewing computers as calculating devices.
Large numbers of calculations are to be done, so that the design aim is to choose a word size which gives suitable precision, and to do all arithmetic in the most efficient way, using fixed or floating.point binary format.
On this view transput will be relatively infrequent, so that the cost of converting to and from decimal format is acceptable.
However, in commercial applications the emphasis is different, as a glance at the Cobol language indicates.
The basic unit is the (variable-length) string of characters.
The manipulations performed may include arithmetic operations on some of these character strings, but we will probably wish to do them directly on the decimal digits, rather than convert to and from binary for each arithmetic operation.
We therefore consider building an architecture round character string manipulation, rather than round word manipulation.
The first common computer using this character.oriented architecture was the (second.generation) IBM 1401 (McCracken 1962, Bell and Newell 1971, pp. 225–34).
The basic addressable unit of storage on the IBM 1401 computer is the character, which holds eight bits.
Six data bits encode a character set consisting of the digits 0 to 9, the letters A to Z, and a number of special symbols (such as blank and comma), as shown in Figure 2.19.
The seventh bit is a parity bit, and the eighth is known as the word mark bit; by the usual definition, this computer therefore has a 7-bit word.
Each operand is a string of one or more consecutive characters.
In order to facilitate arithmetic, an operand is specified by the address of its right-hand (least-significant) character, and the left-hand (most-significant) character of the operand is indicated by having its word mark bit set to one.
Because operands may be very long, no accumulator is provided on this computer; all operations take place between operands in the store.
The most basic instruction is the move, which copies characters from a source field to a destination, until a character with the word mark bit set is encountered in one of the two fields.
For example, suppose we had two fields (or character strings) in the store at addresses 300 and 350, as shown in Figure 2.20 (a); the addresses are those of the right.hand characters of the fields, and the characters whose word mark bits are set are indicated by underlining.
Then if the computer executes the instruction, Move Characters to Word Mark: Source Address 300: Destination Address 350 the result will be as shown in Figure 2.20 (b).
Characters have been   copied from the first field to the second, until a word mark (here in the source field) is encountered.
This instruction does not affect the pattern of word marks in the destination field, but there are other instructions to set or clear the word mark bit in a designated character, to clear all word marks in a designated area, or to copy a source string and word mark to a designated area (ignoring and clearing any word mark: bits in the destination field encountered in the process).
To describe how arithmetic is performed on the IBM 1401, we must look at the representation of characters a little more closely.
The six data bits of a character are divided into two zone bits and four numeric bits.
All six bits are used to represent most symbols in the character set, but it can be seen from Figure 2.19 that the digits 0 to 9 are represented  by the four numeric digits, with the zone bits set to "00" .
However, the zone bits of the right-hand (least significant) digit of a numeric field are set to "10" for a negative value and to any other pattern (00,01, or 1 I, but usually 1 I) for a positive value.
Thus numeric operands are held in sign-and-magnitude form, with the sign on the right, since operands are processed from right to left.
As an example we see from figure 2.19 that 103 would be represented in a four-character field (ignoring word mark and parity bits) as 
A consequence of this technique of representing negative values is that some bit patterns have two interpretations; thus "100011" represents the character "L" as well as 3 and a "minus" sign.
Optional or standard instructions are provided for the four arithmetic operations.
For example, the "add" instruction adds two numeric operands in the above format, leaving the result in place of the second operand.
The operands may be of different lengths; if the first operand is shorter, the computer processes it as if it was extended on the left with zeros; if the second (destination) field is shorter, some of the more significant digits of the result are lost.
There is an instruction which compares two fields for equality or non-equality, and in the latter case it indicates which field is greater.
There are further instructions to test a specified character for a set word mark bit, or for various combinations of zone bits.
The Honeywell H200/2000 range of computers (Honeywell 1965; Flores 1969, pp. 194–227), which was derived from the IBM 1401, provides similar but enhanced facilities.
One enhancement is to provide two mark bits in each character, an "item mark" and "word mark" bit; if both bits are set this is called a "record mark" .
We can thus build up a hierarchical data structure in storage, of fields of characters (terminated  by a word mark) grouped together in items or records.
Most instructions operate on fields, but a special move instruction allows movement of a group of fields as an item, and the record is the unit of transfer in transput devices.
Further data formats are also provided, for variable length fixed-point binary arithmetic and fixed-length floating-point binary arithmetic.
The Byte-Oriented Computer and the IBM 360/370 Architecture
The two lines of computer development, represented by the word- and character-oriented architectures, have coalesced in the byte-oriented computer.
Here we have a design based on the character, providing the character string handling requirements of the commercial application, with a word-oriented architecture superimposed upon it, to allow the more efficient binary arithmetic to be used when required.
This type of computer is typified by the IBM 360 and 370 ranges, and a very similar architecture is used for the RCA Spectra 70 range and English-Electric (later ICL) System 4 range of computers.
The basic unit of storage is the 8-bit character or byte; unlike the IBM 1401, all eight bits are available for holding information, there being no word mark bit and the parity bit being a ninth (hidden) bit.
Because there is no word mark bit, the length of each operand is specified or implied in the instructions which manipulate it, which means that there is an upper limit to the length of such operands.
There are two data types involving byte-string operands.
The first treats the operand as a string of 8-bit characters (allowing a set of 256 characters, as shown in Figure 2.21), or a string of bytes of eight logical bits.
There are instructions to move strings, to compare them, and to perform the usual logical operations.
However, arithmetic cannot be done on these byte strings as they stand; instead they must be converted to a different string data-type, known as packed decimal.
It can be seen from Figure 2.21 that the bit patterns representing the digits 0 to 9 as characters all have binary "1111" in the left-hand four bits of the byte, with patterns "0000" for decimal 0 to "1001" for decimal 9 in the right-hand four bits.
In order to convert a numeric character string to packed decimal format, the left-hand four bits (known as the zone bits) are stripped off each byte, and the remaining sets of four (BCD coded) numeric bits are packed two to a byte, to form the resultant packed decimal byte string.
As on the IBM 1401 computer, the zone bits of the right-hand (least significant) digit of a numeric character string usually represent the sign  ( "1101" for negative, "1111" or "1100" for positive, values).
In packed decimal format the right-hand four bits of the least-significant byte do not hold a digit, but instead hold a pattern to represent the sign of the number.
This is illustrated in Figure 2.22.
Instructions are provided for addition, subtraction, multiplication, division (with a remainder), and comparison of such packed decimal operands.
Provision is made for dealing with operands of different lengths (up to a maximum of 31 digits), the shorter operand being conceptually extended on the left with zeros.
Although most processing of arithmetic fields is from right to left, the operand is specified by the address of its left-hand byte; since the instruction specifies the length of each packed decimal operand, the address of the right-hand byte is easily obtained.
A word on these computers is defined to be a set of four consecutive bytes (32 bits), and is specified by the store address of its left-hand byte.
As on a normal word-oriented computer, 32-bit accumulators are provided.
The instructions for binary arithmetic refer to these accumulators and to the contents of a 32-bit word whose left-hand byte is specified in the instruction.
Notice that a set of four consecutive bytes could be treated as a word containing a 32-bit binary pattern upon which word instructions can operate, or as a byte string to be operated on by the byte string instructions, or (possibly) by the packed decimal instructions.
In fact there are on these computers at least four word-oriented data-types; instructions are provided for fixed-point binary arithmetic on 16-bit half-words (2 consecutive bytes) as well as 32-bit words (or full-words), and for floating-point arithmetic on 32-bit words and 64-bit double words (8 consecutive bytes).
Floating-point arithmetic on quadruple words (16 consecutive bytes) is also available on some models.
On the IBM 360 range restrictions are placed on which sets of consecutive bytes may be considered to form a word-oriented unit; thus the left-hand byte of a half-word must be at an even address, that of a full-word must be at an address divisible by four, and so on.
These alignment rules mean that such computers do not in fact operate as pure byte-oriented machines, and are a consequence of a way in which the architecture of the computers has been implemented.
These restrictions have been removed on the IBM 370 range, but operand alignment at appropriate addresses is still recommended for efficiency.
Character handling on word-oriented computers
Most modern computers attempt to provide facilities for manipulating strings of characters and possibly decimal digits.
We have already seen how this is done on a byte-oriented computer.
On a word-oriented computer the word is interpreted as an integral number of characters, and facilities are usually provided for transferring an individual character between a string and an accumulator, where it can be manipulated (for example, as one or more decimal digits), by the normal logical and (binary) arithmetic instructions.
We illustrate a typical situation in Figure 2.23; this is a simplified version of the facilities available on the DEC PDP-10 computer.
A 36-bit word holds six 6-bit characters, and a character string consists of a number of consecutive words.
In order that character strings can be accessed, another computer word holds a descriptor or pointer to an individual character: it contains the store address of a word and the position (0 to 5) of a character within the word.
The effects are shown of the three basic character.handling instructions, each of which specifies the address of a descriptor word in its operand field.
 "Load character" extracts the character referred to by the descriptor, placing it in the least-significant end of the accumulator (clearing the rest of the accumulator); "Deposit character" stores the character from the least-significant end of the accumulator at the position referred to by the descriptor (leaving the rest of the string unaffected).
 "Increment   descriptor" makes the descriptor refer to the next character in the string.
In this example we have assumed a character size of 6 bits.
In fact on the PDP-10 the descriptor contains a field specifying the character length, allowing it to be anything from one to 36 bits long.
Some computers provide an extra field in the descriptor to specify the number of characters in the string still to be processed, and this would be decremented by the "Increment descriptor" instruction.
We have implied a clear distinction between word- and character.
oriented computers, as is the case for large and medium.sized computers.
But in minicomputers, with short word lengths, the distinction is not so clear.
For example the DEC PDP- Il minicomputer has a 16-bit word and array of accumulators: however the basic store unit is the half-word or (8-bit) byte, and there is a set of byte-oriented instructions.
On several microprocessors (such as the Intel 8080) the basic unit of storage and manipulation is the 8-bit byte.
Two operations on characters strings which are particularly common are translation and editing.
Many computers therefore provide  instructions to execute these operations on character strings, whether they are implemented as bytes or as subdivisions of words.
The translate operation
The translate operation is used to convert a string from one character set to another.
There are three operands, a source and a destination character string, and a translation table with one entry for each possible binary bit pattern that a character can hold.
The characters of the source string are examined one-by-one.
If the ith character has binary value vi, then the ith character position in the destination string is loaded with the vith entry in the translation table.
This is illustrated in Figure 2.24, which shows the translation of an (8-bit) character   string from the IBM 1401 code (see Figure 2.19) to the IBM 370 code (see Figure 2.21).
As it stands, this operation requires three addresses, but it can be made into a two-address operation by making the source and the destination strings the same; thus the vith entry in the table replaces the ith character in the source string.
A variation on this is to hold in each entry in the translation table, not only the value to be placed in the destination string, but also the address of a new translation table to be used for the next source character.
If the address of the current translation table is always specified, we have the simple translate operation described above; however, now a new table can be selected (for example) after meeting a shift character.
The edit operation
The edit operation is used to prepare a numeric value for printed output.
A numeric value stored inside a computer (after conversion if appropriate) consists of a series of decimal digits.
There are several things we would like to do to improve the readability of this value:(a) Leading zeros should be suppressed and replaced by blanks.
Occasionally we wish to replace leading zeros by some other character, usually an asterisk, to guard against manual alteration of the value; this is known as "cheque protection" .
(b) Commas, decimal points, or other separators may be inserted.
(c) A sign character (or some other character, such as a currency symbol) may be inserted immediately before the first significant digit of the value, or some sign indication (such as "DR" and "CR" ) may be placed immediately after the value.
Some or all of these operations are performed by the edit operation, which has three operands; the location of the numeric value to be edited (a string of decimal digits), the destination string to hold the final edited characters, and an editing pattern giving details of the editing to be performed.
The editing pattern consists of one entry per character position in the destination string: each of these specifies insertion either of a designated character or of the next character from the source string, and this may depend on the sign of the value being converted or on whether the first significant digit has yet been reached.
Figure 2.25 describes and illustrates the IBM 370 edit instruction.
The source, destination, and editing pattern are all byte strings, and the editing pattern has four possible entries: digit selector (DS:binary 00100000), significance starter (SS: binary 00100001), field separator (FS: binary 00100010), and message character (MC: any other binary pattern).
The fill character is the first entry in the editing pattern.
This operation may be put in two-address form by placing the editing pattern in the destination string (where it will be overwritten), or inserting it as a string of "micro-operations" in the instruction stream.
The edit operation may also provide the address of the first significant digit, to facilitate insertion of a special character immediately before this digit, as mentioned in (c) above.
2.6.
A multiplicity of data-types
Most computers have at least two representations for numeric data, for example a fixed-point binary format in which arithmetic is performed, and a character string format used by transput devices.
Some computers have a large number of different numeric data.types.
For example, the IBM 370 range has up to seven: character string and packed decimal; half- and full-word fixed-point binary; and short, long, and extended floating.point binary.
It is possible to write subroutines which convert any numeric value from one format to another, but all but the smallest computers provide instructions to facilitate the conversion between at least some of their data-types.
For example the IBM 370 range has instructions to convert between (numeric) character strings and packed decimal strings, and between packed decimal strings and 32-bit binary numbers; the edit instruction described earlier is also a form of conversion instruction.
The PDP- 10 computer has instructions for conversion between fixed and floating.point binary.
When a computer has a number of data.types, many operation codes are used up in providing the same arithmetic operation for operands with different formats.
Thus, on the IBM 370 range, 6 different "add" instructions are required, one for each data format except character string; in fact 15 different "adds" are provided, including those for unnormalized floating-point and unsigned fixed.point arithmetic, and those for the case where both operands are held in accumulators.
This may restrict the set of operations that can be provided for some or all of the data.types; it also makes it easy for the programmer to use an instruction on an operand with the wrong format, although assemble-time checks may help to reduce the number of errors of this sort.
One obvious way to minimize this problem is to reduce the number of different data formats.
Thus the Burroughs B6700 computer provides no fixed-point arithmetic and uses (short or long) floating-point arithmetic instead, as discussed in 2.2.
Alternatively, a single set of arithmetic instructions can be provided, and a processor mode register specifies what format the operands can be expected to be in.
This is an acceptable solution if we expect to concentrate on one form of arithmetic for a number of instructions, but if we are using several forms (for example, floating-point calculations with address computation in fixed-point format) then we have to change the contents of the mode register continually.
A few computers use variations on this method.
Tagged architecture
The use of a mode register reduces the number of operation codes required, but it does not eliminate the problem of an instruction being given operands of one format when it expects another format.
This problem can be solved by storing an indication of the data format in a special "tag field" along with each item of data.
Then only one instruction is provided for each arithmetic operation.
If an "add" instruction is being executed, the hardware of the computer examines the tag fields of the two operands, and selects the correct form of "add" , whether the fixed-point, short or long floating-point, decimal digit string, or some other data-type.
If the operand formats are different, or one operand is unsuitable for arithmetic manipulation, then the hardware can automatically invoke a conversion operation or (more likely) signal an error condition for treatment by software.
Such a tagged computer architecture (Iliffe 1968; Feustel 1973) can obviously be extended.
A simple extension is to provide a particular value of the tag field which does not specify a data format, and which always causes an error condition to be signalled whenever an operand is accessed with this value in the tag field.
This facility can be used to indicate individual items in a set of data that have to be treated in a special way, different from the majority of items in the set; a special case is "undefined" items, which have not as yet been assigned a value.
We can further extend the scheme into the non-numeric parts of the computer, by providing tag field values for all data formats, for instructions, and for descriptors; that is, computer words holding information about areas of the store (we have already met these in 2.5).
This would form the basis of a scheme for protecting all words of store from being manipulated by instructions which are not defined  for the relevant data format.
Two disadvantages of such techniques are that each word has to have a tag field added to it (and this may need to be quite long to hold a suitable range of values), and execution of each instruction becomes more complicated (and therefore possibly slower).
However, these disadvantages are unlikely to be great in the light of current developments in technology.
The use of tagged architecture is limited to research projects at present, although the Burroughs B6700 computer makes some use of tagging.
Problems
2.1.
Show how the following numbers would be held in a 16-bit word in the sign-and.magnitude, one's complement, two's complement, and excess-value representations: 0, + 1, + 2, + 257, + 32767, + 32768.
2.2.
Suppose that the right-hand bit of a 16-bit word represents 2 -15 instead of 2, so that there is an implied binary point immediately to the right of the sign bit.
If the two's complement representation is used, show how the following numbers would be held: 0, + 1, +0.25, +0.171875.
2.3.
On page 37 an algorithm is given for the signed multiplication of integral two's complement n-bit numbers.
Rewrite this as a procedure in a suitable high-level language.
How would the algorithm need to be altered if numbers were stored in fractional form (as in problem 2.2)?
2.4.
The division algorithm described in 2.1 is restricted to the case of a non-negative dividend, a positive divisor, and no overflow.
Extend the algorithm to deal with a general dividend and divisor, and include a test for overflow (i.e. when the quotient cannot be represented in n bits).
Assume the dividend and divisor are integral, in two's complement representation, For definiteness, assume that the remainder is zero or has the same sign as the dividend (see Stein and Munro 1971, Chapter 6 for more details).
2.5.
Devise a suitable representation for signed multiple-length binary integers for a computer to which you have access, and write subroutines to negate, add, and subtract such operands.
Write further subroutines to implement any other operations which you consider would be useful (for example multiplication and division).
2.6.
Make a list of conditions causing overflow under the sign-and-magnitude and one's complement representations, in a similar form to the list on page 42.
2.7.
Show how the following numbers would be held in normalized form in the floating-point format given in Figure 2.8:0, + I, 128 +0.171875.
2.8.
Why may unnormalized floating-point arithmetic give a better indication of accuracy than normalized arithmetic?
(Hint: consider the operation A — B, where A and Bare nearly equal).
The IBM 7030 Stretch computer (Buchholz, 1962) has a "noisy" mode of floating-point arithmetic as well as the standard mode.
In noisy mode the final bit shifted in on normalization is one (instead of zero, as in the standard mode).
How could this be used to assess the accuracy of a floating-point calculation?
2.9.
Show how the "normalize" instruction described on page 48 would be used to convert a two's complement binary number to a fraction and exponent for use in floating-point arithmetic.
2.10.
Show how the Exclusive Or operation can be used to (a) clear a word,(b) interchange two words without using an intermediate location, and (c) add two binary numbers (And and shift may be used in this last part, but not of course any arithmetic operations).
2.11.
Suppose we have a store word which describes the status of a number of conditions, with each of which a subroutine is associated.
Thus if bit i is set to one, condition i is true and subroutine i is to be entered.
In 2.3 an instruction is described which establishes the position of the most-significant bit set to one in a word.
Show how this can be used (with a jump table) to cause entry to the appropriate subroutine in the above situation.
Rewrite the code if the instruction jumps to address X+i, where i is the position of the most-significant bit set, and X is the contents of the operand field of the instruction.
What assumptions have you made about the relative priority of the various conditions?
(After reading 6.3) How might these instructions be used to speed interrupt handling?
2.12.
How could a double-length rotate instruction operating on a pair of processor registers be used to interchange their values?
2.13.
In 2.4 several methods are mentioned for specifying a particular subportion of a computer word.
List these methods, and try to suggest the factors involved in selecting a particular one for a computer design.
2.14.
Estimate the speed increase in using a (hypothetical or actual) block transfer instruction on a computer to which you have access, over the best alternative way of performing the move.
State any assumptions you make.
2.15.
You have to design a character set for a computer with an 8-bit  character.
Leaving aside considerations of compatibility with other computers, in what ways could the allocation of binary patterns to graphic symbols simplify the processing of character strings?
(Consider, for example, putting character strings in dictionary order, or testing character strings for validity).
How good is the character set in Figure 2.21 from this point of view?
2.16.
This chapter mentioned several ways in which the end of a character string may be indicated.
Describe their advantages and disadvantages (and those of any other methods you can think of).
2.17.
The compare instruction on the IBM 1401 computer Compare A B sets an equality indicator if the character string in the field at address A is exactly the same as the character string in the field at address B, and sets a high or low indicator appropriately if they are unequal (using the collating sequence shown in Figure 2.19).
What problems would arise if this instruction is used to compare numeric fields?
2.18.
Design a translation table to translate from the IBM 370 character code to the IBM 1401 character code.
This will be the inverse of the translation table given in Figure 2.24.
2.19.
On the IBM 370 range of computers, the three operands of the translate instruction are all strings of 8-bit bytes.
Suppose we wish to permute the order of the bytes in a string.
For example, the string: How can this be done with the translate instruction?
(Hint: let the translation table hold the string to be reordered.)
2.20.
Consider a translate instruction in which (a) The source and destination strings are scanned from right to left;(b) Each entry in the translation table contains both a value to be inserted in the destination string, and the address of a new translation table for the next source character.
Suppose further that we have an instruction which, given two character strings, perform a binary addition on corresponding pairs of characters.
For example, if we have strings of four 6-bit characters: Show how the above translate instruction could process this result string to make it a digit-by.digit sum to a non-binary base.
For example, to a base of ten, the result should be: What restrictions are there on the choice of base?
(This translate instruction is a version of one found on the IBM 7090 computer, and used for implementing decimal arithmetic).
2.21.
Follow in detail the sequence of events illustrated by each of the editing examples in Figure 2.25.
How must the editing pattern be altered to insert asterisks in place of high-order zeros in these examples?
2.22.
Write subroutines, for a computer to which you have access, to convert between a numeric character string and a format in which numeric calculations can be carried out.
In the most general case, this might convert between strings of the form: and binary floating-point (d is any decimal digit, and  means "times ten to the power" ).
3 The instruction set
3.1.
Instructions as a data-type
IN A STORED-PROGRAM computer the instruction set is one of the most important data-types to be held in the computer store.
What operations then do we need on this data-type?
On early computers it was necessary to provide facilities to manipulate the operand fields of instructions.
In 1.1, for example, we showed a sequence of instructions to add one to the operand field of an "add" instruction, so that when executed it referred to successive locations in store.
Several early computers therefore provided an instruction to manipulate the operand field of another instruction; instructions 18 and 19 on the Von Neumann computer (Figure 1.5) are an example.
Modern computer architecture makes this type of instruction modification undesirable and, since the introduction of index registers, it has become unnecessary.
Therefore the only operations nowadays provided on this data-type are to jump to or execute an instruction; we deal with these operations in 3.3 and 3.4.
Here we discuss possible variations in the format of instructions, and the matching of instruction length to computer word size.
Such variations may be irrelevant to the programmer, even when writing a program in a low-level language, since the assembler deals with the detailed representation in the computer store of the sequence of instruction fields specified by the programmer.
Basic instruction formats
Figure 3.1 illustrates some typical instruction formats.
Figure 3.1 (a) shows the basic instruction format for a single-address computer.
It has an operation code or function field, specifying what operation the computer is to perform, and an operand field, which specifies an operand (usually as a store address) or the location for a result.
Nowadays most computers have several methods by which an operand may be specified: we have met direct addressing and indexing in Chapter 1, and the other methods are discussed in Chapter 4.
The operand field may therefore be divided into two portions, as shown in   Figure 3.1(b).
The larger portion holds a numeric value, and the smaller, the addressing mode field, specifies how the numeric value is to be interpreted; for example, as a numeric quantity for direct use (see the next section), as a store address, or as a numeric quantity to be added to the contents of an index register to form a store address.
Often this field simply encodes the number of the index register to be used, with one value implying "no indexing" : any other addressing methods available would then be specified as part of the operation code field.
Many computers have a further field in the instruction, to specify subsidiary information required by an operation.
This is illustrated by Figure 3.1(c), which shows one of the five formats of IBM 370 instructions, the RX or "register and indexed storage" format.
The subsidiary information field specifies which one of 16 accumulators is to participate in the instruction, a common use for this field.
The addressing mode field is divided into two subfields, each of which specifies one of the 16 accumulators whose contents is to be added to the operand (or  "displacement" ) field to give an effective address: a value of zero in either subfield means no indexing by this field.
Figure 3. l(d) illustrates a typical two-address instruction: this is another format of instruction from the IBM 370 range, the SS or "storage to storage" format.
Here the subsidiary information field specifies the length or lengths (in bytes) of the operands.
The addressing mode fields (one per operand) each specify an accumulator (or none) whose contents are to be added to the appropriate operand (or "displacement" ) field to generate an effective address.
Certain operations do not require the specification of a store address; an example is shifting the accumulator left or right.
There are several possible ways of representing such instructions in the computer store.
(a) The operand and addressing mode fields are simply ignored.
Thus the number of bits to be shifted is specified in the subsidiary information field, or encoded as part of the operation code field.
(b) A second possibility, which makes better use of the instruction length, is to calculate an effective store address from the operand and addressing mode fields in the instruction in the normal way (for example, by adding the contents of an index register).
But the effective address is then used, not to specify a store location, but as subsidiary information for the operation to be performed (such as the number of bits to be shifted).
This allows the subsidiary information for an operation to be modified (for example, by manipulating an index register) without modifying the instruction itself.
This method is used for shift instructions on, for example, the IBM 370 range.
(c) A third approach is to divide the instruction set into a number of groups, and provide a different instruction format for each group.
For example,(on a one-address computer) we might have a group of data-manipulation instructions with an operation code field, a small subsidiary information field to specify an accumulator, and addressing mode and operand fields.
Then a second group comprises those data-manipulation instructions not requiring a store address, which have an operation code field and a large subsidiary information field.
A third is transput instructions, requiring a field to specify one from a range of peripheral devices.
Jump instructions could be a fourth group, if the  layout of the addressing mode and operand fields differs from that of the first group.
For examples see the Data General Nova (discussed later in this section) and the DEC PDP-l 1.
Thus the operation code field is decoded to specify, not only what operation is to be performed, but also how the bit pattern of the remainder of the instruction is to be interpreted.
The operation code field
We have assumed in the above that the operation code field is fixed in length, but we may be able to make better use of the instruction length by having a variable.length operation code field.
If we have a fixed-length instruction word with one of the formats shown in Figure 3.1 (a) -(c), then the number of bits required to specify such secondary information as an operand address limits the size of the operation code field, and therefore limits the maximum number of different operations with this amount of secondary information.
However, if there are further operations requiring less secondary information, then another instruction format can be provided with a longer operation code field.
Typically one value of the short operation code field marks an "escape" into the format with the longer operation code field.
For example, the IBM 370 range has several instruction formats, all with 8-bit operation code fields occupying the first byte (two formats are shown in Figure 3.1(c) and (d)).
But if the first byte has a (hexadecimal) value of 9C, 9D, 9E, 9F, or B2, then a 4-byte "5" format instruction is indicated, and in this format the operation code field occupies the first two bytes.
This can obviously be extended to allow a range of different sizes of operation code field, so that we can provide more operation codes for less secondary information (such as operand fields) in a fixed instruction length.
The assignment of binary bit patterns to operation codes, and of portions of the operation code field to positions in the instruction format, must be done in such a way that the hardware of the computer can decode the instruction; thus it must be possible to extract an initial portion of the operation code from a fixed position in the instruction and decide, from the bit pattern found there, whether an operation is now specified (in which case the rest of the instruction can be interpreted as secondary fields) or whether a further portion of instruction has to be decoded, and so on.
Usually only a small number of different   operation code field sizes are used, rather than a completely variable field size.
Figure 3.2 illustrates the various instruction formats available on the Data General Nova computer.
Notice that we have mentioned two ways of specifying an operation to be performed: by having various lengths of operation code field, or by having a fixed length operation code field together with some form of subsidiary information field.
In a real situation, such as on the Data General Nova, it may be difficult to distinguish between the two.
The subsidiary information field
We have seen the subsidiary information field as providing parameters needed by an operation, such as the number of bits to be shifted or the length of an operand.
Instead we can see it as completing the specification of which operation is to be performed.
Thus some computers use for the jump instruction a format in which the subsidiary information field is a sub-operation code field, which specifies the condition under which the jump is to be performed (for example, accumulator positive, or overflow flag set).
This is discussed in more detail in 3.3.
Alternatively such a sub-operation field might specify secondary functions to be performed with the main operation.
On the Data General Nova computer the group of instructions for arithmetic and logical operations (seen in Figure 3.2(d)) has an 8-bit sub-operation field, detailed in Figure 3.3.
This specifies whether the result of an operation   is to be shifted, how any carry is to be dealt with, whether the storing of the result is to be suppressed, and under what conditions a skip is to be executed.
The last two used together allow tests to be performed without affecting the operands.
Bit significance
The allocation of operation code bit patterns to particular instructions may be arbitrary, within any constraints set by the use of variable length operation code fields.
However, an attempt may be made to allocate particular bits or groups of bits in the operation code field to particular features of the instruction; one bit, for example, might specify whether the result of an operation is to be placed in the accumulator, and another whether the fixed-point arithmetic unit is to be used by the operation.
This is called bit significance.
Since it results in long operation code fields, full bit significance is not used for operation code assignment, except at the microprogram level discussed in 3.6.
Because of this connection, the term "microprogrammed" is sometimes used to describe instructions with bit significance, such as the "operate" instruction on the DEC PDP-8 computer (described on p. 88).
Some limited bit significance is provided by splitting the operation code field into two portions, one specifying the instruction group and one the particular instruction within the group.
This is done, for example, in the operation code field of the IBM 370 range of computers, as shown in Figure 3.4.
It is more common to find bit significance in sub.operation fields.
Thus on the DEC PDP-8 computer, there is a 3-bit operation code field; if the operation code is 7, then the "operate" instruction group is indicated, and a particular function is specified by a sub-operation field consisting of the remaining nine bits (of the 12-bit word).
If the left-hand bit of this field is zero, then a "group 1 operate instruction" is indicated and the significance of the remaining eight bit positions is as shown in Figure 3.5.
Several of these bits may be set in an instruction, in which case all the specified functions are carried out; the order in which functions are performed is fixed by having each function assigned to one of four timing points within the basic instruction cycle.
@@@  For example, if the bits for "logical complement of accumulator" and "increment accumulator" are set, then the instruction "complement and increment accumulator" (i.e. two's complement negate accumulator) is obtained.
If the left-hand bit of the sub-operation field is one, a different set of functions is indicated.
These include tests on the accumulator and carry register (group 2) if the right-hand bit of the field is zero, and operations on an (optional) extended arithmetic element such as multiplication and division (group 3) if it is one.
Instruction length versus word length
In the above discussion we have been considering a word-oriented computer where each instruction occupies one word.
It will sometimes be the case that, for certain instruction groups, the number of bits required fully to specify an operation and all its associated information is less than the computer word length; so these groups are using the word inefficiently, and are candidates for occupying only a portion of a word.
On the CDC 6600 computer (which uses a 60-bit word) instructions may be 15 or 30 bits long, according to the amount of information needing to be specified by the instruction.
In extreme cases all instructions have such unused fields, and then there may be room for two instructions to a word.
An example of this is the Von Neumann computer, where arithmetic precision requirements forced a much longer word length than was required for instructions.
An alternative solution is the provision of multiple.length arithmetic operations, of course.
Conversely, a group of complex instructions may require more bits than are available in a computer word, leading to a two-word format for instructions in this group.
The instruction decoding process needs to recognize such double.length instructions and access a second instruction word.
The most common system on word-oriented computers is, of course, one instruction per word for all instructions.
Instruction syllables
In the zero-address computers discussed in the next section, some instructions require only an operation code field while others require an operand specification as well, so considerable variation in instruction length is possible.
The basic instruction unit is then the syllable (of perhaps 6 or 8 bits), instead of the word.
An instruction then consists of an initial operator syllable which specifies the operation to be performed, perhaps followed by one or more further syllables specifying a store address or subsidiary information.
The operator syllable implies (or specifies in a subfield) how many syllables are required for the instruction.
The syllables making up an instruction are placed immediately after the syllables of the previous instruction, without regard for the boundaries between words.
Notice that the program counter has to indicate not only which word has been reached in a program, but also which syllable within the word.
Instruction addressing
A problem in any word-oriented computer with instruction formats shorter than the word is how to address individual instructions.
This reduces in practice to deciding how to specify the destination of a jump instruction, when this destination may be any part-word or syllable.
(a) One solution (which assumes such destinations are sparsely scattered within the instruction stream) is to force instructions which are jump destinations to be on word boundaries; this is the method used on the CDC 6600 computer.
(b) Another is to provide jump instructions with a field specifying a part-word or syllable address within the addressed word; this is the method used on the Burroughs B6700 computer.
On character- and byte-oriented computers we have the freedom to use these units of storage as instruction syllables, without any of the problems of syllable addressing mentioned above.
On the character-oriented IBM 1401 computer (McCracken 1962; Bell and Newell 1971, pp. 225–34) instructions are from one to eight characters long.
Instead of coding the instruction length into the operation code, the word mark bit is set on the left-hand character of each instruction (which contains the operation code).
Since instructions are processed from left to right on this computer (unlike data which is processed from right to left), the characters of an instruction are picked up until a word mark is reached (indicating the beginning of the next instruction); the instruction is then executed.
On the byte-oriented IBM 370 range, instructions can be two, four, or six bytes long.
However on all models the basic instruction unit is the half-word, rather than the syllable or byte, and instructions must therefore start at even addresses.
3.2.
The accessing of operands
Most of the arithmetic and logical operations discussed in the previous chapter are (or can be reorganized as) binary operations; that is, they require two operand values.
An instruction for such an operation must therefore indicate three locations, two source locations for the two operands, and a destination location to hold the result.
Three-, Two-, and One-Address Instructions
An instruction might therefore explicitly specify three store locations; an example of such a three-address instruction is Add the contents of store addresses A and B, depositing the result at store address C In early computers such a technique was feasible, since store capacities (and therefore operand field lengths) were small, and addressing mode fields were often absent.
An example of a three-address instruction format on an early computer is shown in Figure 3.6.
However, with increasing store capacities and the presence of addressing mode fields, the use of three-address instructions makes for a very long instruction format, and the technique is not now used.
What the programmer wants in many cases is for the result of the operation to replace one of the operands.
Hence we are led to consider a two-address instruction format, where one store address is the source of the first operand and the destination of the result, and the other store address is the source of the second operand.
For example, the instruction Add A to B specifies the addition of the contents of store addresses A and B, leaving the result in store address B. A typical two-address instruction format was illustrated in Figure 3.1(d).
It could be implied in the operation code whether the first or second store address is to receive   the result, but in practice any particular computer uses always the first address, or always the second address, as the destination: for example the SS (two address) format of the IBM 370 range uses the first address as the destination.
If the effect of a three-address instruction is required, for example "add A to 8 giving C" , we need to use two two-address instructions, Move A to C Add B to C (giving C)
A two-address instruction format is usually still too long.
We therefore take account of the fact that the programmer often wants (especially in scientific computing) to do further operations on the result of an operation, and consequently this result can be held in a storage register in the processor, the accumulator.
Thus we have the conventional one-address instruction format (introduced in Chapter 1) where one of the operands and the result are held in the accumulator, and the store address of only the second operand has to be specified in the instruction.
Typical one-address instruction formats were shown in Figure 3.1(b) and (c).
Notice that holding the intermediate result of a calculation in the accumulator not only shortens the instruction length, but also increases the instruction execution speed, since the access time to a processor register is shorter than to a store location.
In this case the address of the first operand and result is implied since (for example) the "add" instruction has the meaning
Add to the accumulator and leave the result in the accumulator In fact, a two address format is really a "two-explicit and one-implicit" -address format, and a one-address format is a "one-explicit and two-implicit" -address format.
Computers designed for scientific calculation, where it is valid to assume that the result of one operation may be an operand of the next have an accumulator and use the one-address format.
Computers for  commercial data.processing, where the assumption may be invalid and where an accumulator would have to be very long to deal with character string data formats, tend to use the two-address format and not provide an accumulator.
Computers for use in both areas (such as the IBM 370 range) may provide two sets of instructions: a one address set for word operands (using an accumulator), and a two-address set for character string operands.
An array of accumulators
The provision of a single accumulator does not really correspond to the way in which most programs are written, since at any given point there are usually several intermediate values all of which are in the process of being manipulated.
For this reason, a small array of accumulators (typically a power of two, such as 8 or 16) may be provided instead of a single accumulator.
Arithmetic and logical operations are now performed between operands both of which are in accumulators, or one in an accumulator and one in a store location.
The instructions have to specify the accumulator to be used as well as the store address, giving some of the advantages of the two-address system without the long instruction format required to hold a second store address; this technique has therefore sometimes been called the "1½-address system" .
Such sets of accumulators seem to have appeared first in the Ferranti Pegasus computer (Bell and Newell 1971, p.170) and most modem computers are now designed with this feature.
Since there are only a small number, it is economic to make the accumulators faster than the store elements, so that operations are performed at greater speed while all operands are held in accumulators.
We therefore attempt to hold the current program "context" in the fast accumulators as the program is executed; since it may be difficult for a programmer or compiler to establish such a current context at each point, it has been proposed that the accumulator array be replaced by an associative store, and this is discussed in 5.5.
As we have seen in the previous chapter, there may be several other processor registers accessible to the programmer apart from the accumulator, for example the MQ register.
To achieve a more uniform instruction set, such specialized registers are replaced by the use of accumulators in the array.
Thus on the IBM 370 range, double.length fixed point binary operands (such as for multiplication and division) are held in a pair of consecutive accumulators, and the instruction  species the lower numbered accumulator of the pair; since the effective MQ register is an accumulator, we can use any of the computer's instruction set to manipulate it.
The most important such special register (after the accumulator) is the index register (or registers).
At most points in a program there are several intermediate results and several index quantities; further, calculated results may later be used as index quantities.
It is logical, therefore, to make provision for a subset of the accumulator array to be used as index registers, instead of providing separate index registers.
In some computers all accumulators may be used as index registers, although an index specification of zero usually means no indexing rather than a specification of accumulator zero as index register: this is the case on the IBM 370 range.
Thus the normal instruction set can be used on index quantities, without the need for a special index arithmetic set of instructions.
Instructions for an array of accumulators
Let us now consider the types of instruction required by a computer with an array of accumulators.
First we require normal "load and store accumulator" instructions, as described earlier for computers with a single accumulator, except that now the instruction must specify the particular accumulator involved.
Second, instructions to transfer data between a sequence of store locations and a sequence of accumulators, as described in 2.4, are useful for rapidly changing the processor context.
Third, we require an instruction to move data from one accumulator to another, perhaps performing subsidiary operations on the data (such as negation) during the move.
For binary operations, on both one- and 1½-address computers, we have a number of possible ways of interpreting the explicit and implicit locations of operands and result.
The following are possible:(a) The first operand is in the (specified) accumulator, the second is in a store location, and the result replaces the contents of the original accumulator.
This is the standard form, described in Chapter 2.
(b) The first operand is in the (specified) accumulator, the second is in a store location, and the result replaces the contents of the store location.
A small number of instructions of this form is found in some, but not all, one- and 1½-address computers; a fairly common example is an "add into storage" instruction.
(c) The first operand is in the (specified) accumulator, the second is in a store location, and the result replaces the contents both of the original accumulator and of the store location.
This form is much rarer than (a) or (b).
The peripheral control processor (PCP) of the CDC 6600 computer (see 6.5) has what is referred to as a "replace add" instruction, where the result of adding the contents of the accumulator and a store location replaces both the accumulator and the store location.
On the DEC PDP-10 computer most fixed point binary and logical operations (such as And and Add) have four modes, as shown in Figure 3.7.
Note that in the above discussion we have carefully distinguished the first from the second operand.
In some operations, such as the logical operations, add and multiplication of equal-length operands, it is irrelevant which operand is considered as the first.
However, certain operations are inherently asymmetrical, the most important being subtract and divide.
If operand A is in an accumulator and B is in a store location, the above forms allow us to compute (A-B) and (A-:B), but not (8-A) or (8-A) without further manipulation.
To this end, some computers provide "reverse" or "inverted" subtract and divide, which (respectively) subtract the first from the second operand and divide the second by the first operand.
(d) Both operands are in accumulators and the result replaces the contents of one of the accumulators, whether the first or the second usually being fixed for a particular computer (as with two-address instructions).
This is illustrated by the RR format of instruction on the IBM 370 range, shown in Figure 3.8 (a), where the result replaces the first operand.
Some computers have room in the instruction to specify a third accumulator to hold the result, giving the effect of a three-address instruction restricted to the accumulator array.
For example Figure 3.8(b) illustrates one of the instruction formats on the CDC 6600.
Instructions with both operands in accumulators are shorter than those of forms (a),(b), and (c) above.
In a computer with several instruction lengths this is, of course, no problem, but otherwise some of the instruction word may be unused.
On the Data General Nova this problem is solved by the provision of a sub-operation field in the accumulator-to-accumulator format (see Figures 3.2 and 3.3); in fact, as these figures show, no accumulator-and-store operations are provided except load and store.
Stacks and zero-address instructions
Consider now an arithmetic expression written in a high-level language like Algol or Fortran, for example A compiler for a computer with an array of accumulators would translate it into a form such as The compiler performs this translation by first transforming the original into reverse Polish notation, where each operator appears after its operands, in this case  (see, for example, Randell and Russell 1964).
It then uses this intermediate form to produce the final form above, allocating temporary results to accumulators (or to store locations if necessary).
However, this intermediate form could be interpreted directly by a computer with a stack or last-in-first-out store.
The English-Electric KDF9 computer (Davis 1960; Haley 1962; ICL 1968) provided such a stack, implemented in hardware and referred to as a "nesting store" , instead of a conventional accumulator.
This nesting store could hold up to sixteen operands, and is illustrated in Figure 3.9.
New operands are always inserted at position one, and all operands already in the stack are moved down one position (or "pushed" ); only the most recently added operand could be removed (from position one) and the remaining operands are moved back (or "popped" ).
An analogy can be made with the push-down stack of plates in a cafeteria or (as in the KDF9 manual) with the magazine of a Sten gun.
On such a stack-oriented computer an operand can be transferred from a store location to the top element of the stack (pushing down all previously entered operands), and the contents of the top element of the stack can be copied to a store location, the top element remaining on the stack or being deleted, moving up all earlier elements; the former is more akin to the conventional "store" operation, although the latter is more common.
All unary operations (such as negate) are performed on the top element of the stack, and binary operations are   performed on the top two elements of the stack (after which they are removed) and the result placed on the top of the stack.
The above example therefore becomes 
It will be seen that binary arithmetic and logical instructions need no explicit operands, and consist only of an operation code field; such an instruction therefore has a zero — address format.
Instructions are required to jump to a specified store location conditional on the value in the top element of the stack (which may be removed by the jump instruction); an example is Jump to location A if top of stack is positive On the KDF9 computer various stack manipulation instructions were provided, such as to duplicate or erase the top element or to rearrange the top few elements.
There are two problems with the type of stack found in the KDF9 computer:(a) it is of fixed size, so that the programmer or compiler has to take care to keep within its bounds; and (b) in a multiprogramming environment the stack has to be emptied and refilled at each process switch; the latter could be done on the KDF9 only one element at a time (although suitable stack dump and restore instructions could be envisaged), and the problem was solved on that computer by providing multiple nesting stores.
The Burroughs B6700 computer gets round these problems by using a different form of stack, which is illustrated in Figure 3.10.
Only the top two elements of the stack are (for speed) held in processor registers A and B. Operands are added to the top of the stack by being placed in A, whose contents (if any) are moved to B. The remainder of a process's stack is a vector of contiguous store locations.
Three processor registers point to the bottom or base of the stack area (the BOS register), the top of the stack area (the LOS or limit of stack register), and the current position of the last element placed in the area (the stack register S).
If an operand is "pushed" out of B, the stack register is incremented and the operand placed at the resulting address.
To remove an operand from the stack, that in A is removed, the contents of B moved to A, the contents of the store location referred to by the stack pointer are moved to B, and the stack pointer is decremented.
This sequence is carried out by hardware, which also checks that the stack pointer remains pointing at a location within the stack area.
Because the stack is such a common programming device for retaining intermediate results in the right order (for arithmetic calculation, syntactic analysis, subroutine and interrupt entry and return, etc.), many modern computers provide facilities for manipulating a stack, even though they are basically one-address computers rather than zero-address like the KDF9 and Burroughs computers.
A common technique (see for example the University of Manchester MU5 and ICL 2900 range, and many microprocessors) is to allow any contiguous block of store locations to hold a stack, and for a special processor register, the stack pointer, to hold the address of the location currently containing the top element of the stack.
Provision is then made to  manipulate the stack pointer and to transfer the data between the stack and an accumulator: this is discussed further in Wichmann (1974) and in the next section.
Immediate operands
We have seen that one implicit method of specifying an operand or result location is to imply the use of a processor register, such as the accumulator.
Another way of implying an operand value (without giving a store address) is to include the value in the instruction stream.
Such a value is known as an immediate operand or literal.
Most commonly certain instructions, or modes of instructions, interpret the contents of their operand field as a literal instead of an address.
Thus Load 100 loads the accumulator with the contents of store address 100, while Load literal 100 loads the value 100 from the operand field into the accumulator.
Other operations (such as addition) could have similar literal modes: see, for example, the systematic sets of modes on the DEC PDP-10 shown in Figure 3.7.
An implication of this is that the contents of the operand field may be modified in accordance with the addressing mode field before being used as a literal value; this allows some alteration of the value (for example, by adding the contents of an index register) without altering the instruction.
A second way of implying an operand is by encoding it as part of the operation code, although the range of values is very restricted as we do not wish to allocate many operation codes in this way.
Notice that a clear accumulator instruction can be seen as such an instruction with an implied operand value of zero.
Typical instructions in this class increment or decrement an accumulator; here the implied value is plus or minus one.
Since the incremental value is encoded in the operation field, there is room to specify a store address in the instruction.
This suggests an instruction to increment (or decrement) a specified store location by an implied value of one.
This is a very popular instruction, since it allows counting without disturbing an accumulator; it is commonly  made more useful by testing whether the count has reached zero after incrementing, and this is discussed again in the next section.
3.3.
Control instructions
We have been considering, in , groups of instructions to perform operations on the different data formats held within the computer.
We must now turn to those instructions which control the order in which these data.manipulation instructions are executed.
Normally the program counter holds the store address of the next instruction to be executed, and is incremented appropriately as each instruction is executed.
In order to transfer control to a new sequence of instructions, a new value must be deposited in the program counter.
This is done by a group of jump instructions, to be discussed in more detail in the remainder of this section.
The simplest transfer of control instruction is, of course, the unconditional jump or unconditional branch, after the execution of which the next instruction is taken from the store address specified in the jump instruction.
Notice that the operand field of the jump instruction is not used to access a store location directly, but is loaded as an immediate value into the program counter (after any address modification, such as indexing); thus "jump to location x" is equivalent to "load program counter with literal x" , rather than to "load program counter from location x" .
Conditional jump instructions
The unconditional jump is, of course, inadequate by itself to select a sequence of instructions to be executed, if the choice depends on results previously calculated by the program or read as input data (although instruction modification could be used).
We therefore require a set of conditional jump or conditional branch instructions, which transfer control to the instruction at the specified store address only if a certain condition is met; if the condition is not met, the next instruction to be executed is that immediately following the jump instruction.
A typical set of jump instructions for a medium-sized, one-address computer might be those shown in Figure 3.11.
The basic conditional jump instructions are (b) and (c); the condition may be encoded in the operation code field, or it may be specified in a subsidiary information field.
The skip instructions (e), and the special jump instructions (f) and (g), are discussed later in this section.
If the jump conditions are specified by bits in a subsidiary information field, then we may be able to specify further conditions by setting combinations of bits.
Consider a field of three bits x, y, and z, and let r and 5 be (respectively) the value of the contents of the accumulator and the value of the sign bit of the accumulator.
Then suppose the jump condition is  This is a simplified version of a portion of the group 2 operate instruction of the DEC PDP-8 computer, and gives the list of jump conditions shown in Figure 3.12 (another example of this technique of combining jump conditions is given in Figure 3.3).
Notice especially the first two.
Jump on condition "always" is an unconditional jump, so that we do not need a special operation code for this (although the assembler language may have a special mnemonic for it).
The null instruction
Jump on condition "never" is an instruction whose operation phase   does nothing (but presumably takes some fixed length of time to do it).
It is a particular example of a no-operation or dummy instruction.
On some computers such instructions are special cases of normal instructions, as in the example above; another example might be a multiples length move instruction, with the number of words to be moved specified as zero.
On other computers an operation code value (perhaps zero) is allocated to a special no-operation instruction.
This curious instruction may be used to pad out an instruction sequence so that it occupies a desired length of time or storage.
Examples of the latter are for dummy entries in a table of instructions, or as an alignment filler in a computer with variable-length instruction formats.
The skip instruction
The jump instructions (b) and (c) in Figure 3.11 contain an operation code field (perhaps together with a subsidiary information field specifying the jump condition), and an operand field (perhaps together with an addressing mode field) to specify the location to which control is to be transferred if the condition is true (that is, if the jump is successful).
However, suppose that (in a single.address computer) we wish to provide a conditional jump instruction to test an operand which is not in an implied location such as an accumulator.
For example, we might wish to test a specified store location to see if its contents are zero or non-zero, or to Compare the contents of an accumulator with the contents of a specified store location as in Figure 3.11(e).
There is then no room in the instruction to specify both a jump address and the address  of the location to be tested.
Further, on some small computers the subsidiary information field to specify a rich set of jump conditions may be so large that there is no room in the instruction for a jump address field.
A common solution is for the instruction to imply a jump address, the next location but one after the instruction.
We thus have the typical "skip" instruction; If the condition is true skip the next instruction; if it is false execute the next instruction On a single-address computer (such as the DEC PDP- 10) we may find conditional jumps on a test of the contents of an accumulator, or on a comparison of two accumulators, but conditional skips on a test of the contents of a store location, or on a comparison of an accumulator with a store location.
On a small computer (such as the DEC PDP-8) a minimal set of instructions is several conditional skips and an unconditional jump; thus "jump to A if accumulator contents are zero" would be coded as: skip if accumulator contents are non-zero
Jump to location A The skip technique may be extended in several ways:(a) On several computers a subsidiary field specifies the direction and length of the skip with respect to the current instruction.
Notice that such a skip instruction bears a close resemblance to a jump instruction using relative addressing (see 4.3).
(b) A multiway skip instruction has sometimes been provided.
For example, a comparison instruction might execute the next instruction if the first operand is high, skip one instruction if the operands are equal, or skip two instructions if the second operand is high.
(c) An instruction may perform some data manipulation and skip the next instruction if an unusual condition occurs.
A common example is an instruction to increment the contents of a specified store location  and skip if the result is zero.
Further examples are given in the search instructions in 3,4, and the transput instructions of 6.2.
Condition codes
A second way of dealing with the amount of information to be specified in a conditional jump instruction is to separate the test and the jump into two different instructions, and to provide a short processor register to communicate between the two instructions.
This register, the condition code, is set by the testing instruction to reflect the result.
Then the only jump instruction that is logically necessary (though others may be provided for efficiency) is one to jump if the condition code is set to a specified binary bit pattern (or perhaps a specified group of patterns), and to drop through to the next instruction if the condition code is set to any other pattern.
This technique is used in the IBM 370 range, which forms the basis of the examples which follow.
Suppose a comparison instruction sets a two-bit condition code, depending on a comparison of the contents of accumulator A with the contents of a store location B, as shown in Figure 3.13(a).
Then, in order to jump to store address C if the contents of accumulator A are not less than the contents of store address B, we write Compare Accumulator A to store address B Jump to C if condition code is 00 or 10 The set of condition code patterns, any one of which is to cause a successful jump, is specified in the IBM 370 range by a four-bit subsidiary information field.
As with the skip technique, we can allow instructions other than tests and comparisons to set the condition code, with the added advantage over skipping that the setting of the condition code can be ignored where necessary.
For example, Figure 3.13(b) shows how the add instruction on the IBM 370 sets the condition code.
Notice how the overflow flag has been incorporated into the condition code.
Stack-oriented computers may set a condition code in the top element of the stack as a result of a test or comparison instruction; a jump instruction is then provided to jump conditional on the state of this top element (removing the element after testing).
The loop
One of the most common programming devices is the loop, whereby a set of instructions is performed a specified number of times.
Because of the frequency of occurrence of this device, most computers include instructions tailored to its use.
Such an "increment count and jump" instruction specifies an accumulator, index register, or store location whose contents are to be used as a loop counter (and which will have been initialized by another instruction).
When the instruction is executed it adds an increment value to the count, and tests whether the count has reached a limit value.
If not (that is, further iterations of the loop are to be performed), a jump is made to a specified store address (presumably the beginning of the loop).
If the limit has been reached, the next sequential instruction is executed.
The increment value and limit are commonly implied by the instruction as (respectively) plus or minus one and zero.
If the jump address is also implied we have the "increment count and skip" instruction described earlier.
Index register manipulation
In many loops, the set of instructions manipulates successive elements from a vector in successive iterations.
An index register normally holds the offset of the current element within the vector, and needs suitable modification between iterations of the loop.
A group of instructions is therefore needed to perform a suitable range of manipulations on the index register or registers.
Typical operations are to load an index register from a store location, to deposit the contents of an index register in a store location, and to increment  or decrement an index register (or to add the contents of a store location to an index register).
We may terminate the loop either by means of an "increment (decrement) count and jump on zero" instruction, or when the contents of the index register reach a certain value (in which case we need an instruction to compare an index register with a store location).
The groups of instructions to manipulate index registers need to specify an index register and a store location.
In the past the field which normally specified the addressing mode was often used in these instructions to specify the index register to be manipulated, thus reducing the available addressing modes.
Nowadays index registers are Commonly a subset of an accumulator array, so that the normal fixed-point arithmetic instructions are used to manipulate index quantities.
Because of the connection between looping and indexing, it is tempting to provide instructions which incorporate both loop counting and index modification.
We briefly describe a number of variations on this idea below, However, while such techniques can be used effectively when programming in an assembler language, it is much more difficult for a compiler to recognize which constructions in a high-level language call for their use.
Such compound instructions may therefore be less useful than appears at first sight.
(a) An index register is divided into a modifier field and a loop count field: the former holds the value to be added to the operand field of an instruction requiring indexed addressing.
An instruction adds one to each field of a specified index register, and jumps to a specified address unless the loop count field has reached zero.
This is available on the DEC PDP-10 computer.
(b) An index register is divided into a modifier field (as before) and an index limit field.
An instruction adds one to the modifier field of a specified index register, and jumps to a specified address unless the two fields are equal.
This is a simplified version of a facility provided on the IBM 370 range (but see below).
(c) In (a) and (b) we assume that the index register is large enough to hold both fields; for example, the DEC PDP- 10 uses the two halves of a 36-bit accumulator.
However, on the IBM 370, the fields are held in  two separate accumulators specified by the instruction.
(d) In (a) and (b) we assume that the modifier field is always incremented by one, but other arrangements are possible.
On the IBM 370 range the increment value is held in a third accumulator specified by the instruction.
An alternative method for dealing with vector elements of different lengths is for an increment value of one always to be used, but for the modifier field to be suitably manipulated when an element is accessed.
Thus single-length elements require no manipulation; double-length elements require the index quantity to be shifted one bit position left before adding to the vector base address; half-length elements require a right shift, with the bit shifted out used to select the appropriate half-word.
The type of manipulation required would be specified by the operation code of the accessing instruction, or by a descriptor referred to in the instruction.
This technique of index scaling is used in the University of Manchester MU5 and the ICL 2900 range.
Index registers and stacks
One special use for an index register is to implement a stack in an area of store, by having the modifier field point to the current top element of the stack.
The "push" instruction performs the following sequence of operations: Increment the modifier field Store the new data at the address pointed to by the modifier field The "pop" instruction performs the following sequence of operations: Extract the contents of the store location pointed to by the modifier field Decrement the modifier field This pair of instructions is available on the DEC PDP- 10 computer, where the stack pointer is the modifier field of a specified accumulator, and the source of the data to be pushed or destination of the data to be popped is a specified store location.
The push and pop instructions also increment or decrement the count field of the specified accumulator by one, and signal count over- and underflow.
As was mentioned in the  previous section, it is very common nowadays for such facilities to be provided with an implicit stack pointer register in the processor, rather than with an explicitly specified accumulator.
The subroutine
The subroutine or procedure is another important programming device, whereby several pieces of similar program code are replaced by one piece of code to which control is passed as required.
The problem here is to keep track of the point from which the subroutine is called, so that control can be returned to the correct place in the calling routine after the subroutine has been executed.
In the Von Neumann computer, control was returned to the calling routine by suitably modifying a jump instruction in the subroutine before entry.
The method used in all modern computers is to provide a "subroutine call" (or "link" ) instruction, which jumps to a specified store location and saves the value of the program counter, to enable control to be returned from the subroutine.
A possible place to save the contents of the program counter (that is, the address of the instruction following the subroutine call) is in a suitable store location in the subroutine, typically its first location; the subroutine jump will then be to the second location of the subroutine.
This method, illustrated in Figure 3.14, is used on the DEC PDP-g computer.
To return to the calling routine we perform a "jump indirect" via the first location of the subroutine (see 4.3).
A slight variation (used by the CDC 6600 computer) is for the subroutine call to store a "jump to location (n+ I)" ; to terminate the subroutine, we jump directly to this jump instruction.
This technique for calling subroutines has the disadvantage that locations are being modified in the instruction area, and that recursive subroutine calls are difficult.
A more common system on modern computers is to store the return address in an index register.
Thus a typical subroutine call instruction at location n calling a subroutine at location 5 places the address (n+ I) in the index register and jumps to location 5.
A particular index register could be implied for the return address, but usually the index register is specified in the call instruction.
Parameters (or addresses of parameters) may be passed to a subroutine by placing them in locations following the subroutine call instruction, In which case the index register containing the return address can be used to access the parameter.
The return to the calling routine is by a jump using an address indexed by the appropriate index register.
This is illustrated by Figure 3.15.
Subroutine calls and stacks
If subroutines call themselves recursively, a stack must be implemented to hold the return addresses.
Because of this close association between stacks and subroutine linkage, some computers provide hardware stacks to deal with subroutine calls and returns.
The KDF9 computer (Davis 1960; Haley 1962; ICL 1968) provides a subroutine jump nesting store (or SJNS) of 16 elements.
The subroutine call instruction places its address on the SJNS (pushing down all earlier entries) and jumps to the specified store address.
The sub-routine return instruction removes the top address from the SJNS (popping up all earlier entries), and jumps to it.
Since n rather than(n+ 1) is stored on the stack, the address has to be adjusted by the length of the subroutine call instruction and any parameter list before jumping, and this length is specified in the operand field of the return instruction.
This method is illustrated by Figure 3.16.
On the Burroughs B6700 computer the subroutine linkage stack is integrated with the operand stack described in the previous section and with the store addressing system; it is described in 4.6.
On the DEC PDP-10 computer, subroutine call instructions are provided for saving the return address either in the first location of a subroutine or in an accumulator; but it also has call and return instructions which treat a specified area of store as a return address stack, in a manner similar to that described earlier in this section.
Subroutine transparency
Subroutines are usually written to be "transparent" to the calling routine; that is, the processor registers such as accumulators and condition codes can be assumed to be unaltered by the subroutine call, unless they are explicitly part of the subroutine linkage.
This is achieved by saving information at the beginning of the subroutine if it is likely to be changed, and restoring it before the return.
The instructions to save and restore a set of accumulators, described in 2.4, help here.
However sometimes the subroutine linkage instructions themselves automatically save and restore some part of the required information (typically processor flags such as overflow).
The halt instruction
Finally, mention should be made of the halt instruction.
Most first and second generation computers had such an instruction to stop the computer, the instruction sometimes containing a store address at which execution was to be resumed upon suitable manual intervention.
Further, some instructions halted the computer if an error occurred, such as attempted division by zero, whereas in a third generation computer this would be dealt with by setting a flag or generating an interrupt.
On a modern computer a halt instruction must be a privileged instruction (see 3.5) if it is provided.
Some modern computers (such as the IBM 370 range) do not have a halt instruction.
Instead, the computer enters an idle or "wait for interrupt" state; the full halt is then provided only by manual intervention.
3.4.
Special groups of instructions
In this and the previous chapter we have discussed a number of groups of computer instructions, oriented to the principal data-types and operations for which the computer is designed.
We will see further such groups in later chapters, particularly in Chapter 6 when we discuss transput and interrupt handling.
In this section we attempt to provide a framework for discussion of a number of special instructions (or groups of instructions), provided in current computers or proposed for the future, which do not naturally fit into these groups.
Notice that some instructions already discussed, such as "edit" or "shift and count" , could be treated in this section as special-purpose instructions.
The computer's instruction set has evolved to provide the facilities required by the programmer, or to provide as one instruction a group of operations commonly found together.
But the advent of almost universal programming in high-level languages alters the requirements of a computer's instruction set.
What is now required is a simple instruction set without idiosyncrasies, since a compiler may be unable to recognize points in a program where it might use a sophisticated facility.
The more complex instructions, described in this section for interest, are therefore tending to disappear from modern computers.
The execute instruction
A common instruction on the larger computers is execute, which causes execution of the instruction at the store location specified in the execute instruction.
There may be restrictions on the type of instruction which can be executed in this way; for example, in the IBM 370 range it cannot be another execute.
However, it could be a jump instruction, in which case control is transferred (assuming the condition is true, for a conditional jump instruction) to the jump address: otherwise, the next instruction executed is that immediately following the execute.
Thus, the final result is as if the execute is replaced by the "target" instruction; that is, by the instruction specified by the execute, or by the final non-execute instruction if a cascade of executes is allowed, as shown in Figure 3.17.
On the IBM 370 range it is possible to specify that the execute is to modify the second byte of the executed instruction.
This technique   gives us some of the advantages of instruction modification, while keeping instructions and data in separate areas.
We can therefore use the execute to make up for some of the deficiencies of a computer's instruction set; examples might be the coding of jump tables where the Computer does not have indexed jump instructions, or operations on dynamically variable-length data where the operand length is coded in the instruction format; the latter is illustrated in Figure 3.18.
Iterated instructions
We can envisage creating a new group of instructions by selecting one of the operations already met with, and producing an "iterated" version of it.
Thus, if we have a binary operation "X" we could produce an "iterated X on addresses A and B" which would operate as follows: Perform X between store locations A and B Update the addresses A and B Conditionally repeat The updating of addresses A and B might take place in private registers within the processor hardware, or in program-accessible registers (such as the index registers).
Some examples (where the conditional repetition is based on a previously initialized counter) are:(a) If X is "move" and the updating is incrementation by one, we have the block transfer operations described in 2.4.
(b) If the elements addressed by A and B are smaller than a word, we are ill the realm of the character string operations of 2.5.
(c) A fairly common example is "sum" , where X is "add to accumulator" and A is updated by incrementing by one (B is not used).
This sums ill the accumulator the contents of a sequence of locations.
(d) If X ranges through the arithmetic and logical operations, we have the vector operations discussed later.
If X is some form of test instruction then the iteration might cease before the counter reached zero.
For example, a "compare for equality" between two vectors of store locations would cease as soon as an unequal pair was found.
A few computers allow the programmer to create his own iterated instructions by providing an "iterated execute" or "repeat" instruction, which operates as follows: Execute the (perhaps modified) target instruction Update an index register or the execute modification field Conditionally repeat Again the conditional repeat is based on a count set up by the repeat instruction, or on the setting of a flag by the target instruction.
Search instructions
An interesting type of instruction is the search or scan instruction, which may be seen as an iterated compare.
A general search instruction has to specify several pieces of information; the address A and length L of a table to be searched, a comparison condition and a compared value (the former usually implied in the operation code or specified In a subsidiary information field, the latter in an implied location such as an accumulator).
The instruction then operates as follows: Load A and L into internal processor registers Perform the comparison with the word at A If the comparison is successful, terminate the instruction Otherwise, decrement L and increment A If L is zero, terminate the instruction Otherwise, jump back to the second line
The instruction could set a condition code to indicate whether the search was terminated by a successful comparison or by L becoming zero; alternatively, one of these two results could cause a skip.
If the comparison is successful, a program-accessible register is loaded with the address of the compared words; alternatively, A and L could be retained in program-accessible registers throughout the process, so that the programmer can interrogate them for the result of the search.
Examples of search instructions are the "table look-up" (TALU) instruction on the University of Manchester MUS computer, which performs a masked equality comparison between the contents of one store location and the contents of each of a vector of elements, the similar "masked search for equal" (SRCH) instruction on the Burroughs B6700 computer, and the "linked list look-up" (LLLU) instruction, also on the B6700, which performs a "greater than or equal" comparison between an argument value and each of a linked list of elements.
A common type of search (called "translate and test" on the IBM 370  range) is a variation on the translate instruction of 2.5, and facilitates the scanning of character strings for delimiters.
If the character currently scanned contains the value v, then the vth entry in a specified table is extracted; the scan terminates or continues depending on the value of this vth entry.
On termination the extracted value and the address of the associated character in the string are made available to the programmer.
Unusual data-types and instructions
In the last chapter we discussed a number of simple operations on two basic arithmetic data formats, fixed point and floating-point.
We could envisage more complex operations (such as square root, logarithm, sine), but in conventional modern computers these are provided by software, though they are common in the programmable calculators of 7.3.
We have seen pairs of words (a, b) interpreted as double-precision arithmetic formats, with corresponding operations.
A word pair (a, b) could be interpreted as a complex number (representing a+ib), as a rational number (representing a/b: see Knuth 1968, pp. 2902), or as an arithmetic interval (representing the interval [x; a_{x_{b]: see Knuth 1969, p. 207; Nickel 1968), but no conventional computers provide these data-types in hardware.
Computer instructions generally operate on individual scalar variables, although we have seen how the use of index registers allows us to interpret these scalars as elements of vectors or stacks.
We can envisage providing arithmetic and logical operations on a vector as a single unit.
Thus "add vector A to vector B" performs the iterated "add A [i]to B[i]" for all elements of the vectors, which would be referenced via descriptor words.
This system has been implemented on several super-computers, since it provides scope for instruction overlapping (described in 8.2).
Examples are the CDC Star-100 (CDC 1971), the Texas Instruments ASC (Enslow 1974, pp. 274–89), and the Cray-l (Yuval 1977; Russell 1978).
Other instructions are occasionally met with.
Instructions to calculate check digits are fairly common on computers oriented towards communications.
Further examples (mainly on older computers) are instructions to generate a random number (for example on the University of Manchester Mark I computer (Lavington 1975, p. 20)), to select the higher of two operands (for example on the KDF9 computer), or to sort a vector of values into ascending order.
3.5.
Processor modes and process switching
There is a large discrepancy between the functions provided by the hardware of a computer (as described in this book), and the problem-solving device required by the user and the computer manager.
Only a single user can access the computer at a time, and he has to write, debug, and run his programs in the most basic computer-oriented form.
All but the smallest modern computers therefore have a layer of supervisory programs, the operating system, between the hardware and the problem programs, as shown in Figure 1.9.
The tasks of the operating system are:(a) To share between users the computer's resources (processor time, store, transput devices, and information usually in the form of files of programs and data).
In the simplest systems, where only one program is running at a time, this means merely expediting the changeover from one program to the next.
Commonly it means at any instant having several partially-executed programs, among which the computer resources have to be shared in such a way that they interact (if at all) only in authorized ways.
(b) To make the hardware easier to use.
This is done by having a virtual computer which more closely matches the user's requirements than the real computer, and implementing it partly by direct operations of the hardware and partly by operations of the operating system running on the hardware, A major aspect of the virtual computer is the provision of high-level languages, but the operating system is expected to provide other facilities, such as simple access to transput devices (in particular a filing system, and communication with the operator, and perhaps the user, at a terminal), the current date and time, etc.
For flexibility and economy the operating system is implemented as a set of programs, though in future more of its functions are likely to be taken over by hardware.
For a more detailed discussion of operating systems see Hansen (1973), Madnick and Donovan (1974), or Lister (1975).
Since we wish to keep small the amount of main store occupied by the operating system, most of it is non-resident, being stored on magnetic  disc or drum and brought into main store as required.
However there must be a core of supervisory functions resident in main store at all times, which can bring in other functions as required.
This is given various names, such as the nucleus or executive; we will use the term supervisor for this basic set of main-store-resident supervisory programs.
Supervisor mode and privileged instructions
Because the problem program is under the control of the supervisor, it cannot be allowed unrestricted access to all the facilities provided by the computer.
For this reason most computers run in two modes or states, the supervisor (or master) mode and the problem (or slave) mode; the current mode is signalled by a one-bit register in the processor.
In supervisor mode all the facilities of the computer are available for use, while in the problem mode certain instructions (known as privileged instructions) are disallowed and cannot be executed.
These privileged instructions include all instructions concerned with handling transput devices, the interrupt system, the store protection and virtual store mapping systems (if present), and usually facilities like the clocks and interval timers.
Any use of these facilities by a problem program must be via a request to the supervisor, which can check the validity of the request and whether the problem program is authorized to use the facility.
Two points should be noted in connection with supervisor mode and privileged instructions.
(a) Large areas of an operating system (for example the compilers and the scheduling and accounting programs) do not require the use of privileged instructions, and therefore should not run in supervisor mode.
(b) Privileged instructions are not the only way of protecting critical facilities.
If transput devices and processor facilities are controlled via store addresses (as discussed in 5.3 and 6.2), then the facilities can be protected by the store protection system, and privileged instructions can be dispensed with.
Mode-changing facilities
The computer commences operation in supervisor mode after start-up.
A privileged instruction is provided to pass from supervisor mode  to problem mode, so that the instructions of a problem program can be executed.
Any interrupt (such as that caused by the completion of a transput operation, or a floating.point overflow) causes a return from problem to supervisor mode, since it is the supervisor's task to process such conditions.
From time to time the problem program will wish to gain entry to the supervisor in a controlled manner, for example to request that a transput operation be performed.
One way this could be done is to cause a program error of some kind, such as by trying to execute an invalid operation code (or a privileged instruction), since this of course causes an interrupt and entry to the supervisor.
This is a risky method, since a program error might be interpreted as a supervisor request.
A better method is to provide a special supervisor call instruction which causes a distinguishable interrupt into the supervisor to be generated.
Such an instruction may have an operand whose value is passed to the supervisor as part of the interrupt.
For example, the supervisor call instruction on the IBM 370 range passes a one-byte literal to the supervisor, to indicate the type of request being made: any further parameters are placed in accumulators.
Extracodes
Another way in which controlled entry may be made to the supervisor is by means of extracodes, which were first used on the University of Manchester Atlas computer (Sumner, Haley, and Chen 1962).
It may be uneconomic or too inflexible to implement in hardware all of a computer's instruction set.
This is especially the case where we have an instruction set defined for a compatible range of computers, since it will be difficult to implement economically at the lower (and cheaper) end of the range the complex facilities required at the upper end (though microprogramming may be an answer).
An extracode is an instruction which has a normal instruction format, but attempted execution of which causes an interrupt to be taken into the supervisor, instead of causing execution by the hardware.
A subroutine within the supervisor performs the sequence of operations required by the instruction, and returns control to the instruction following the extracode.
Consider now a range of compatible computers.
Some functions available to the problem program (such as transput operations, or program intercommunications facilities) will be implemented as extracodes  on all models of the range.
However some functions (for example, floating-point arithmetic) may be implemented by hardware on the larger, and by software on the smaller, models in the range.
So the same instruction would, on one model, be executed directly by hard-ware and, on another, be recognized as an extracode and interpreted by a subroutine in the supervisor.
Notice now that the range is compatible only at the virtual computer level and not at the hardware level.
Since extracodes are defined by the way in which they are handled by the supervisor, we could make the supervisor react in an "extracode" fashion to any unallocated operation code.
However, extracodes are usually distinguished by the fact that operation codes have been allocated to them at the computer hardware design level, either because they will be normal hardwired instructions in some models, or because an instruction requirement has been recognized that is uneconomic to implement in hardware.
The invalid operation code interrupt and unimplemented instruction interrupt are thus distinguished at entry to the supervisor.
In the latter case the hardware may provide some assistance in interpreting the extracode.
Thus it may perform the effective address calculation from the operand and addressing mode fields of the extracode, before entering the supervisor.
Alternatively, assistance may be provided in the form of an "analyse" instruction, which treats a specified store location as holding an instruction, unpacks its various fields, and calculates an effective store address.
Process-switching and the PSW
Let us now consider the requirements of a multiprogramming system, where we have a number of programs occupying main storage and competing for an allocation of time on the processor.
We begin by introducing the term process, which is to be distinguished from the term program.
A program (or procedure) is a static sequence of instructions, while a process is the dynamic execution of the instructions of a program.
We must distinguish the two concepts since two processes may share a program of instructions (or, more generally, any set of computer words which are not altered, such as a table of constants).
Thus, in a situation where five users are interactively editing files from their own terminals, there would be five processes but only one program, the editor being shared among the users.
For further discussion of this concept see, for example, Lister (1975).
In a multiprogramming computer system, then, we have a number of processes competing for the resources of the computer, such as main storage areas, processor time, and access to transput devices.
Most of these processes will be problem processes, but some may be operating system processes, such as one reading job descriptions from a card reader into a job queue on disc.
It is a task of the supervisor to select (from time to time) the most suitable process to run, according to some strategy, and to switch between the process currently running and the new process.
While it is currently in execution, a process occupies certain registers within the processor, certain areas of main storage, and certain transput devices or parts of devices (such as disc storage areas).
Now a process's occupation of main storage and transput devices changes relatively slowly.
However, on each process switch we need to save the contents of all processor registers for the old process (the process context) and restore their contents for the new process.
What is needed, therefore, is a way of collecting together all the processor registers to allow rapid process switches.
Now, the accumulators nowadays usually form an array, which can be rapidly saved and restored with the multiple accumulator instructions of 2.4.
We then collect together all the remaining processor flags and registers into what is known as a process state word or PSW (some computer manufacturers say that the P stands for "processor" , or, incorrectly, for "program" ).
The PSW thus contains the program counter, the condition code (if present), the processor mode register, and other miscellaneous processor flags, such as overflow.
An interrupt causes the current PSW to be stored (in main storage) and a new PSW to be loaded.
This new PSW initiates the interrupt handling routine, a first task of which is to save the accumulator array.
To switch to a problem process, the supervisor restores the contents of the accumulator array; it then issues a special privileged instruction to load a new PSW from a specified area of store, and the processor reenters the problem process under control of this PSW.
Control registers
The implication has been that the PSW is one or two computer words long, since (except for the program counter) it consists of a number of short processor registers.
However, in many modern computers, we begin to require numbers of special processor registers to control  store mapping and protection, and other sophisticated facilities, with new privileged instructions to manipulate them.
Rather than expand the PSW to hold such a set of miscellaneous registers, we can replace them by an array of control registers, just as we introduced an array of accumulators.
Manipulation can be provided by store-and-control-register multiple-word move instructions (which are of course privileged), and the supervisor uses these to save and restore the control registers.
Notice that, unlike the accumulator array, where each accumulator had generalized functions, each control register or portion of a register has a specific function, although it is accessed or manipulated by instructions in a general way.
The IBM 370 range has a set of sixteen 32-bit control registers, which control various aspects of interrupt handling, multiprocessing, store-address mapping, process monitoring, and hardware-fault finding.
Multiple accumulator sets
A way in which process switching may be speeded up is by the provision of multiple processor register Sets.
Thus a computer might have four or eight accumulator arrays, and a special processor register (or field in the PSW) specifies the particular array being used by the currently running process (although privileged processes may have Some means of accessing other arrays than their own).
After a process switch, the new process can immediately use the accumulators without having to save the contents for the old process.
Usually there will be more processes than accumulator arrays, in which case one array is allocated to all the low-priority processes (the contents being saved and restored as before), and the rest will be allocated one array to each high-priority process, to improve process switching time.
Some computers provide several so-called processor modes, distinguished by the allocation of a separate accumulator set to each mode; one mode (the problem mode) is further distinguished by having privileged instructions disabled, while the others are all supervisor modes entered under different interrupt conditions.
The processor mode revisited
We introduced the processor mode register to distinguish two modes of running, the difference between the two modes being whether or not privileged instructions were enabled.
Further differences may distinguish  the two modes of running; thus supervisor mode might imply a particular accumulator array, no address mapping or protection, and ill interrupts disabled, while problem mode implies a second accumulator array, use of address mapping and protection, and all interrupts enabled.
If such an all-or-nothing division into two modes is too restrictive, we can have several intermediate processor modes between the supervisor and problem modes.
A fairly common system (such as on the DEC PDP-10 computer) is three processor modes; a problem mode, an intermediate mode with some restrictions in which much of the operating system is written, and an unrestricted mode for privileged manipulation of transput devices and processor facilities.
This inner ode may be called the kernel mode, in which case the intermediate ode is called the supervisor mode.
More flexibility still is provided by separate processor registers in the PSW to indicate processor mode (with respect to privileged instructions), whether address mapping is required, how the store protection system is to operate (if this is separate from the address mapping), which accumulator array is to be used, and the state of the interrupt system.
3.6 Microprogramming
The instruction set of a computer includes a range of instructions, from simple ones such as to load an accumulator from a specified store location, to complex ones such as the edit and search instructions described earlier.
However, in all cases the instructions can be broken down into a sequence of primitive operations on the various parts of the processor, such as the accumulators, the adder and the program counter; notice that some of these parts are not directly accessible to the programmer.
For example, the fetching and execution of a "store accumulator" instruction can be broken down into a sequence of more primitive operations as shown in Figure 3.19.
In practice this sequence would need to be expanded to include performing any specified address modification, and checking for pending interrupts before accessing the next instruction.
In a conventional computer design, the sequence of control signals execute the instruction set is wired into the control unit of the processor.
Instead we can design a computer which has as its instruction   set a set of primitive operations (or micro-instructions), to cause control signals to be Sent to the various parts of the computer, and data to be transferred from one processor register to another.
Then each instruction at the normal level of machine code (such as "store accumulator" ) is implemented as a sequence or microprogram of such primitive operations or micro-instructions; the more complex instructions of course require looping and conditional jumping in the microprogram, just as in conventional programming.
The computer holds the microprograms in what is normally referred to as a control store.
It is nearly always separate from the main store of the computer, since it is faster and more expensive, and is normally read-only, so that the microprograms cannot be inadvertently altered.
Computers with control units implemented in this way are called microprogrammed computers.
The concept was introduced by M. V. Wilkes in his paper "The best way to design an automatic calculating machine" (Wilkes 1951), and was first used to implement the control unit of the Edsac Il computer at the University of Cambridge (Wilkes, Renwick, and Wheeler 1958).
For further reading on microprogramming see Rosin (1969), Husson (1970), Davies (1972), and Tanenbaum(1976).
Figure 3.20 gives a simplified layout for a microprogrammed computer.
The control Store, containing up to a few thousand words, is set up initially with bit patterns representing the microprograms for the instruction set to be implemented.
Each micro-instruction is divided into two portions, the control bits and the sequencing bits.
Each control   bit position of the control store is connected to a unique control gate within the processor.
Thus the first two bits might be connected to the gates which control the data paths from the accumulator to each side of the adder, the next two to the gates which control the data paths from the SDR to each side of the adder, and the next one to the gate which controls the data path from the adder output to the accumulator.
Then if a micro-instruction contains the control bits 10011 followed by zeros, the paths from the accumulator to the left-side of the adder, from the SDR to the right side of the adder, and from the result latches to the accumulator are opened; thus the contents of the accumulator and SDR are added together and placed in the accumulator.
Encoded control
In a large computer there may be several hundred control gates in the processor, so it would be uneconomic to have a bit for each gate in the micro-instruction.
Instead of direct control of each gate by a control bit, we can have encoded control in which a group of bits control number of gates.
Thus N control bits could directly control N independent control gates.
However, if this group of N bits is taken as one field, then we can encode a range of 2N binary values; thus 2N gates can  be controlled (or 2 N -1, since one value may be required to specify that no gate is to be opened).
For example, to connect any of 16 accumulators to one side of an adder we need 16 bits with direct control, or 4 (=log2 16) bits with encoded control.
Notice that we can specify only one at a time of the gates controlled by such a field, so we must encode as one group only gates which need never be opened together.
Notice also that we need extra electronics and time in the micro-instruction execution cycle to decode the fields.
The control portion of our micro-instruction now takes the form of a number of short encoded fields, each specifying one of a mutually exclusive set of control signals (such as the function to be performed by the arithmetic unit) or one of a mutually exclusive set of processor registers as the source or destination of a transfer.
An extension of this is for one field (the emit field) to hold a short literal value which can be gated (under control of another field) to any of a number of destination registers.
Micro-instruction sequencing
The sequence bits of a micro-instruction hold the address of the next instruction to be executed, and this field is gated into the SAR of the control store at the beginning of the next micro-instruction cycle.
This   enables us to dispense with a micro-program counter, but at the expense of a next-instruction address in each micro-instruction.
To perform conditional jumps, the computer has to select one from a set of two (or more) next micro-instruction addresses.
One method is for the sequence field of the micro-instruction to specify all but the least significant few bits of the next micro-instruction address.
These last few bits are then specified by testing various conditions in the processor.
For example, suppose the adder made available as a secondary output the sign of the result of the add operation.
Then a field in the micro-instruction could open a gate to insert this output as the least-significant bit of the next micro-instruction address.
This is illustrated in Figure   3.21: the address of the next micro-instruction is 010110 if the adder result is positive or zero, and 010111 otherwise.
Some computers allow several different test functions to be connected as each of the least significant few bits of the next micro-instruction address, allowing 4-way, 8-way, or even higher order conditional jumps.
Horizontal and vertical micro-instruction formats
The micro-instruction format which we have described is very much oriented towards the hardware of the processor.
It is long (perhaps 50 to 100 bits), and is made up of a series of short encoded fields specifying gates to be opened and control signals to be transmitted.
An example (of the format of a micro-instruction for the Data General Eclipse computer, taken from Data General 1977) is given in Figure 3.22(a).
This layout is called a horizontal micro-instruction format.
However, some commuters are designed with micro-instruction formats much more similar to normal machine code instruction formats, especially those computers which are designed to be microprogrammed by their users.
Here we are trading user convenience against the extra logic levels needed to decode the micro-instruction.
We typically have a relatively short micro-instruction format (perhaps 16 bits), part of which specifies how the remainder of the micro-instruction is to be interpreted (that is, it represents an operation code).
The remainder of the format is usually taken up with a number of short fields, representing encoded processor registers or control signals.
This is called a vertical micro-instruction format: some examples (from the Burroughs B1700 computer, taken from Burroughs 1972) are given in Figure 3.22(b).
Micro-instruction sequencing in this case would be by the automatic incrementation of a microprogram counter; jump instructions are then required as in conventional programming.
Timing problems
The timing of operations is much more important in microprogramming than in conventional programming.
This arises in two ways.
First, the cycle time of the various parts of the computer are not hidden from the microprogrammer, as they are from the conventional programmer.
Thus, a microprogram may have a wait for a store read to be completed, or the result from a test function may not be available  for the micro-instruction sequence until several micro-instruction cycles after the function was initiated.
Second, there is the possibility (especially with horizontally micro-programmed computers) of simultaneously performing several operations under control of one micro-instruction if the data paths are available.
Thus we may be able to initiate an instruction fetch from store and increment the program counter in one micro-instruction.
The advantages of micro-programming
What are the advantages of implementing the control unit of a processor by microprogramming, Wilkes (1951) introduced the concept as a means by which the design and implementation of a control unit could be carried out in a Systematic and logical manner; this advantage is a Particularly valuable one today, when uniform electronic layouts are well suited to the technology of LSI.
Microprogramming was first used on a large scale in the IBM System/360 range of computers, where a common architecture and instruction set had to be implemented on a number of different models the range with widely-varying basic hardware.
It has since been used on many small computers as a means of implementing a rich instruction set at a reasonable price.
Here then, the advantage is economic; we can design a basic computer hardware to meet certain cost objectives, and then use microprogramming to implement the fetching, decoding, and execution of instructions from a rich set, compatible with the instruction sets of other computers.
As well as the advantages of uniformity (which include ease of maintenance) and cost, microprogramming offers greater flexibility than hardwired logic; it is easier to alter a control store than to rewire a control unit.
Thus a computer architecture and instruction set can be frozen at a later stage in the design process, and can be altered as a result of any inadequacies or improvements.
If a computer spends a significant portion of its time in carrying out a particular sequence of steps, then performance could be improved by replacing these steps by a single instruction implemented as a microprogram.
Thus in computers oriented to particular tasks, we can tailor the instruction set to the application area; several examples are given in Husson (1970, Chapter 3).
In the operating system area there are many tasks, such as process intercommunication and switching, or transput control, where heavily  used sequences of Steps could be written in microcode.
In the future much of a computer's Supervisor may be in microcode: see for example the microprogrammed operating system functions on the experimental VENUS System (Liskov 1972).
A further use of microprogramming is in writing programs to test for and diagnose faults in the computer hardware.
Because of the closeness to the hardware, such microdiagnostics can drive signals through particular data paths and isolate faults to a much smaller number of units than would be possible with programs written in machine code.
Emulation
One way in which the flexibility of microprogramming can be used is in providing several different computer architectures and instruction sets on one basic hardware.
Wilkes and Stringer (1953) suggested the provision of several control stores, one of which could be plugged into the basic hardware to create a particular architecture.
One particular time when we are concerned with several computer architectures is when transferring programs from an old computer to a new one.
Instead of converting the programs from one machine code to another, we could envisage writing a program for the new computer to simulate the old computer at the instruction level.
In a microprogrammed computer this simulator program could be written in microcode to improve performance; of course, the standard microcode in the new computer is really only a simulator for the new instruction set.
A more common way in which the simulation of the old computer can be improved is to select heavily used portions of the simulator program and replace them with individual specialized instructions (such as for instruction fetching and decoding); these new instructions are then implemented in microcode.
Such hardware-enhanced simulation is called emulation, it was provided on several models of the IBM 360 range, to emulate second generation computers such as the IBM 1401 and 7090.
Writeable control stores
We have so far assumed that the micro-instructions are held in a read-only control store, although we have considered the possibility of interchangeable plug-in control stores.
Instead some computers provide a writeable control store (WCS) which can be dynamically  loaded with a set of binary patterns as new micro-instructions.
Notice that, although we are now able to alter the control store, we need not have the general write-access available with normal main storage; what is required is the ability to reload a portion of the control store at rather infrequent intervals.
This might be done by a special (privileged) "load control store" instruction, which transfers a set of bit patterns in main Storage to an area of control store.
It is fairly common to have two areas of control store, one read-only (to hold microprograms for the computer instruction set) and one writeable.
Writeable control store is attractive in tailoring a computer for a particular application, but there are major problems, in potential loss of compatibility with the manufacturer's standard software or even between problem programs; this may be compounded by multi-programming, and errors in microprogramming (particularly as the latter is usually more complicated than conventional programming).
WCS is also useful for testing micro-instruction sequences, before lacing them in read-only storage for permanent use.
The Burroughs B1700 computer
Computer manufacturers have a continuing problem of protecting their investment in programming and yet persuading their customers to transfer to new computers.
One possible solution is the evolutionary approach, by which only minor enhancements are made to a pre-existing architecture so that all programs are upwards compatible to the new computer.
The emulators on third generation computers indicate another possible solution.
This is the concept of the soft machine, exemplified by the Burroughs B1700 computer.
This computer has no machine code instruction set or data formats in the ordinary sense.
Instead it provides a writeable control store and a micro-instruction set oriented wards the interpretation of what are referred to as S-languages (for secondary languages).
An S-language could be the machine language of another computer, in which case the B 1700 can emulate that computer if an interpreter is written in the microcode.
Alternatively it could be an intermediate language into which a high-level language is compiled.
Notice that we can have a difference intermediate language (and microcoded interpreter) for each high-level language, designed for the efficient and compact representation of the primitive elements of that language.
The micro-instructions provided for writing interpreters manipulate a number of general-purpose processor registers and function boxes (which produce various functions of two input arguments, such as the binary sum, or the logical disjunction), and access variable-length operands in main store starting at any desired bit address.
Thus the B 1700 computer can emulate, at the machine code level, any real or hypothetical computer; whether this is the way of the future remains to be seen.
Further details of the B1700 computer are given in Burroughs (1972), Wilner (1972a, b), and Tanenbaum (1976, pp. 204–11).
Problems
A 16-bit computer has two instruction formats: Two schemes are possible for allocating bit patterns to operations:(a) One 4-bit pattern (say 1111) in the 4 most-significant bits of an instruction word indicates the second format, while all other 4-bit patterns represent the first format.
(b) The most-significant bit of an instruction word indicates the format.
How many different instructions can be specified under each of the allocation schemes?
Why might a computer designer choose the first scheme?
3.2.
The instruction formats on the Data General Nova computer are shown in Figure 3.2.
Ascertain that, despite the variable length operation code, a binary pattern is always uniquely decodable into an instruction.
3.3.
When a programmer uses an assembler language, he uses  mnemonic instruction names and symbolic addresses.
Consider a computer with which you are familiar: to what extent must the assembler language programmer be aware of the precise format in the computer of the instructions he writes?
For example: does he need to know (and if so, when?) the length of the instruction (if it can vary), the order of fields within the instruction, the value in the operation code field, etc.
3.4.
Figure 3.5 shows the "group 1 instruction" on the DEC PDP-8 computer, which uses the two's complement representation for negative numbers.
The bits of this instruction can be set so that its execution leaves any one of the following values in the accumulator: How can this be achieved?
3.5.
Suggest a set of sequences of assignment statements, which you would expect to be typical of programs involving arithmetic calculation.
Making reasonable assumptions about instruction lengths and speeds (see any recent computer manual), compare the time and store requirements for executing these sequences on computers with zero-, one-, and 1½-address instructions.
Estimate how difficult it would be for a compiler to generate efficient code for these three architectures.
Perform the same analysis for the Data General Nova computer (see figures 3.2 and 3.3).
3.6.
Suppose a computer has an array of accumulators (with the appropriate instructions), and accumulator zero is the computer's program counter.
What effect could this have on the design of the computer's jump instructions and addressing modes?
Is it a good idea? 3.7.
For a computer to which you have access, list all the transfer of control instructions.
Are symmetric conditions provided: that is, "if jump on accumulator ≥0" is provided, is "jump on accumulator <" also provided?
Is the condition encoded in the operation code or in a subsidiary information field?
Are the addressing modes provided the same as for data manipulation instructions?
3.8.
A simplified version of the "group 2 operate instruction" on the DEC PDP-8 computer is given as a boolean expression on page 103.
How would you describe the significance of bits x, y, and z in a form suitable for a programming manual?
3.9.
An "increment store location and skip if zero" instruction is described on page 105.
How could this be used for controlling a repetitive loop?
3.10.
Suppose that an instruction which specified indexed addressing could also specify whether the index value was to be modified (for example, it could be incremented by one).
How would this affect the provision of instructions for performing loops?
How useful would it be?
How is it related to stack manipulating instructions and to "auto-indexing" (see 4.3)?
3.11.
In 3.3 and problem 3.10 a number of different groups of instructions were mentioned, which involved modifying the contents of an index register.
List the different methods by which this modification value could be specified, and discuss their relative advantages and disadvantages.
3.12.
Early computers did not have a "subroutine call" instruction.
One technique for getting round this problem was mentioned on page 110, whereby a jump instruction in the subroutine was modified before entry.
Another technique (the "Wheeler linkage" ) involved constructing a return jump in the accumulator at entry to a subroutine.
Program this on a computer with which you are familiar.
Extending this idea, we could provide an instruction "load accumulator with contents of program counter" and this, together with an unconditional jump instruction, would suffice for subroutine entry and return.
Discuss this proposed method.
3.13.
A character string consists of words separated by commas or spaces.
Show how the "translate and test" instruction described on page 117 can be used to search for the first comma.
3.14.
A{ declaration}is defined in BNF as: Using the "translate and test" instruction, show how a program could be written to check any character string against this definition, and extract the{ ident}s in order to look them up in a dictionary.
3.15.
Consider a computer with multiple arrays of accumulators and control registers, in which a process's complete context is contained in one array of accumulators and one array of control registers (in particular, the program counter is a control register).
What information needs to be changed at a process switch, and therefore how long is the PSW and what does it contain?
3.16. (a) What difficulties would you expect to arise in writing a microprogram?
(b) Some microprogrammed computers allow the next micro-instruction to be specified by loading into the control store SAR the contents of some portion of a processor register.
Why would this help with the microcode's main task, of decoding and executing machine instructions?
