Electronics World + Wireless World September 1991 £1.95
Computing
What's in a graphics file?
Hypothesis
Why power lines kill
Circuits
The very best from the US
Development
Working with DSP
Design
Scientific magnetometer
Contents
Features
726 Cover: Fluxgate magnetometery
Measuring the directions of the Earth's magnetic field provides the basis of an electronic compass while the strength of the field says much about solar activity — a factor important to HF radio propagation.
Richard Noble describes a science quality instrument.
738 What's in a graphics file?
David Bacon looks into the proliferation of complex graphical file formats and explains their structure and how information is transferred.
743 Review — Table Curve
Manually adding best-fit curves to data plots can be laborious and prone to error.
But Don Bradbury reviews TableCurve and finds it has all the right lines.
747 PC Review: Hard to resist?
Rescalc 2.0 resistor calculator has been much improved since its original release.
Ben Duncan analyses the differences.
759 A toolbox for DSP engineering
Allen Brown looks at how to choose the best DSP engineering tools for application in a fast-developing market.
764 Lessons in applied science
Some of the most exciting and practical research in electrical, electronic, fibre optic and communications engineering is being carried out at London's Kings College.
Andy Gothard reports.
774 Killing fields: a cause?
Living close to overhead power lines may increase the risk of cancer.
Harold Aspden hypothesis on cyclotron resonance as the cause.
777 Waves, wind and the Teredo worm
Plans for global communications now often seem to rest on the semantics of international standards.
Greg Grant looks back to the Victorian adventurers who conquered nature to put a communication girdle around the world.
788 Who designed this?
How close is the innovative audio circuit patented by Matsushita in 1983 to the Class S amplifier designed by Aubrey Sandman in 1982.
Dr Sandman presents his case.
Regulars
715 Comment
Analogue changes
716 Update
Getting the measure of superconducting buckyballs, is £420m enough for EC R?
720 Research notes
Discovering new galaxies, superconducting in a perfect world, the coldest place on earth, fast SiGe devices, high speed low power flip-flop, and did you know…
743 Letters
Cuk vs Buck, watching HDTV, whither Class D, Ferrite aerial response, telepointless?, any old valves, light hearted.
748 Circuits, Systems and designs
In the first of a regular series, we bring together some of the practical designs and circuits which first appeared in the US magazine EDN.
753 Circuit ideas
Sound sampler filter, multimeter as frequency meter, sine waves from 4046 VCO, motion direction detector, D-to-A converter current booster, speech compressor.
771 Applications
Amplifier for microphones and moving-coil pick-ups, low-power frequency synthesizer, power mos electronic ignition, applying multipliers.
783 New products
EW + WW 's round-up of all that's new in electronics.
794 Broadcast
Could RDS change the shape of broadcasting?
Starting next month: Technical software discount purchase scheme.
See page 776 for details
In next month's issue.
John Linsley Hood on filters.
JLH provides the definitive guide to audio filter design.
Change in publishing date
Starting with the October issue,Electronics World + Wireless World will appear about week later than before.
The publishing date will be last Thursday in the month.
Hence the October issue will appear on September 26.
Comment
Analogue changes
Editor Frank Ogden 081–661 3128
Deputy Editor Jonathan Campbell 081–661 8638
Design &Production Alan Kerr
Editorial Administration Lindsey Gardner 081–661 3614
Advertisement Manager Jan Thorpe 081–661 3130
Display Sales Manager Shona Finnie 081–661 8640
Advertising Administration Kathy Lambart 081–661 3139
Advertising Production Neil Thompson 081–661 8675
Publisher Rob Marcus 081–661 3930
Facsimile 081–661 8956
You can't please all of the people all of the time.
And there are a few people who won't be pleased whatever you do.
Thus when we present an article on structured programming electronics, a proportion of our readership will relate the subject to their own interests while an equally large cross-section will simple regard its inclusion as a displacement of their own preoccupation.
We can't win but we must keep trying.
Although the electronics industry has changed greatly, possible the greatest change is that very little component level manufacture is done in this country.
In general, designers no longer work with a heap of transistors and a hot soldering iron.
Systems are build from board level products designed and manufactured somewhere in the Far East.
Their assembly into something functional requires the subtle application of a screwdriver allied with a complete grasp of the operating software.
Indeed, there is very little in the real world of electronics which doesn't involve digital techniques and computing.
DSP seems a particularly important topic since it represents the doorway between the way that people do things and the tasks which they expect electronics to do.
One could argue that digital technology isn't the only way to solve a particular circuit problem; a nifty bit of design work with a couple of op-amps plus a few Rs and Cs can replace a complex digital filter system.
Furthermore, it can be done in a language with which many EW + WW readers will be totally familiar.
However, the same little collection of components wouldn't allow a robot arm to interpret the signals from a CCD sensor in a manner which allows the arm to be placed reliably alongside an item on a conveyor belt.
And if the robot control function requires a signal filter, it would seem sensible to incorporate the filter block as part of the DSP recognition software rather than reach for discrete analogue components.
The same arguments for use of digital electronics apply equally to communications systems, test and measurement, broadcasting and consumer electronics.
Indeed, if we as a magazine were to reflect accurately the role of ‘traditional’ design relative to its actual usage, we might manage about one page per issue.
It should be said that we have no intention of implementing this ratio.
Analogue design remains important because it accommodates and defines the basic building blocks of electronics.
It is very hard to design a house if you don't know the shape of the bricks.
While there are a few things in digital technology which have no possible representation in analogue form — infinite impulse response filters are a good example — most digital blocks derive their shape from the original analogue function.
There is another aspect.
Although the changing voltage on a line connecting a pair of gates may be classed as digital, it remains an electrical signal like any other.
A design engineer might well require an appreciation of transmission line theory to ensure that the two connect together without data corruption.
The same goes for the avoidance of earth loops, common mode inductance nodes and all the other pitfalls.
We will continue to accord analogue design the importance which it deserves.
But we must also acknowledge the significance of computing and digital design.
EW + WW has enjoyed an illustrious past by presenting the important technology of the day.
We must continue to do this if we are to safeguard its future reputation.
Frank Ogden 
Reed Business Publishing Group
REGULARS
UPDATE
EC announces £420m for R— but is it enough?
Research grants worth £420million have been announced by the EC, revealing its plans for the third set of Esprit projects.
But it may not be enough according to some industry commentators.
US-based companies will play a major role for the first time, but the Japanese are not expected to be heavily involved.
Main change introduced in the Esprit III programme is a new category of research called the Open Microsystems Initiative (OMI).
Its aim is to establish Rprojects so as to: ‘Make the European Industries competitive in the design, manufacture and use of future integrated circuit microsystems,’ according to the EC's documents inviting companies to take part.
US companies are expected to be involved in the research, backed by £65 million of EC money.
Rosalie Zobel, deputy director of the Esprit projects told EW + WW that Mips Computer Systems, Sun, Motorola, DEC and IBM have all been invited to preliminary OMI meetings.
Motorola is interested in joining some of the projects, according to Barry Waite, general manager of the company's European semiconductor group.
But he questions whether the programme as a whole is too ambitious.
‘I'm very sceptical whether the grand aims can be met in that time scale with that amount of money,’ he says.
Zobel said that there had been no approaches from Japanese companies wanting to work in the OMI.
The Esprit III work programme, which is backed by 600 million ECU, is split into six sections.
As well as the OMI and microelectronics research, there will information systems and advanced home systems projects.
Rob Causey 
Unappealing face of Blumlein biography
Francis Thomson, the man who for nearly twenty years has been promising to write a biography of Alan Blumlein is once again appealing for more material on Blumlein.
Thomson's latest letters to the media very, but in some he explains the decades of delay with a puzzling tale of his attempts to ‘shield the Blumleins from the belated discovery that a person close to them had supplied me with letters…stolen from the late Mr A K Van Warrington’.
As before, Thomson, who has also put out calls for biographical material on other subjects, including S G Brown, signs himself a Member of the Institution of Electrical Engineers, helping establish credibility with editors unaware of the fact that he started collecting original papers in 1972.
But he has published nothing, refuses third parties access to his collection and will not catalogue what he has.
Although some editors have treated the ‘biographer's’ latest call with suspicion and either ignored his request or published it with qualification (eg Hi-Fi News , other have simply regurgitated Thomson's plea for ‘letters, notes and photographs’.
Unqualified appeals have already been published in Television (journal of the Royal Television Society),Image Technology ,(journal of the BKSTS),Nature and the AES Journal.
Charlie Chester obligingly broadcast the appeal on Radio 2.
The IEE has tried to negotiate with Thomson, offering help with writing and publishing his biography in time for the fiftieth anniversary of Blumlein's death in June 1992.
But Thomson has rejected the IEE's offers and it now looks certain that the anniversary will be missed.
Some members want the IEE now to use more muscle, with threats of expulsion from the Institution, if Thomson does not at least provide a catalogue of the material he has collected and give rock solid guarantees on its long term security, after his death.
Barry Fox 
Preparing to test an underwater optical fibre cable joint on the 600kV DC test set at the new high voltage laboratory at Southampton University.
Photo STC Submarine Systems
Superconducting buckyballs
A new method of producing very pure samples has allowed researchers at Los Alamos National Laboratory and UCLA to make two crucial measurements of superconducting compounds built with ‘buckyballs’.
The compounds' responses to magnetic fields and pressure have boosted scientists' hopes that they will form the basis of practical superconducting.
Buckyballs, of buckminsterfullerene, are soccer-ball-shaped assemblages of 60 carbon atoms.
Earlier this year researchers learned that inserting a few atoms of potassium into a framework built of buckyball molecules makes the compound a superconductor: below a critical temperature of about 19 Kelvin — or 19 degrees above absolute zero — the material conducts electricity with no resistance.
The Los Alamos and UCLA researchers measured the critical magnetic fields of the material — which point to how useful the superconductor may be.
The researchers relied on the ability of the material to repel a magnetic field, a trademark of superconductors.
They cooled the sample below its critical temperature so it became superconducting, then applied an increasingly strong magnetic field.
At a certain field strength — the lower critical field — the magnetic field began to penetrate portions of the sample and so the sample was a mixture of superconducting and normal states.
At the upper critical field the magnetic field completely penetrated the sample and it reverted entirely to its normal state.
The upper critical field was estimated to be about 800,000 times stronger than the earth's magnetic field, advantageous for applications of the superconductor.
Researchers used the measurements to estimate the value of the critical current, or how much electricity the material can carry in its superconducting state.
‘The critical current calculated is already higher than the critical current first reported for the high-temperature superconducting materials based on copper-oxide compounds,’ says George Gruner, director of UCLA's Solid State Science Centre.
Researchers have also measured the response of the superconducting properties of a potassium-doped buckyball compound to changes in pressure — one of the fundamental measurements of any new superconductor, as it sets immediate constraints on the theoretical interpretations of the material's behaviour.
Results showed that as pressure was increased, critical temperature of the material dropped dramatically, a behaviour opposite to that predicted.
In general, increasing the pressure on a material pushes atoms together, stiffening the lattice.
Under certain circumstances, this allows an electrical current to begin moving without resistance at a higher temperature, as found in the high-temperature copper-oxide superconductors.
In copper-oxide superconducting materials, this suggest that to raise the critical temperature you want to make the lattice smaller.
But the buckyball compounds' opposite behaviour suggest that researchers should pursue ‘negative pressure’, created by inserting a larger atom into the lattice.
In fact doping the buckyball compound with rubidium — a larger atom than potassium — raises the critical temperature to nearly 30K.
Researchers at ATBell Laboratories reported the discovery of superconductivity in buckyball compounds at the American Chemical Society meeting in April.
But only a small portion of their material had superconducting properties.
The UCLA team has refined the method for manufacturing sample of potassium- and rubidium-doped buckyball compounds so that the entire sample is superconducting.
They believe that the measurements were possible because of the high quality of the samples prepared by the UCLA scientists.
Russia is hi-tech
There is no technology gap between the Soviet Union's and the West's electronic research activities.
But the country's production facilities lag the west by about ten years.
The assessment is made by analysts Instat, following a visit by Instat's European president, Malcolm Penn, to the Soviet Union in April to assess its technical capabilities.
His view is that what the Soviet Union has in small quantities is as good as the technology of the West, but that the country lacks the ability to turn out production volumes.
One facility visited, NPO ‘SVT’, is developing a Mezaepiplanar cmos process using ferro-magnetic masks — giving the ability to make non-planar ICs without relying on photolithography.
Penn says that engineers have built chips on the outside of cylindrical substrates.
The masks hold the key to the technology.
They are made using precision alloys which are thermally matched to the temperature expansion characteristics of the substrate materials.
Each mask is pulled down onto the base, which can be variety of shapes, by a magnetic catcher.
Plasma etching and ion implantation techniques are then used to build up transistor structures, rather than diffusing them into the substrate material.
Various materials can be used as a base, such as silicon, gallium arsenide, germanium and some metals, cmos, bipolar and bi-cmos circuits have been built.
The features on the masks can vary in size between 10 and 0.2mm with the whole mask measuring up to 400 by 500mm.
They range from 0.01 to 0.5mm in thickness and are made from copper, nickel or iron-nickel alloy depending on the application.
By avoiding the use of some of the more active chemicals needed for photolithography, the Soviet scientists believe the process will reduce the environmental damage caused by chip fabs.
They hope to make between 10,000 and 100,000 chips a year using the equipment.
Rob Causey 
The BBC is to shut down its shortwave broadcasting facility at Daventry.
The station, which also houses the longwave domestic service has been in continuous operation since 1925.
Shortwave broadcasting began from the site in 1932.
The photograph shows the station as it was in 1937; its appearance has changed little to the present day.
Euro-supercomputing boost
Europe's supercomputing industry has received a boost with unveiling by German company Parsytec of plans for a new range of parallel machines.
Parsytec intends to use the same basic design of a 1.6Gflop processing unit as a building block for systems executing up to 400Gflops. the largest computers on sale now can perform tens of Gflops.
Parsytec will sell the single processing unit, about the size of two briefcases, and larger computers made up of different configurations of the blocks.
‘We set design goals to make a 400Gflop machine.
Then we broke it down into smaller components to set us scale it down.’
said Francis Wray, head of product development.
The company has used a modified version of the design in its proposal for the European Teraflop initiative.
This is a project aimed at building a computer which can process one million floating point operations per second.
The systems will be built using Inmos' T9000 transputer.
Parsytec has received assurances from Inmos that it will be able to buy the transputer in large enough numbers to build the computers.
Each 1.6Gflops module, of Giga Cube, contains 18 T9000s. 16 for processing, one redundant device and one configuration controller.
If a failure occurs in one of the 16 on-line chips, the controller detects it and switches in the redundant chip.
The array of processors is linked using four of Inmos' C104 communications chips.
The Giga cubes each have their own cooling system.
Heatpipes draw energy away from the electronics and pass it into a cooling unit where either air or water is used to dissipate the heat.
Systems processing up to 50GflopS, with 16 Giga Cubes, can be cooled by air.
A water system and a lot of plumbing is needed for bigger machines.
Rob Causey 
Digital TV connection
The European Eureka programme has brought together 32 organisations from 12 countries in a bid to develop the technology needed to bring all-digital TV to the home or office.
The Vadis project (video-audio digital interactive system) aims to develop coding techniques to compress digital TV signals by a factor of between 20 and 40 while retaining most of the original quality.
The compression will let digital audio-visual services be carried by terrestrial and satellite channels, telecommunications networks or digital storage devices.
Companies involved include Aspex Microsystems, the BBC, BT, Italtel, Philips, Olivetti, Siemens and Thomson-CSF.
UK attracts foreign money
Record numbers of overseas companies are investing in the UK, according to industry minister Edward Leigh.
In the year ending March 1991 he says there were 350 direct investment projects by foreign owned companies into the UK, recorded by the DTI's Invest in Britain Bureau.
Can Babbage help Science Museum balance books?
Charles Babbage's Difference Engine will form the centre-piece of a new gallery at the Science Museum next year — if fundraisers can find the £4.5million needed to build the new show.
The working version of the machine, built to celebrate the bicentenary of the mathematician's birth, will be moved into the new Information Age gallery which will replace the current computer gallery.
The museum hopes to display and explain telecomms systems as well as the history of more traditional technology.
‘We're at the final state of putting proposals together.’
says Lynn Foster, the Science Museum's main fundraiser.
‘We hope for support from telecomms as well as computer companies and users’.
The museum believes that the fundraising will take about eight months.
Once the commitments are in place, it will then need to decide which exhibits its wants to build.
There are examples of the interactive displays in the current Babbage exhibition, which runs until 31st December.
UCM, a software house from Hove, has developed an animated video showing the workings of the Difference Engine No 2.
RC 
Toshiba-IBM LCDs on-stream
Display Technologies, a Toshiba-IBM joint venture, has completed construction of its manufacturing plant in Himeji and has started production of large size, LCDs for computer terminals.
The plant produces 10.4-in (640 x 480 pixel) LCDs and will gradually add large size units.
The plant is said to be a state-of-the-art facility that forms transistors onto large glass substrates, in a class-100 clean room that has the similar level of air cleanliness to those used in the manufacture of LSI chips.
Total production management is coordinated using a cim system.
Superconducting gradiometer
Researchers at IBM TJ Watson research centre report the first high-temperature superconducting magnetic gradiometer to operate at liquid-nitrogen temperatures.
The ability to operate at these temperatures is advantageous, because liquid nitrogen is cheap and the devices need less thermal insulation.
Fabrication of the gradiometers — which measure magnetic gradients — relies on IBM's wet-etching process.
The technique depends on a highly selective etchant solution that etches patterns in the thin film of material used to insulate layers of superconductor in the thin films of material used to insulate layers of superconductor and stops when the superconducting is reached.
David Edward Hughes, whose experiments with the ‘printing telegraph’ led to the development of an efficient microphone during the 1870's, has been honoured with a plaque at his former lodgings.
Exactly how long Hughes lived at 94 Portland Street, London, is uncertain but the first experiments which led to the development of the microphone were conducted here during 1878–80.
The house is prominently sited on the corner of Portland Street and Langham Street.
RESEARCH NOTES
New galaxy throws bright light on quasar formation
A British-led team of astronomers has discovered a new very-distant galaxy which is radiating three hundred million million times as much energy as the sun.
The galaxy, known as IRAS F10214+4724, is in fact the most luminous object ever found in the universe.
But it is not radiating its energy at visible wavelengths like previous contenders for the title, all of which were quasars (quasi-stellar objects).
Instead it is a massive cloud of dust, radiating 99% of its energy at far infrared wavelengths.
Astronomers believe (Nature Vol 31 No 6329) that it is either a massive galaxy in the process of formation or a quasar buried within a dust cloud.
The team, led by Professor Michael Rowan-Robinson of Queen Mary and Westfield College, University of London, includes astronomers from Cambridge, Oxford and Durham Universities as well as QMW, and from Caltech, Pasadena and Charlottesville, Virginia.
They discovered the object with the UK's 4.2m William Herchel telescope (WHT) on La Palma in The Canaries on the last night of a 40-night observing campaign, studying very faint sources detected by the IRAS infrared astronomical satellite with optical ground-based telescopes.
Subsequently the team obtained images of this mysterious object with the 200in Mt Palomar telescope and the WHT.
It was slightly fuzzy and presumably a distant galaxy.
The spectrum was that of a very distant galaxy with redshift of 2.3 (spectral lines shifted in wavelengths towards the red by 230%).
Such information implies a distance of 16 thousand million light years and that the galaxy is seen at a time 83% of the way back to the Big Bang.
Previous high redshift for a galaxy or quasar discovery by IRAS was 0.4.
The powerhouse responsible for heating up the dust is hidden from view but the team believes it is either a quasar or a massive burst of star formation.
In the latter case we would almost certainly be seeing the galaxy during its formation phase.
Either way it is a unique object and will shed light on how galaxies and quasars form.
Superconducting in a perfect world
If you were to draw up the specification for the ideal material for electronic engineering, I guess it would be something combining the properties of the perfect conductor, the perfect insulator and the ideal semiconductor.
Such a material does not of course exist…or does it?
A paper recently published by Jacek Baranowski et al working at the University of California, Berkeley and the Lawrence Berkeley Laboratory (Phys Rev Lett , Vol 66 no 23) describes a new form of gallium arsenide that will superconduct at a temperature of 10K.
LT-GaAs as it is called, is made by molecular beam epitaxy (MBE) at the relatively low temperature of 200°C.
Chemically it contains about 1.5% excess arsenic in the form of crystal defects.
At ordinary temperatures the material is quite conductive.
But below 10K it is possible by means of field-modulated microwave absorption (FMMA) and the Meissner effect (magnetic field expulsion) to demonstrate clearly the existence of a hitherto unknown superconducting phase within the material.
Baranowski and his team attribute this to a layered arrangement of atoms — visible in the electron microscope.
In addition to this superconducting phase, and the well known semiconducting properties of GaAs, LT-GaAs can be made into a well-nigh perfect insulator by annealing at 500°C so that As-rich material precipitates out.
Such insulating GaAs can be used for gate isolation in metal-insulator-semiconductor fets and, because of its short minority-carrier lifetime, in high speed optoelectronic devices.
With such a range of useful properties, the Berkeley team envisage as chip in which insulating, semiconducting and superconducting components are fabricated together, based on epitaxial layers.
This, they say, might result in completely new directions for device technology including epitaxial superconducting interconnects, junctions and switches.
Obviously there is a price to be paid in holding the liquid helium temperatures.
But the group is convinced that it is only a matter of time before someone discovers superconducting semiconductors that will work at the economically viable temperatures of liquid nitrogen.
Optical image of IRAS F10214+4724 taken with the Mt Palomar telescope.
Ellipse shows the estimated location of the infrared source.
F if the high redshift galaxy.
Colorado: absolutely the coldest place on earth
American scientist working for the Astrophysics Institute at the University of Colorado in Boulder have created the lowest temperature ever recorded — one millionth of a degree above absolute zero.
This is approaching the temperature below which theoreticians expect atoms to fuse together in a new kind of matter.
When things are cooled to within a few degrees of absolute zero — minus 273.16°C — strange things happen.
Metals and some new ceramics lose their electrical resistance completely.
In addition to superconducting, another spectacular low temperature phenomenon called superfluidity occurs.
At about 2° above absolute zero — 2K — liquid helium develops the peculiar ability to flow through tiny holes without friction.
Small wonder that scientists are searching for every lower temperatures, nearer still to absolute zero, where even more mysterious effects may be waiting.
But it is no easy task.
Conventional refrigeration, where heat is removed by evaporation, will work down to about 1K.
Below that, different approaches are necessary, until recently involving extracting energy from atoms using magnetic fields.
Heat is nothing more than energy of atoms in motion, so if you can slow down atoms, you are in effect cooling them.
Within the last few years, physicists have come within a few thousandths of a degree of absolute zero using a new technique called laser entrapment.
The idea is to hit atoms head-on with laser beams, slowing them down.
Laser beams consist of photons which, though small by atomic standards, have a cumulative inertial effect, Dr Carl Wieman of the University of Colorado, who heads one research group, says that cooling an atom with a laser beam is like trying to slow down movement of a heavy object by bombarding it with ping-pong balls.
In practice it is quite complex because you need at least six lasers to trap a clump of atoms; the lasers also have to be tuned to the natural movement of the atoms.
Otherwise the effect is to add heat energy rather than remove it.
What Wieman has done — as well as reaching the lowest temperature on record — is to simplify what has been an immensely complex piece of laboratory apparatus.
Not only has the essential large vacuum cell been reduced to a small glass cell, but Wieman has also been able to replace gas lasers with solid state devices.
In fact what he has been using are exactly the same as the cheap mass-produced lasers used in CD machines.
They cost around £50 a piece, less than a tenth the cost of the gas lasers.
Wieman told EW + WW that the great benefit if this simplification is that it will enable a greater number of laboratories to participate in low temperature research and discover more and more bizarre properties of matter.
Most exciting is creation of a ‘Bose-Einstein’ condensate — a weird state of matter, predicted by Einstein and the Indian physicist Satyendranath Bose, where individual atoms lose their identity and merge into a kind of atomic soup.
When asked what temperature at which this transformation would occur, Wieman commented: ‘It depends on the density and the number of atoms in your trap, the more atoms you've got, the higher the temperature.
We're at one millionth of a degree above absolute zero.
We need to get between ten and a hundred times lower’.
Design opens up fast SiGe devices
Researchers at IBM have built an experimental p-channel mosfet that uses both silicon and germanium and delivers high performance.
Scientist believe the device — which makes use of novel design and new materials — could lay the ground for a promising future for silicon-germanium transistors in future chips.
Details of the improved mosfet were revealed at a recent VLSI technology symposium in Japan.
The mosfet is less than 1µm wide and operates at liquid nitrogen temperatures (-196°C).
Improved performance is obtained through building an ultra-thin channel of silicon-germanium alloy under the silicon surface of the mosfet to restrain mobility of the charges carrying current through the device.
The germanium layer confines the charges just under the skin, preventing them from bumping into the surface and scattering from their designated path through the transistor.
Researchers built the channel so that it is free of dopants, stopping the charges from bumping into impurity atoms along the way.
Instead, a layer of doped silicon is placed directly underneath the channel.
This dopant layer sets the voltage at which the transistor turns off and on.
As a result, the mobility of the charges as they travel through the transistor is twice as good as any similar, submicron p-channel mosfet transistor yet built.
A p-channel transistor, relying on hole mobility, is usually slower than an n-channel device.
The goal is to use the improved p-channel transistor to boost the speed of complementary circuits where both types of transistors work together.
Complementary metal oxide semiconductor (cmos) circuits save power, allowing designers to put more circuits on a chip.
Dr Carl Wieman (left) and his team has opened up research into low temperature physics.
Tests confirm high speed/low power flip-flop
A team at the Fraunhofer Institute for Applied Solid State Physics in Freiberg has reported a breakthrough in high speed frequency dividers.
What the German group has done (Electronics Letters , Vol 27 No 13) is to create a static master-slave flip-flop which operates at 14GHz but has a dissipation of less than 35mW.
Until now, there has been a choice between high speed (22GHz) together with high dissipation (>per flip-flop) or medium speed (9–12GHz) with medium dissipation (<).
The development, they say, will make it easier to integrate high-speed prescalers onto counter circuits, multiplexing circuits and other multi-function chips used in communications.
The circuit comprises two D-latches, each constructed in source-coupled fet logic.
(In the diagram, E indicates enhancement mode and D depletion mode fets.)
To achieve the extremely low power consumption, D-fet active loads were used with gate widths of only 5mm.
This ensured that pumped charge on the chip was kept to a minimum, helped by a reduction in wiring capacitance.
On test with no load the divider chip consumed only 19mA at a supply voltage of 1.8V.
Maximum operating frequency was 14.2GHz at which the minimum required input power was 8.3dBm.
At lower frequencies (<), the flip-flop toggled reliably will less than — 15dBm on the input.
Psst — want to know a secret?
Did you know that upgrades to the EW + WW office software mean that in future it will be possible to print three-dimensional graphics and stereoscopic photographs?
Unfortunately, I'll have to come clean; that story is only a rumour.
But while some rumours are harmless — like the persistent one about certain restaurants serving up dead dogs — others can be very damaging for businesses, especially in the volatile electronics sector.
Over in the US, psychologists at a number of universities are making a serious study of how rumours are born, how they propagate and how they can finally be killed off.
The analogy is so close that Alan Kimmel of Fitchburg State College describes rumour (New York Science Times , June 4 1991) as a sort of opportunistic virus that thrives on fear and uncertainty.
Some rumours, he says, have survived for centuries, merely by mutating and reappearing in a different guise.
What, then, about anti-rumour software?
As a personal contribution to the stability of British industry (who on earth invented that one?) may I pass on a few nuggets of advice:
1.
If possible, track the rumour back to its origin.
In nine cases out of ten it turns out to be the office cleaner's milkman's financial adviser.
2.
Evaluate the rumour.
See if it really is lowering sales of demoralising staff.
Check how many people believe it.
3.
Plan a counter-attack immediately.
Check all the facts and refute them with sound evidence.
Latest research suggests that a firm rebuttal will kill any rumour within 24 hours.
…
Tell that one to the Palace press office!
D-latch constructed in source-coupled fet logic.
E indicates enhancement, D, depletion mode fets.
Design
Fluxgate magnetometery
Measuring the direction of the Earth's magnetic field provides the basis of an electronic compass while the strength of the field says much about solar activity, a factor important to HF radio propagation.
Richard Noble describes the heart of an instrument which can be used for serious scientific study.
Traditional magnetometers use inductive loops and long wire lines to measure magnetic flux by induced EMF.
Such instruments are bulky.
Hall effect semiconductors will measure low flux levels although their response tends to be non-linear and temperature dependent.
The fluxgate magnetometer, which depends for its action on the detection of saturation occurring in magnetic material, can be made both small and highly accurate.
In explaining its operation, it is easier to describe a simpler arrangement that would actually be used in practice.
In its most basic form, it comprises a single straight nickel-iron alloy core carrying two windings.
One winding functions as an excitation coil in which current flowing creates a field to magnetise the core in alternate directions.
The other acts as a pickup coil producing a voltage proportional to the rate of change of magnetic flux linking it.
At low levels of excitation this structure, outlined in Fig. 1, will obviously behave as if it were an inefficient transformer.
To convert it to a fluxgate transducer, the excitation is increased so as to force the core into saturation on alternate peaks.
This would be undesirable behaviour in a transformer but is the essence of fluxgate operation.
A square voltage waveform is applied to the excitation coil, of sufficient magnitude to saturate the core.
During the periods when the core is not saturated, the inductance of the coil causes the current passing through it to change linearly from one saturation state to the other.
As the core reaches saturation the inductance falls to a low value and the current rises rapidly to a limit set by the DC resistance of the coil forcing the core well into saturation.
The waveform of this current is shown in Fig. 2 which also serves as an illustration of the magnetic field produced by the coil, since this must be directly proportional to the current.
The induction in the core is magnified by the high permeability, except in the saturation regions where the result can be seen to be a trapezoidal induction waveform (Fig. 3).
The voltage induced in the pickup coil by this waveform is directly proportional to the rate of change of magnetic induction.
During the periods of saturation, there is no change and no signal voltage, but during the linearly changing regions there is a constant voltage output of appropriate polarity, as shown in Fig. 4.
All of the above assumes that the only influence on the core is the magnetic field produced by the excitation coil.
If this is not the case and the core is affected by an additional external field, a small change takes place in the output.
The component of external  field which is in line with the core axis either aids or opposes the excitation field in its alternating polarity phases.
When it aids the excitation field, the entry into saturation occurs slightly earlier and the departure from saturation occurs slightly later than would be the case.
The opposite effect arises when the external field opposes the excitation field.
For clarity, this is illustrated in very exaggerated form in Fig. 5.
The amount of advance and delay in the waveform corner is proportional to the size of the external field.
In this way the desired object has been achieved in the form of a signal variation which is the function of the external magnetic field strength.
It remains to exploit this.
Since a perfectly symmetric waveform foes not contain even harmonics and an asymmetric one does, a possible technique would be to isolate the even harmonics as the usable signal.
However the presence of very large fundamental and odd harmonic components in the output makes the isolation of the small even harmonics a very daunting task.
A Better Technique
Fortunately there is a very effective simple solution in the form of two parallel cores with opposing excitation windings and a single overwound pickup coil (Fig. 6).
The inductions in the two cores cancel out precisely when the cores are placed in a zero external field; an external field causes asymmetric pulses of amplitude and position dependent on the polarity and strength of the external field.
The resultant waveforms are shown in Fig. 7.
In this way, the trick of using two cores has performed the seemingly impossible task of isolating the tiny wanted signal from the comparatively huge unwanted one.
If the pulses marked with arrows in Fig. 7 are isolated from the others and applied to an appropriate low pass filter, the output is a DC or slowly varying voltage whose magnitude and polarity model the external field.
It would appear that isolating the set of pulses not marked with arrows would work equally well and, in fact, combining both in a suitable way could double the sensitivity.
For low gain systems such as fluxgate compasses or short range metal detectors this is true, but for high sensitivity application such as the magnetometer described here, there are good reasons to avoid this approach.
A practical solution
All of the foregoing assumes perfectly matching cores and windings and in practice is not easily achieved without individual and fiddling adjustment.
The problems associated with the need for perfect matching can be avoided by one further design change.
The long straight cores are abandoned.
They are instead bent into semicircles and joined together at their ends to form a solid circular toroid.
The two opposing excitation coils merge into a simple, single, full toroidal winding.
The pickup coil remains a single winding over the toroid (Fig. 8).
By these modifications the sought-after simplicity is achieved, coupled with the virtual elimination of all high-level signals.
As an additional benefit, the closed magnetic circuit of a toroidal core greatly reduces the drive requirements to produce saturation levels, simplifying the circuitry need by the system.
It may be felt that replacing two straight cores with a ring structure is too drastic a change to gloss over, but the mechanism is not too different in the two cases.
Figure 9 represents the way in which the lines of force are concentrated from the immediate vicinity and makes plausible the idea that an overwound coil would not see much difference in the flux changes it experiences from either system.
An additional advantage of the ring core is that it will accept more than one overwound pickup coil and they have different orientations to the external magnetic field.
For example while the coil orientation shown in Fig. 10a produces the largest output, that 10b links none of the changing flux and has a null output.
Angles between these produce an output which varies as the cosine of the angle, leading to the familiar figure-of-eight polar diagram for directional sensitivity.
This is the characteristic which is exploited in the design of a fluxgate compass.
The trace on the left hand page shows the disturbances caused to the earth's magnetic field by a solar storm occurring between June 12 and 14, 1991.
The measurements were made with the system described in this article.
The double coil structure is also of interest in the observation of the earth's magnetic field.
By making a magnetometer with two pickup coils at right angles and orienting it to give the maximum and null outputs referred to, the magnitude variations and angular variations are effectively separated.
In the maximum output direction the polar diagram is almost insensitive to small angular variations and the signal represents earth's field magnitude changes.
In the null output direction the signal changes are almost solely due to angular movement of the earth's field and over the small range involved are linearly proportional to angle.
To do a complete job would of course require two rings and three pickup coils.
If the toroid is inclined at the earth's field dip angle for the locality,(about 67° in the UK) and the pickup coil is positioned for a null output, the system should have maximum sensitivity and there is no requirement for an offset null arrangement prior to large amplification to reveal the fluctuations.
If the coil is positioned for maximum output an offset adjustment equal to the mean magnitude of the field is required, but need be no more complicated than an offset bias to the following amplifier input.
A more usual setup would probably be to fix the core in a horizontal position, aligned so that one pickup coil responds to the maximum horizontal component of the earth's field, in which case the other would respond to the variations in declination or angular change in direction.
This corresponds to two of the standard measurements made by recording stations and should provide adequate indications for radiocomms prediction.
Instrument design
A simple RC oscillator supplies trigger pulses to a frequency divider formed from two D-type bistables.
Complementary outputs from the lower frequency bistable are used to operate voltage switches which connect the full supply voltage across the excitation coil in alternate directions thus providing the square voltage waveform.
A small resistor is connected in series with the coil to limit the current during core saturation.
The frequency is chosen to make sure that the core reaches saturation at each alteration, but does not spend any more than a short time in this condition so as to maximise the final output signal; the circuit should produce as many saturation signals as possible.
The pickup coil signals connect to analogue switches controlled by the first (and higher frequency) bistable in the driving divider chain, in order to isolate only those signals which coincide with the saturation waveform transitions.
The signals from the two switches are fed to two operational amplifiers configured as simple RC low-pass filter/integrators.
A second pair of amplifiers provide additional outputs at higher sensitivity.
The first stages are fitted with input offset adjustments just large enough to cancel the largest field likely to be encountered, namely the total inclined component in the middle latitudes.
They also have a gain setting which will allow a field of this magnitude not to overload the output of the stage.
This is useful in setting the zero field output correctly.
Fields in the range of interest are usually measured in gamma, one gauss being 100,000 gamma.
The total component of the earth's magnetic field in the UK is around 47 000 gamma inclined at around 67° to the horizontal.
The particular core and winding arrangement used in the design has an intrinsic sensitivity of about 12mV/gauss or  0.12µV/gamma.
A gain of 400 in the first amplifier/filter will then produce an output of almost two and a half volts from the total earth's field vector, avoiding overload and permitting a peak-to-peak measurement by rotating the core.
Calibration
The core is aligned with one coil picking up the north-south field and inclined at roughly 67° to the horizontal and adjusted so as to produce the largest positive output from the amplifier.
It is then rotated through approximately 180 degrees to find the largest negative output.
This process is repeated, while the zero offset is adjusted, until the positive and negative readings have the same magnitude.
The amplifier will then have been adjusted so that zero field corresponds to zero output, the first step in achieving calibration.
The next is to calibrate the gain, using a pair of Helmholtz coils.
These are not the expensive-looking, beautifully crafted works of art found in old school laboratories.
With a sensitive magnetometer all that is needed is a few turns of enamelled wire reasonably carefully wound onto a piece of plastic drainpipe.
The requirement is a pair of close-wound coils, mounted on the same axis and separated by a mean distance equal to their mean radius, as shown in Fig. 12.
The result is a fairly large volume between the coils of almost uniform field strength given by: where r is the mean radius of the coils in  meters,N is the number of turns in each coil and I is the current flowing in the coil in amperes.
Using a 110mm diameter piece of plastic tubing as a former, a pair of coils wound with 24 turns/coil of 0.2mm wire give a coil constant of 3.95 gauss/ampere.
The calibration is carried out by mounting the magnetometer core centrally in the coils and adjusting the peak-to-peak output while reversing the coil current, set to a constant value appropriate to the range being calibrated.
This would range from about 125mA to match the earth's field to 0.75 mA to set the most sensitive range.
The first stage amplifier has a gain of only 400 giving a sensitivity near 20000 gamma/volt.
This could show a severe magnetic storm but a higher sensitivity is desirable for serious observation.
A second stage amplifier provides a gain of around 200, increasing the instrument sensitivity to 100 gamma/volt.
This is high enough to permit observation if the diurnal variation.
At this gain the noise level of the instrument can be seen in quiet periods to be approximately 10 gamma — 15 gamma over a bandwagon of the order of 0.3Hz, reasonable enough for serious studies.
If the instrument is only to be an indicator of magnetic activity then calibration is not really necessary.
However a small scattering of calibrated stations could provide an interesting and potentially significant source of scientific date on radio propagation.
Quantitative information, even of limited accuracy is often useful if enough people collect it.
Sensor design
The core used in the magnetic sensor is made by Telcon Metals of Crawley and consists of a toroid wound from a flat tape, one thousandth of an inch thick HCR alloy.
This metal was specially developed for use in magnetic control systems and amplifiers and has a remarkably rectangular hysteresis loop in which the remanent flux is only 2.6% less than the saturation flux.
As supplied the core is protected by being enclosed in a toroidal shaped hollow plastic case.
This makes the winding simple since the wire can be applied directly over the case.
The only toroidal wound coil is the excitation coil and it should be made up using 0.5mm diameter enamelled wire.
Start with a length of about seven metres of wire, precisely half of which should be spooled onto a shuttle thin enough to pass through the centre of the toroid.
The other half should be neatly coiled and taped for the moment.
Temporarily secure the centre of the length to the core with a piece of tape and begin winding by passing the shuttle through the core centre repeatedly.
As winding progresses, unloop from the shuttle just enough wire at each stage to make handling as reasonable as possibly, taking care to avoid kinking the wire.
Keep the turns close wound on the internal diameter and reasonably evenly spaced on the outside.
This is easily said and less easily done, but it is not critical to the final working.
Be careful however to avoid overlapping turns.
When the winding extends around approximately half of the core cut off the end, leaving a few inches of spare wire and tape the wire down temporarily.
Repeat the whole exercise with the other half of the original length of wire until the entire core is filled.
Secure the wire ends by twisting them together for a short distance to prevent the turns trying to unwind themselves.
The total number of turns need not be exact but will be around 170.
The next stage is to find a piece of plastic or card tube which will just slide snugly over the wound coil to provide the former for the pickup coils.
It is possible to make up  a tube by winding and gluing layers of brown paper round a suitable former.
Shallow slots should be cut or filed into the top and bottom of the tube as shown in Fig. 13.
These are to facilitate the winding of the pickup coils.
Next wind some tape through the slots and around the core to provide a base for the pickup windings.
The coils themselves should consist of 500 turns of 0.2mm enamelled wire scramble wound as neatly as possible over the core, the second winding crossing over the top of the first at right angles.
The whole assembly can then be given several coats of varnish to help hold the windings in place and glued down to a square mounting board, etc.
The ends of the coil should be soldered to the termination strips to allow for the later attachment of heavier gauge connecting cable.
Electronic Design
The detailed electronic circuit is shown in Fig. 14 and consists of three basic sections, a power driver for the excitation coil and two identical sense amplifiers.
The driver section starts with an RC oscillator made from two 4011 gates wired as inverters.
The component values are chosen to give a frequency of 720Hz.
The output is fed to a two stage frequency divider formed from the D-type 4013 bistables, producing a final output square wave at 180Hz in both normal and inverted phases.
These two signals are connected to two pairs of complementary emitter followers in bridge connection for core driving.
The excitation winding is connected in series with a small current-limiting resistor.
The current in the coil rises rapidly to a value limited by the series resistor when the core reaches saturation.
At this point the voltage drop across the emitter followers also increases causing a sudden small reduction in the available square wave voltage, which persists until the signal reverses.
This effect causes the pickup coil pulses associated with entry to saturation to be noise and undesirable for use in the final measurement system.
The reason for this is that as the core begins to saturate, the voltage available for saturation falls, tending to slow down the approach to saturation.
This makes the transitions less well defined and subject to jitter in rather the same way as a multivibrator will jitter if the initial approach to transistor turn-on is not rapid.
The signals generated in the pickup coils are not neat rectangular pulses although their amplitudes very with the external field.
The lack of perfect balance and coil symmetry also allows some of the fundamental switching frequency to leak through.
However, the key fact remains true that the area under the pulses is proportional to the external magnetic field, after discounting the small offset caused by the fundamental breakthrough.
The waveforms look something like Fig 15.
The pickup coil in each amplifier is loaded by a 1kΩ resistor and applied to one section of an quad analogue switch.
The output is loaded by a 10kΩ resistor to avoid the following circuitry seeing a high impedance when the switch is open, thus becoming susceptible to interference or hum pickup.
The control signal for the switch is derived from the first divider bistable (at twice the core switching frequency) by differentiating the square wave and clipping off the negative pulses with a diode.
This results in short positive pulses coincident with the core switching transitions to select the desired pickup of coil signals.
The signals from the switch go to an inverting DC coupled operational amplifier with lowpass filter characteristics through the addition of a capacitor in parallel with the feedback resistor.
The variable input resistor allows for gain calibration.
The corner frequency of the filter is about 0.3Hz.
The signal then goes to a second  amplifier/filter of the same configuration also with a corner frequency of about 0.3Hz.
This second stage has two alternative gains of x20 and x200 selectable by 4016 switch.
The non inverting input of the first stage amplifier is connected via a resistor to a potentiometer linked across positive and negative voltage divider chains to provide an adjustable input offset.
The potentiometer is a stable cermet multi-turn type to allow the high resolution setting required at the highest sensitivity.
(One tenth of a turn corresponds approximately to 500 gamma, the full range covering 100,000 gamma).
There is nothing particularly remarkable about the power supply.
Setting up
If this all appears to work, an oscilloscope should show a pickup coil signal similar to Fig. 15, variations depending on the individual winding symmetry.
Amplitude changes should occur as the core is rotated in space.
Similarly, a DC voltmeter connected to the test point at the output of the first stage should see a voltage which varies as the core is moved about.
The trimpot used to set the first stage gain, should be set at maximum resistance and the multi-turn zero offset trimpot to the centre of its range before this check.
If the first stage functions then the final second stage output is likely to be hard against the positive or negative amplifier limits, but very careful manipulation of the core orientation will find a position in which tiny movements will flick the output between limits.
Even more careful adjustment may permit the output to settle somewhere between the amplifier limits.
Try to bring the output voltage to zero either by moving the core or adjusting the zero-offset trimpot.
Shorting the gain change control line to ground will increase the second stage gain by a factor of ten and should reveal micropulsations in the shape of an output which fluctuates randomly by about 0.2V.
At this level of sensitivity metal objects in the vicinity should have a marked effect if they are moved.
A mildly magnetised screwdriver may drive the system right off-scale from a foot away and a small magnet can be detected at six to eight feet.
Even the car keys or a belt buckle may upset things if placed too close.
The second amplifier should perform in the same way, but the core will have to be rotated through about 90° to find the corresponding zero field position.
Calibration
Select one amplifier for calibration and connect a voltmeter to the test point at the first stage output.
Place the sensor on a flat surface and rotate it slowly through 360°.
The voltmeter should show a maximum (positive), a minimum (negative) and two zero crossings approximately at right angles to the maximum and minimum.
Make a note of the values at maximum and minimum and if they are not of equal magnitude adjust the zero-offset trimpot so as to make them equal.
This may take a few iterations to achieve a good balance, but should eventually make the zero crossings correspond to more or less zero field.
Next rotate the sensor to locate the maximum again and, while trying to maintain the heading, tilt it upwards to find the absolute maximum reading.
In the UK this will be at about 67° to the horizontal, but it is not critical, as the maximum is quite broad and not very sensitive to angle.
Compare this positive reading with the negative minimum found by rotating the sensor by 180° in the same 67° plane to the horizontal.
The magnitudes of these two readings should not be the same, if the zero-offset has been set correctly.
This is not as difficult to do as it sounds, simply because of the very broad maxima and minima.
If the intention is to make serious measurements calibration with a Helmholtz coil will be required.
The outlined design produces a field of 3.95 gauss/ampere.
Strictly this implies a precision greater than is reasonable and should probably be read as 4.0 gauss/ampere or 400 gamma/mA (one gauss = 100 000 gamma).
The sensitivity sought at the test point is 20 000 gamma/volt: + 2.5V corresponds to a range of 100,000 gamma so the initial Helmholtz coil current setting should cover this.
At 400 gamma/mA this calls for a current of 125mA variable.
This should include a reversing switch to enable the current through the coils to be reversed easily.
Place the sensor in the centre of the Helmholtz coils with one pickup winding aligned to pick up maximum flux.
Monitor the appropriate amplifier test point with a voltmeter or calibrated oscilloscope and rotate the Helmholtz coils and sensor to give zero volts.
Switch on the coil current (set to 125mA) and the test point voltage should change by about two volts or so, positively or negatively.
Using the amplifier gain trimpot, adjust so that the output changes by 5V when the coil current is reversed.
(+2.5V range).
Since the final stage gains are set by 1% tolerance fixed resistors, this completes the calibration, the output sensitivities being either 1000 gamma/volt or 100 gamma/volt.
Repeat the exercise to calibrate the second pickup winding and the instrument is ready for use.
The core used in this design is a Telcon Metals HCR alloy core type 7a and is available from Telcon Metals, phone 0293 528800
The flux gate transducer outlined in this article can be used at the heart of a scientific magnetometer of highly accurate electronic compass
Fig. 7.
The filter output is derived from the sum of the core contributions to the pickup coil output
Fig. 8: the toroidal core winding system.
The finished transducer includes a second pickup coil at right angles to the first.
Fig. 9: a toroidal core tends to concentrate the magnetic field tangentially producing the same sensitivity pattern as a pair of bar cores.
Fig. 10: The toroid exhibits greatest sensitivity at right angles to the pickup coil
Fig. 11: The electronics divides into two parts: the exciter section drives the core into saturation with a current ramp of alternating polarity while the measurement section integrates the pickup coil pulses caused by the action of a static magnetic field on the core.
Fig. 14: complete circuit diagram in three parts.
Note the bridge driver for the exciter coil allied to the synchronous rectifier downstream of the pickup coil
Second measurement channel mirroring the bottom half of the upper diagram
Power supply circuit.
There is nothing special here.
Fig. 12 winding details for the Helmholtz calibration coils
Fig. 15: the sort of waveform expected across the picup coils.
The amplitude and polarity of the pulses changes with strength and direction of the static magnetic field.
Magnetometery and radio communications
The study of propagation is one of the important aspects of radio comms activity.
The ability to forecast ionospheric conditions is related to solar activity.
A magnetometer, detecting changes in the magnetic fields arising from charged particles deflected around the earth, measures the effects of corpuscular and non-corpuscular radiation emanated from the sun during solar flares and other events.
Non-corpuscular radiation is in the form of high energy X rays which, assuming that the earth is in the path of the rays, will reach the earth in fifteen minutes.
This radiation may increase the depth of the ‘D’ layer due to ionisation and produce the all too well known Dellinger face out when long-distance HF communication ceases abruptly.
Corpuscular or particulate radiation appears in the form of protons and neutrons which take longer to reach the earth's upper atmosphere than the higher energy radiations.
These nuclear particles arrive at the F1 and F2 layers approximately 48 hours following a solar event and produce ionisation by colliding with gaseous molecules and cosmic particles.
This explains why so few protons are detectible at the earth's surface except after very major events.
The increased ionisation along the earth's lines of magnetic force make them more conductive and leads to an increased current flow with consequent rise in magnetic flux levels.
It is under these conditions that the magnetometer is useful in detecting the fluctuations in the magnetic fields.
In terms of radio propagation, varying electrical conductance in the ionosphere influences either the absorbency or reluctance of radio emissions.
A knowledge of the ionospheric status will often suggest which bands will be more productive.
It is interesting to note that a Dellinger fade out is often followed by a magnetic field change around 48 hours later.
Many articles have appeared in the amateur radio press and elsewhere describing the many aspects of propagation and giving more detailed information on the theory of and effects of solar radiation on communications systems.
This brief and simplified introduction has been included for completeness, and hopefully to show that the examination of the earth's magnetosphere in relation to propagation is not a difficult project to undertake, the equipment is easy to build and has the advantage of being a both interesting and useful addition to your capability.
David Lomax GW0FXA 
Regulars
Letters
Cuk vs.
Buck
According to Terrence Finnegan, the Cuk power converter offers the best of all power supply worlds, approaches 90% efficiency, requires few components, and produces minimal amounts of interference (EW + WW July 1991).
The rather woolly nature of these claims aside, readers might be forgiven for wondering why, if the Cuk converter offers so many desirable features, it is not more commonly used.
The fact is that in the world of switch-mode power supplies (and it is a very large world indeed, covering PCB, mainframe computers, electronic telephone exchanges, military equipment, aerospace, and more besides) it is rare to come across a Cuk converter.
Interestingly, even Dr. Cuk himself, after starting up his Teslaco power supply consultancy, did not use the Cuk converter for his early power supply contracts.
The lack of popularity with the Cuk topology may be due to its problems relating to the energy-coupling capacitor.
In the non-isolated version of the topology, as used in Terrence Finnegan's design, the capacitor is, as he says, smaller than the equivalent magnetic component used for energy transfer.
However, this seems to be a red herring, since the standard Buck topology uses a pair of inductors (though not coupled) and a single transistor switch just as the Cuk topology does, but need no other energy coupling device.
Most power converters are required to provide transformer isolation between input and output, and the isolated version of the Cuk converter has to have energy coupling capacitors on all its windings, so that they then have to be provided as energy coupling devices plus the transformer.
Regardless of whether we are talking isolated or non-isolated, however, the capacitors introduce two entirely non-trivial problems.
First, at currents of more than a few amps, capacitors that can carry the current without suffering di/dt stress, heating stress, and MHz ringing tend to be physically large, and expensive compared with normal power supply capacitors.
Furthermore, the required low series resistance results in high Q resonances with the inductors at audio frequencies.
These will then have to be damped by paralleling each capacitor causing this problem with another of higher value and higher series resistance.
In addition, during start-up and overload conditions the capacitors (and their larger parallel damping capacitors, if fitted) find themselves with the wrong DC bias, and charging occurs through potentially very low impedances.
Care has to be taken to make sure no excessive currents can flow during these transients.
Furthermore, these DC offsets across the capacitors appear on the transformer windings, causing shifts in the core operating flux.
Therefore, extra headroom has to be provided for the operating flux.
As a result another apparent advantage often cited for the Cuk converter (that it can use the full peak-to-peak bidirectional flux swing) is, in practice, whittled away.
As for the claim that the Cuk converter approaches 90% efficiency and produces minimal RF interference, exactly the same claims can be made for well-designed standard topologies.
Good efficiency is mainly a matter of using good switches and rectifiers, since they are where most of the losses occur.
The low RF interference and low component count claims are usually made because the Cuk topology is not only able to accommodate the input and output filter chokes on the same magnetic core as the transformer, but it can also reduce the input and output ripple currents to very low levels.
This does indeed seem like getting something for nothing, but is it really?
Since the input and output inductors store energy, then just like any other inductor, they produce flux in their common core and they require space for large enough diameter wire to keep I2R losses reasonable.
Extra core volume and winding window area must be provided to support these.
What no one, as far as I know, has ever done is to show whether there is any space, weight, efficiency or cost saving that results from assembling all the windings onto one core.
Standard bobbins cannot be used for E cores, since all three legs have to be used for the isolated topology.
Of course, if the standard easy-to-use E-core bobbin is dispensed with, there is automatically more room for the windings, so increased copper area is traded for increased difficulty of manufacture.
This trade-off may be acceptable in the military and aerospace fields, but not in the commercial field.
The large reduction in ripple currents is an advantage, but input and output decoupling capacitors are still required.
Those on the input will be similar to what the standard Buck topology requires; those on the output(s) could be made smaller, though the reduction is limited by the need to keep enough capacitance  to absorb fast load transients.
To summarise, the Cuk converter has an apparent advantage over standard Buck topology in that it uses an integrated magnetic structure and can reduce input and output ripple currents to very low levels.
On the other hand, the required energy coupling capacitors cause problems and can be expensive at higher current levels, and there does not seem to be any proof that there is a volume, weight, or cost saving over the standard Buck topology for any given equivalent performance level.
All credit is due to Terrence Finnegan for his two recent articles which, for the first time, have made available useful and usable designinformation.
Nevertheless the Cuk converter topology seems likely to remain an outsider until a true trade-off comparison can be made between Cuk and Buck at a series of realistic equivalent performance levels, a task that would take a lot of time, but whose results would be of great interest to many in the power supply design field.
Brian J. Pollard Watford Herts 
To be Class D or not
Why has Class D not been more widely used?
It is very efficient from a power dissipation point of view, not suffering from cross-over distortion.
In addition, even if overall negative feedback is used, it has low overall distortion.
On the surface it would appear superior to Classes A and B.
It was invented by my late friend, Dr A H Reeves, who also invented pulse code modulation and the capacitor microphone, both of which have enjoyed enormous success.
In my view and also, if my memory serves me correctly, in Dr Reeves' view, this is quite simply because it doubles up as a transmitter with very sharp rise and fall times on its pulse width modulated waveform.
This means it needs effective screening and filtering which can be difficult, bulky and expensive.
I fully accept that in Andy Gothard's article (EW + WW July 1991) about King's College, there is a circuit system developed by Dr Mark Sandler which produces a very accurate, and indeed ideal, analogue output from the digital (CD) to analogue (PWM power amplifier drive) converter.
But Dr Sandler's circuit has at least three problems.
First, there are unwanted transmissions over the RF spectrum as mentioned earlier.
Secondly, there is distortion introduced in the power amplifier following conversion since unavoidable rise and fall times of the output power stage introduce non-linearity if they are not equal which, in practice and for a number of reasons, they will not be.
Thirdly, overall feedback is not possible in this arrangement, unlike Class D.
Because of the practical difficulties in overcoming the first of these problems, I cannot see, even if the price of the D-to-A converter were a few pence, Class D in its Reeves or Sandler guise making much progress.
This is a shame, since switching in some ways is much better than linear amplification.
A Sandman London 
Watching for the HDTV con trick
The impressions gained by the Waddington experiments suggest there were more habitual television viewers (4:3) than avid cinema goers (16:9).
Nearly two generations have passed since Hollywood introduced the larger format (incidentally not 16:9) in an attempt to halt the then falling box office revenues.
Since then home viewing has largely taken over from cinema going but there does not seem to have been much criticism of the format as such but perhaps more so the screen size.
Today the increased reliability of television receivers causes commercial concern for set manufacturers and hence their hopeful exploitation of so-called HDTV and change of format.
So-called?
Yes.
The human eye in its best years has a maximum angular resolution of one minute of arc.
In other words it can just distinguish stationary 1mm black vertical lines 1mm apart on a white background at a distance of 3m.
A little mathematics shows that with a monochrome 4:3 70cm CRT one could just resolve a 625 line black and white stationary picture consisting of 4MHz dots using today's European standards at the normal viewing distance.
Any increase in these standards therefore could not be resolved by any earthbound human being.
The introduction of colour and moving scenes involves other aspects.
With the colour mask tube the horizontal definition is theoretically reduced by a factor of three but the eye is quite tolerant to small changes of detail in moving scenes in the opposite direction.
The use of correctly converged colour television projectors with pixels superimposed rather than being shown side by side allows present television standards to be used satisfactorily, and economically, unless and until genetic engineering succeeds in altering human optics.
Incidentally, I have used projection television receivers at home for the last 40 years and, possibly their use does not require the eye to be concentrated in one continuous subtended angle, my in eyes in their fourth quarter century still resolve that one minute of arc!
Based on the above, which HDTV hype cannot alter, I suggest that commercial technical research should concentrate on larger flat screens with superimposed pixels with change of standards but with some attention to the basic defects in the NTSC, Pal and Secam systems which are only mildly irritating.
In any technical comment you make, please remember that a former editor made a passionate technical plea for the retention of 405 lines but failed to realise the commercial advantages which had then already accrued in the 625 line camp.
These do not yet apply in HDTV.
M Le M Manson Squadron Leader RAF (retired) Rome, Italy 
Baffling stabilisation
Comparison with the original ring-of-two current stabiliser of Peter Williams (WW September 1966) reveals that V Lakshminarayanan has over simplified the circuit to such an extent that it no longer works as a current stabiliser (EW + WW July 1991).
The fault cannot be attributed to a draughting error because the 2N3904 and BC546 are npn transistors and the circuit will pass only 5mA when 2.7V is between A and B.
As for the zener diode D, that is described as being in series with the base of Tr3 which has me completely baffled.
John C Rudge Harlington Middlesex 
Aerial confusion
J.J. Gameson seems a little confused in his letter in the July issue.
Everything appears OK until he introduces the ferrite rod antenna (aerial).
The null in the directional response of such an antenna is quite independent of the Q of the coil.
It is linked to the magnetic field of the transmitted wave — not the turns of the coil.
Null depth varies in practice, because some antennas respond to the electric field component of the wave as well as to the magnetic field component.
When we come to whip and dipole antennas, Gameson criticises the whip for not being directional and the dipole for being directional!
Incidentally, a horizontal dipole without parasitic elements is not unidirectional it is bi-directional, having exactly the same ‘figure-of-eight’ response as a ferrite rod antenna!
Having noted Gameson's earlier remarks about a ‘deep null’, it is surprising to read that ‘The ferrite rod is electromagnetically non-directional’, and then to return to the ‘very sharp null’ idea in the next paragraph.
Furthermore, VHF ferrite rod antennas are not less affected by standing waves than whips.
Unfortunately Gameson's remark about listeners' confusion is true.
Broadcasters do try to offer advice, but it often goes right over the heads of enquirers.
However, there are two solutions for the quality enthusiast.
In a fringe area, or where distant stations are wanted so that a directional antenna is essential, a rotator can be purchased for quite a modest by today's standards.
Maplin Electronics have one at just under £41.
For those who only need relatively local stations, advantage can be taken of the fact that nearly all FM stations now radiate, either slant-polarised or circularly polarised signals, so that a vertical dipole, completely non-directional in the horizontal plane, gives as much signal as a horizontal one.
This does not of course, apply to Yagi antennas, which are more or less unidirectional.
The BBC Research Department have built FM receivers with ferrite-rod antennas, and published reports on them, but the fact that such receivers are not commercially available, strongly suggest that any technical advantages (if any) do not justify the extra cost.
The wideband cable systems which were trumpeted some years ago could well carry high quality sound as well as umpteen television channels but their development has run into serious economic problems.
There is a new sound broadcasting system under study, for introduction early in the next century.
This is Digital Audio Broadcasting (DAB), which without mathematics, it would be impossible to explain its mechanism in words.
But you will still need an antenna!
J M Woodgate Rayleigh Essex 
Telepointless
I am bemused by the media attention being focused on the fate of the telepoint services.
The public is being asked to believe that an ill-conceived government has it in for these companies.
This argument just does not hold water.
The whole system was ill-conceived from the outset.
That some very large companies should have lost a great deal of money on their little experiment shows only how unwise their decision to pursue the technology was.
Look at the arguments.
Since calls must be made close to a node, then why not simply find a telephone kiosk — must work these days and the cost will only be at the standard rate.
Then there is the absurd lack of ability to receive calls.
This alone was surely enough to doom the venture from the outset.
Then there is the cost: cellphone equipment is virtually given away with crisp packets and the running cost is only fractionally greater.
Did you ever hear the cellphone operators complaining that the early competition adversely affected their business?
Did you heck!
Given that this was all well-known at the outset what utter fool would believe there could be any money to be made out of such a system.
Well we learned all too well, didn't we?
If any other pioneering company had brought out a call-only cordless telephone, I wonder who would shed tears when it failed to sell.
Peter Johnsonn Electronic Design Laboratory Bristol 
We told them so — Ed.
Any old valves
It was good to see a reference in your magazine to valve amplification (EW + WW August 1991), but I still have a certain frustration that you may be able to help me with.
I have for many years enjoyed dabbling with valve amplifiers, usually modifying old ones, but all the information I have has to be gleaned from circuit diagrams or an old Mullard handbook containing a selection of designs (I'm sure you know the one!).
Despite many years of searching, I have not yet been able to track down any other source of reference on the design and construction of valve pre and power amplifiers.
Can anyone help?
There must have been text books around at one time but I cannot even identify any titles let alone suppliers.
DA Ellis Shipston-on-Stour Warwickshire 
Light hearted
With regard to the debate about c and Doppler shifts in the letters pages, light (velocity c ) reflected or emitted from a surface moving with velocity v towards an observer appears to be blue shifted, or red shifted in the case of a receding surface.
As a non-physicist I have often wondered if the difference in photon energy (E' — E = hf' — hf ) of the shifted and unshifted light could be wholly accounted for by the effect of the surface velocity (v ) acting together with the particle equivalent mass of the photon.
The extra energy not being allowed to be expressed as an increase in c appears as a commensurate increase in f, that is:(formula included) where m is the photon equivalent mass (yes I know it is related to E but I am trying not to confuse myself),E is the energy, and h is Mr Plank's claim to fame.
I write this letter in the hope that someone will explain the errors of my ways, forgive me for mixing relativistic and Newtonian physics, and enlighten me.
Steve Bennett IDT Europe 
Bad vibes
I detect all kinds of vibrations and illnesses in people, animals and plants simply with an old motor car speedometer cable which starts swinging in my hand.
The combination of electric fields (EW + WW February 1990) and earth energy lines is deadly.
When the patient is removed there is a dramatic change for the better!
I have been told the Chinese have a simple inexpensive instrument to trace extra low frequencies.
Could you help me find out if such an instrument is available in the UK?
George Heye Somerset West South Africa 
For those not familiar with the effects of earth energy lines, Mr Heye sent further documentation with his letter that listed some of the tell tale signs that may show someone is suffering from bad earth vibes.
These include: home making expansion and contraction noises at night (stronger during full moon); cracks in walls and pools; ants and reptiles laying eggs; car battery often flat; suffering pot plants; and twisted trees.
He added that people usually pale below the eyes and above the cheek bones, have many dreams including nightmares and hallucinations, suffer disturbed sleep especially at full moon, get up tired and listless, are prone to attacks of flu, and are driven to drink, drugs and heavy smoking.
He also said that most Westerners laugh at these ideas.
— Ed.
Computing
Graphical formats and file transfer
There is a proliferation of complex graphical file formats.
David Bacon explains how these files are structured, and how graphical information is transferred.
If a picture is worth a thousand words, it might take a thousand words to describe a picture.
This is the first thing you notice about graphics interchange files — they tend to be long.
The second thing is that there are an awful lot of them.
Do we really need.
CGM,.
CUT,.
DXF,.
EPS,.
GIF,.
IMG,.
PCX,.
PIC,.
TIF, and many other types of file, all to do the same job?
Standardisation has never been the IT industry's strong point, and the answer is ‘probably not’.
However, they don't actually all do the same job.
The application of computer graphics is wide — from management pie-charts to mapping Mars.
Broadly speaking, however, there are about four general reasons for files which transfer graphics information:
—
To save or transfer drawings made on cad systems;
—
To save or transfer images from computer art systems, scanners, or acquired from a camera;
—
To control advanced printers;
—
To communicate between networked graphical user interfaces (GUIs) and host computers.
Inside such files there are two fundamentally different methods of describing graphical information.
Some graphics files use one method or the other exclusively, others can mix the two methods in the same file:
—
A high-level approach, sometimes referred to as a drawing description or vector file, or ‘meta file’;
—
A low-level approach based upon the pixel information used to display a digital image, sometimes referred to as bit mapped, raster based, or image file.
Drawing description files
The main application of these files is in cad.
Computer aided design (cad) systems are the draughtsman's equivalent of a word processor.
For a skilled operator it is faster and easier to produce a new drawing, and once a drawing is on file, making changes and producing updated versions becomes much more efficient.
In a large team effort, different people can be working on separate aspects of a design, with changes rapidly distributed to all of them.
Cad transfer files normally describe drawings in a way which is referred to ‘device neutral’.
To give an example, a statement in a cad file might consist of ‘CIRCLE 10.5 24.75 3.9’.
It doesn't take too much imagination to guess that this means ‘draw a circle centred at x = 10.5, y = 24.75, and of radius 3.9, in whatever units we happen to be using at the moment’.
This is describing the circle, not how the circle is drawn.
In fact, it is describing the circle in the most accurate way possible.
Any attempt to actually draw the circle is bound to have some kind of imperfection, such as the irregularities if a bit mapped device, or servo non-linearities in an X-Y plotter.
But the cad transfer file is concerned with the object itself, not how it is portrayed.
(The Greek philosophers were aware of this point, that perfection only exists in virtuality.
Any attempt to produce something in reality leads to imperfection.
For example, we can all sing perfectly in tune in our heads.)
However, although ‘device neutral’ in the above sense, a cad file must support the features of the cad system which produced it.
Since such features, which can be very powerful, differ for various cad systems, output files tend to be ‘system specific’.
Transferring work between systems which do not support a common interchange file format requires translation software, and raises the problem of how to handle features of one system not supported by the other, or handled differently.
Drawing description files are normally ascii text files, with the drawing attributes and all relevant set-up information described in a suitably coded but printable form.
One reason for this, rather than the use of binary data, is that it allows the file to be sent via communications channels coded for 7-bit plus parity.
Another advantage is that the file is printable, and can be checked by eye if necessary.
The coding normally consists of keyboards and parameters.
For instance, we could have ‘CIRCLE X Y R’, or ‘LINE X1 Y1 X2 Y2’.
POLYLINES is another useful device, consisting of an indefinite sequence of X/Y pairs to be interconnected by straight lines.
Polylines can efficiently describe shapes made from visibly straight lines, such as rectangles, or by using points sufficiently close together, complex curves such as spirals.
In similar ways, arcs, ellipses and regular polygons can be described.
A more advanced drawing ‘primitive’, as these basic shapes are called, is the spline.
This was originally a flexiblestrip used by railwaymen to get smooth curves when laying railway lines.
In a cad system it is an electronic ‘French curves’.
The basic idea is that a few parameters are turned into a smooth curve by a standardised computation.
One particularly useful type, shown in Fig. 1, is defined by the position of each end (x and y), plus the direction of a tangent to the spline at each end (a).
Fig. 1 illustrates that by linking such splines at a common point and opposite tangent directions, an indefinite length of smooth curve can be defined with relatively little data.
Providing the underlying maths is the same for all systems using the file, the same shape will be reproduced each time.
Cad systems which work in 3 dimensions will produce ‘drawing’ description files which actually describe the object being drawn, using 3D position co-ordinates.
To get a view of the object a viewpoint and direction must be stated, and the system then computes the required 2D projection.
This is ideal for visualisation applications, such as planning the use of 3D space, impact of new buildings, style of a car body, etc.
System parameters also need to be described in drawing description files, such as:—
—
Definition of fonts for textual information;
—
Line styles, widths, varieties of dashed lines, etc;
—
Styles of area fill, hatching patterns, etc;
—
Layers.
Layers are an important part of most cad systems.
They allow a drawing to consist of several layers, equivalent to overlaid transparent sheets, each containing a different aspect of the drawing.
For instance, an architect may want an office floor plan with walls, furniture, plumbing, electrical wiring, and air conditioning all shown on different layers.
The plan can then be produced in different versions showing different aspects of the overall design.
All such detail goes into the drawing description of file.
The result can be a very long document in a complicated coding scheme, but it forms a powerful description of design information without any compromise made in accuracy.
Examples of drawing description file formats are:—
—
Initial Graphics Exchange Specification (Iges): this appeared in 1980.
It uses ascii character coding, and tends to produce very long files.
Perhaps in view of this, a compressed format is available.
—
Computer Graphics Metafile (CGM): this became an ansi standard in 1986.
There is a choice of file coding in ascii or binary form to suit different applications.
—
Drawing Exchange Format (DXF): this originates from AutoCad.
It is in widespread use, and is supported by other cad systems.
A glance at a short section of a DXF file, see Fig. 2, might lead to the conclusion that whoever designed the format either had shares in a paper manufacturing company, or a grudge against rain forests.
A printed DXF file is quickly recognisable by consisting mainly of a narrow strip of short lines down the left hand margin of each page, usually a large number of pages, and a great deal of white paper.
It needs to be remembered that the format is not primarily designed for printing on paper; this is just a handy feature.
Part of the power of cad is how the powerful graphics capabilities can give a friendly solidness to engineering drawings.
Unfortunately, much less friendly is the relation between systems when it comes to file transfer.
AutoCAD picture by Nick Manning, AutoDesk 
Fig. 1.
Smooth join between two splines
Fig. 2.
Binary tree to generate variable length codes
Fig. 3.
Fragment of DXF file with comments
Another feature of DXF files is that alternate lines consist of a simple integer.
This is a key point of the format.
Each item of information is wrapped up in two lines of the file.
The integer declares what kind of information is on the next line, and the following lines gives a name or a numerical value.
Fig. 3 shows the same section of DXF file with explanatory comments.
It is a long way short of defining the format, but gives the flavour.
The very rigid structure looks tedious and clumsy to us humans, but we are not meant to be reading it.
Processors like it.
Bit image files
As powerful as drawing description files can be, they would get a bit unworkable if faced with, say, a colour photograph of a budgerigar.
This is exactly the kind of picture which we might want to save from a screen paintbrush job; lovely blends of colour, subtle shades and shapes, and not a straight line anywhere.
Images captured from TV, sent back from space, or more prosaically produced by a document scanner, are likely to fall into the same category.
For this type of picture the bit image file comes into its own.
The picture is written to file in terms of pixels, individual spots of light or ink which, in suitably large numbers, create the picture.
In some cases a bit image is little more than a memory dump of video ram.
Video ram like most memory, is usually organised in 8-bit wide words, or bytes, but interpreted in a bit oriented manner.
Fig. 4 shows the first few bytes of video ram, with addresses running from the base address upwards.
For a monochrome monitor, where each pixel is either ON or OFF, each bit will control one pixel.
Typically, bits are taken in most significant to least significant order in each byte, and in ascending address order.
So starting with b7 of the base address byte and reading left to right across the diagram, the sequence of bits controls the pixels of the top row of the screen, also from left to right.
At the end of the top row the sequence of bits starts at the left hand end of the second screen row, and so on.
It is obviously convenient for line boundaries to coincide with byte boundaries, and the horizontal resolution of VDU screen is invariably a multiple of eight.
For a colour monitor, more than one bit is needed to describe each pixel, and there are two approaches to this.
In the ‘packed-pixel’ method, the sequence of memory bits is divided into blocks according to the number of bits required per pixel.
Thus for 16 colour screen, where each pixel requires four bits, each byte would describe two pixels.
Typically, bits b7 to b4 of the first byte would control the top left hand pixel, and so on.
This is illustrated in Fig. 5, where the first few pixels are more widely spaced simply to fit everything into the diagram.
In the ‘bit plane’ method, Fig. 4 is, in effect, duplicated for as many bits as are needed for each pixel.
For a 16 colour monitor, there would be four such sequences of bytes in different blocks of memory, each starting from a different base address.
Each memory block is called a ‘bit plane’, and supplies one of the bits needed for each pixel.
This is illustrated in Fig. 6, showing typical mapping from the start of the four bit plane base addresses and the first line of pixels on the screen.
The main body of a bit image file consists of data bytes with bit mapping similar to figs 3 to 5.
Bit image files normally have a fixed format header giving general information about the file and its format.
One way to handle different bit mapping schemes is to include in the header details such as the number of bit planes, bits per pixel, pixels per scan line, and the number of scan lines.
This low level pixel oriented approach obviously makes sense for complex images with no formal structure, such as budgerigars.
It does, however, have its drawbacks.
One is length.
a 640 by 480 pixel 16 colour screen has 307,200 pixels, each described by four bits, thus requiring 153.6 Kbyte of storage.
So bit image files frequently use data compression.
Since data compression relies on redundancy in the information to be compressed, performance is affected by the nature of the data.
There are various methods which can be used for  computer files (see box ‘Getting quarts into pint pots’).
Run Length Limitation (RLL) is probably the most common for bit image files, although the LZW algorithm is also in use.
In the future, compression using techniques from digital TV, or based on fractal theory, could become important.
Another problem is compatibility.
While a cad transfer file can describe objects with the accuracy of floating point numbers, a bit image file is restricted to a digitising process.
Far from being device neutral, the image file is usually coded in a way which reflects the device which first produced it.
How should it handle transfers between different resolutions?
At first sight the obvious answer might be to convert all images to some enormously high resolution which would satisfy any possible requirement.
Mature reflection — say two seconds — makes this less attractive.
However high you choose this super resolution, the day will come when it's not enough.
Another point is that it would make most bit image files far longer than necessary.
The best scheme, by far, is to do what is normally done, ie., write a bit image file in terms of the resolution available at the time it is created.
If you want to display it on a device with different resolution, you have the following options:—
—
If the new resolution is greater than the original, you can show your entire image at its original resolution in part of your new display device, ie., in a window of it.
If you want to, you can expand it to fill the complete display by using a suitable expansion algorithm.
—
If the new resolution is less than the original, you can show part of the original at full resolution, and pan it around if appropriate, or compress it to make it all visible.
In any case, keep the original bit image file.
It represents all of the available picture information, and no surplus information.
Remember that any compressed or expanded version of a bit image will contain distortions of the original, however, intelligent the algorithms used.
Examples of bit image file formats are:
—
Paintbrush/Frieze PCX format: Paintbrush is a screen drawing programme from Zsoft, and Frieze is a terminate-stay-resident utility which can be used by other applications to capture a screen to a.
PCX file.
I can find Frieze invaluable for capturing graphical output from number crunching software.
The PCX format starts with a 128 byte header giving various general details, including bits/pixel, the number of bit planes, and the size of the image in pixels.
This is followed by the binary pixel data, compressed with bit oriented Run Length Limitation coding (see box).
—
Bit Image (IMG) format: this is used by the GEM system.
The header information includes the physical size of each pixel, in microns, thus linking the image firmly to its original representation.
The binary pixel information is compressed by RLL coding which can define repetitions of a single bit value, or of a repeated bit pattern.
—
Screen files, or SCR, are used by a number of screen painting programmes, including VGA Paint.
This format handles the problem of different screen sizes and resolutions by using a filename extension of.
SCX, where ‘X’ indicates the format.
Thus, for example, a 16 colour 640 by 480 screen would generate an.
SCV file.
—
Tag Image File Format (TIFF)(file extension.
TIF.)
This was developed by the Aldus Corporation and Microsoft particularly for images obtaining from scanning devices, and is widely used in desk top publishing.
It is supported by many scanner manufacturers, and can in some cases be produced from fax.
It is also supported by a number of screen based graphics applications.
The format has a tree like structure which provides more flexibility than most bit image files, and a powerful range of alternative compression methods.
Other bit image formats are GIF, which uses LZW compression and is widely used for clip art; PIC, which is used by the Halo screen art programme; and CUT, which uses the enhanced small disk drive RLL compression algorithm.
Colour and dithering
Like video ram most graphics files support colour by, in effect, indexing into a look up table.
If a drawing description file refers to ‘COLOUR 1’, the actual colour will be whatever colour 1 is set to.
Similarly, if a 16 colour bit image file format refers to red, green, blue and intensity planes, this will only be true of the default palette settings.
However, it is quite common for graphics files to allow a colour table to be included, thus defining the palette.
A 16 colour PCX file header, for example, includes a 48 byte colour table.
Each colour is defined by 3 bytes, one for each of the primary additive colours, red, green and blue.
Most printers are monochrome, but the often have a higher resolution than bit image date.
Thus they can convert colour to a grey scale by a process called dithering.
Although each printer pixel can normally be only black or white, a grey scale can be produced by varying the proportion of black to white dots.
Some screen graphics utilities allow the user to improve an image by ‘hand dithering’.
Page description formats
In the early days of computers, printers were basically electrically controlled typewriters.
All you could send to them were the printable characters, plus a few simple commands like ‘carriage return’ and ‘line feed’.
The ascii code allows for these by setting aside the first 32 values, from 00h to 1Fh, as ‘control characters’.
As printers became more powerful, it became necessary to send more control information to them, such as to set margin widths or select fonts.
The original 32 control codes became inadequate, and ‘escape sequences’ came into use.
The ascii ‘Escape’ character, 27 or 1Bh, allowed the control process to escape from the limitations of 32 control codes.
The convention was adopted that 1Bh introduced a specific sequence of (normally printable) characters which should be interpreted as control information.
However, the coding which follows an ESC character tends to be specific to each application.
This is a major issue, since not only must the control information be interpreted correctly, but the printer must know when the control characters have finished and it should start interpreting the following characters as text again.
One question which arises is why, with 256 code values available in each byte, has this control process limited itself to the 128 7-bit codes?
Probably the main reason is the number of printers controlled by RS232 communications using 7 bits plus parity.
There is a side benefit, however.
Printer control information in 7-bit form remains printable.
For de bugging special printer  control software, it is handy to be able to read escape sequences.
For this purpose some printers can be put into a special mode in which they print the escape sequences rather than interpret them.
Even so, escape sequences are only barely legible.
At the start of a document on my printer, a typical command sequence might be: ESC/ESC[? 1; 4; 5; 81ESC[11hESC [? 2hESC[7IESC [? 0KESC[ etc
Again it needs to be remembered that this sort of coding is designed to be read by a machine, not a human.
It is all perfectly logical and unambiguous, but not designed for people.
Examples of page description formats are Hewlett-Packard's printer command language (PLC) used for the LaserJet series of printers and the many which emulate it, and the Canon printing system language (CaPSL).
Control formats based on escape sequences can be of indefinite length, and page description languages can be extremely powerful.
They need to be.
The modern printer is normally a bit-mapped device with a wide range of facilities.
Before printing a page, the printer's processor assembles a complete bit mapped image into its memory.
Like other graphics, fonts can be described in a high level manner, or as bit mapped images.
Those stored in high level form are called ‘outline’ or ‘stroked’fonts, and scale much more gracefully than bit image fonts, particularly to large sizes.
A pixel addressable printer can obviously reproduce a bit mapped image, and page description formats have developed to allow this.
A special escape sequence defines a rectangular area of the page and then introduces a block of pixel data, very similar in principle to a bit image file.
The pixel data may be in 8-bit binary words, but it is common for them to be 7-bit binary words, to preserve the capability of 7-bit transmission or coded as ascii hex characters, ie., 0–9 and a-f, thus also preserving printability.
Such a bit image blocks are not normally compressed, and anyone using graphics in a document is familiar with the time it takes to load the printer.
A similar process can be used to download soft fonts, which means that a bit mapped printer can be programmed to handle any font, including fonts designed by the user.
The flexibility of such printers makes desk top publishing possible, and can also internationalise computer printout with non Latin fonts, such as Japanese characters.
Page description languages
A more advanced approach to page description is to design a software language which can be used to control the processor of a display device.
It may be difficult to define a formal distinction between this and the page description formats described above, but a glance at the results makes the difference obvious.
Compare the set of escape sequences in the previous section with Fig. 7, which shows a fragment of the Postscript language.
The difference is clear.
Postscript is recognisable as something which can be written and read by us humans.
It is not the easiest language to learn, but it has structure and legibility.
Postscript originated from Adobe Systems, and has become something of a de facto standard for desktop publishing.
It is widely supported and cloned, which proves its success.
As a language it has some particular features:
—
It is an interpreted language.
In each application a source file is interpreted in real time.
Thus a printer which supports it must contain a Postscript interpreter.
The advantage of this approach is compatibility.
There are no hardware dependent ‘executable’ files floating around.
—
It is a stack based language.
This is the main feature which saves Postscript from the slow execution normally associated with interpreted languages.
In some situations Postscript can be faster than the escape sequence type of printer control file.
—
It uses post fix notation, where arguments come first and operators follow.
This is basically the same as Reverse Polish Notation as used on certain calculators, and follows directly from the stack based approach.
—
It can include blocks of bit image graphical data, coded in hex characters.
—
It is a large language, with over 300 operators.
—
Although it has loops, procedures and conditional operators, in terms of modern software theory it is not strongly structured.
Although in theory Postscript could be viewed as a general purpose programming language, it is strongly biassed towards visual representation.
However, it should not be viewed simply as a means to control powerful printers.
It has an important application in screen graphics, as described below.
Graphical User Interfaces
A final major application of graphical interchange is in the area of networked computer systems.
Early computers were boxes with ports for terminals.
The terminals worked on a line by line basis, with the screen information scrolling upwards just as it would on a teleprinter roll.
When personal computers became popular (ie., cheap) the fact that the processor was close to the screen was rapidly exploited by application software to give the user a much more dynamic look and feel.
Data could pop up in boxes around the screen, and in due course graphics, mice and icons led us into the wimps era (window, icon, menu, pointer).
Initially this left the larger machines, with consoles scattered around on the end of RS232 cables, somewhat behind.
The line by line console had a distinctly old fashioned look and feel.
If PCs were networked, it was normally to share expensive resources, such as printers or mass storage.
The application still ran close to the user to give a crisp look and feel.
When wider bandwidth networking became available, the ability to move graphical information quickly around a network brought the cental multi user processor concept into the wimps era, and this has required clearly defined standards for the Graphical User Interface (GUI).
One such standard is the X Window system, developed by an industry collaboration group and the Massachusetts Institute of Technology.
It is largely based on the bit image approach, and assumes a resolution of around 100 pixels per inch, which could become a limitation in the future.
However, X is very widely supported, and is associated with the Unix operating system and the move to open standards in the IT industry.
A competitor to X is Sun Microsystem's Network extensible Window System (NeWS) which in some ways is technicalsuperior.
It uses Postscript as the transfer format, thus providing hardware independence.
It also makes more powerful features available at the user interface, such as image transformations.
Another very neat feature is that one Postscript interpreter can handle both the user's screen and a printer directly from the same source material.
Commercial battles are not always won on technical merit, and at present X Windows is probably winning.
Whatever the outcome, the movement and control of graphical information is here to stay.
Data compression methods
Data representation.
Economy in bits can be achieved by using more efficient ways to represent data.
One example is to pack the seven bits of the ascii code contiguously into 8-bit bytes, thus getting eight characters into seven bytes.
Another method is to convert ASCII numerical values into binary words.
Such methods are built into several drawing description file formats.
Run Length limitation (RLL).
This exploits repetition.
Instead of‘AAAA’, send ‘4A’.
Lempel Ziv Welch algorithm (LZW): this uses a larger coding space than necessary for the symbols to be transmitted, and allocates the extra codes to sequences of symbols which occur in the message.
Thus each extra code describes a block of several symbols, and with suitable data this can lead to considerable compression.
Huffman coding: this is approximately the opposite of LZW, and generates a set of variable length codes for fixed length data items, using the shortest codes for most common data items.
Digital TV video compression.
The same techniques can be used for bit image files.
The most common method is to divide each picture into square blocks of pixels.
A mathematical process called the discrete cosine transformation (DCT) is then used to convert pixel values to Fourier coefficients in terms of the frequencies represented by the digitising process in both the horizontal and vertical directions.
Fractals, using simple mathematical algorithm to produce complex objects typical of those which occur naturally.
Fig. 5 Typical packed-pixl mapping 18-colour screen
Fig. 6 Typical bit-plane mapping for 16-colour screen
Fig. 7.
Fragment of Postscript file
pc engineering
Curve package that really fits the bill
Manually adding best-fit curves to data plots can be laborious and prone to error.
Don Bradbury finds Jandel Scientific's TableCurve curve-fitting program has all the right lines.
You don't need to be statistician or a graphic artist to be able to use TableCurve — the professional-standard curve fitting program designed for scientists and engineers.
But to benefit from its more esoteric features you certainly need a good comprehension of the underlying processes.
Demands made on personal computing skills are minimal, and installation is simple.
One command-line switch will make use of a maximum of 512K of expanded memory, either EMS 3.2 or 4.0; another will put overlays into extended memory; a third disables the ‘degree of freedom’ facility for errors in curve fitting, and a fourth can be used to disable a mouse even if one if fitted and nominally active.
If any further additions are made to the dos command line, they are assumed to refer to an ascii file to be loaded directly.
Other importable file formats can only be loaded from within the TableCurve environment, so suitable data need only be typed into an ascii file with a word processor or line editor to be used, and a path to any directory on the disk can be included if necessary.
Where operation is to be directly from a different data file directory then TableCurve has to be in the dos path.
Program start-up is rather annoyingly interrupted by a 15s delay at a screen showing Jandel Scientific's reminder of the licence agreement, followed by a summary screen showing the program's status in terms of memory use, maths coprocessor installation, estimated processing speed rating for the detected hardware, and various user-modifiable program settings.
When a file is loaded, the summary screen also shows various details of the data ranges — minimum, maximum, standard deviation and so on.
After processing the data, the equation, selected by the program or user, is also displayed on return to the summary screen.
Using the menu system, which can itself be modified to give pop-up, pull-down, or first letter selection of options, the colour scheme can be selected from a choice of four.
Other modification options include video mode, in the unlikely event that hardware has been incorrectly read, mouse response speed adjustment, EMS/help arrangements, printer selection, setting of a user option for function key F3 and so on.
Help is context-sensitive and very useful, though there can be some delay in retrieval sometimes prompting the worry that the program has not received an instruction.
Even longer delays can occur when the printer is set to work.
But what looks like a lock-up just means that the program needs time to respond before any further keys are struck — a ‘wait’ sign or similar signal could be usefully displayed here.
Curve fitting
FILE option allows import of any data file in Lotus 1-2-3 worksheets up to version 3.0, dBase, Symphony, Quattro, ascii or directly from the keyboard.
Data can also be imported from defined binary files.
All these files are listed for cursor selection if the file is in the current directory.
Useful demo files plus the ‘Mini-Tour’ section of the manual demonstrate many of the most-used features of TableCurve.
Up to 1500 data points can be accepted on a 640K machine, or up to 3000 points through use of expanded memory.
If data is loaded from a spreadsheet file, for example, X and Y variable columns to be used can be selected, and default labels the program  expects to use can be edited.
Automatic prescan of data follows for the summary screen, and calculations can be made on a range.
If the range maximum is 40 but 100 is preferable, then Y=Y*100/40 could be entered to transpose the range before plotting.
TableCurve can calculate the data's best fit to one of its equations, or the plot of the data can be previewed to decide whether any data needs to be edited or excluded, or weighting factors applied prior to processing.
Weighting allows pre-judging of the relevance of any points and so reduces emphasis on suspect data.
The weighting screen presents twenty options including on-line help, and saving of changes, while others allow sorting of data, changing of titles, reversing of the XY ranges and so on.
Data processing is achieved automatically: TableCurve can use all of its 221 equations to find a best fit, or the ‘special function’ equations can be omitted or ‘simple’equations selected.
Curve fitting parameters include the correlation coefficient, R-squared, and ranking of fit is according to that factor.
But ‘goodness of fit’ is not the whole of the art of curve fitting — users are encouraged to step through other equation fits, viewing the data and residuals plots as well as the data tables, and all are very conveniently accomplished.
Linear, log, inverse, negative or positive exponential, power, polynomials up to 10th order, rational and special functions including Gaussian, sigmoidal, sine-wave, and user-defined functions are all available.
Other parameters of fit include residuals, standard error for coefficients, standard error of fit, analysis of variance, prediction and confidence limits, and descriptive statistics.
Processing involves floating point calculations, and speed of processing is heavily dependent on hardware.
A maths coprocessor is highly desirable, especially for large data sets, though I didn't use one and the 16MHz 80286 review machine was not unduly slow with files of modest size.
Typically, 20 or 30s was required for a full evaluation using all the equations, though processing could have been ten times as quick with an 80287 fitted.
Plotting
Plotting functions encourage experimentation, allowing not only trials of equations other than that selected as optimum, but also zooming parts of plots to amplify relevance of the fit and resizing the graph to judge extrapolation effects.
Four different parts of a plot can be viewed at once on the screen, the X and/or Y scaled between linear and logarithmic and size of the plotted data points changed.
Colour scheme can be modified to taste.
Variable confidence and prediction internals can be activated, special function parameters displayed, a data summary for a selected equation viewed, and data residuals displayed — either directly as data, in percentaged form, or graphically.
There are also facilities to display the data's standard errors and select polynomial and rational functions.
Equations can be evaluated with an ‘equation preview’ option which provides a calculator-like evaluation process based on the equation currently being reviewed.
Various X or Y values are entered, and the corresponding calculated Y or X value, based on the equation in use, is put into the table.
In all of these processes, table or graphic, a Micro-Soft-compatible mouse can be used for screen navigation, graphic manipulation help screen calls, item selection, and menu operation.
Speed of response can be set over a range of five steps, and may well have to be reset because, particularly in graphics mode, if it is too sensitive, loading up several movement commands into the buffer will require having to wait for them to clear.
This is principally because the mouse can roll in  four directions to replicate the equivalent +,—, * and /keyboard commands to zoom and relocate the graphics.
Using a mouse is very convenient — but some might prefer the keyboard.
Output
TableCurve output options include printing the curve-fit graphs and the data summaries in a high quality monochrome mode.
Choices include full-page graphs alone; to half page graphic and equation summary, or extended equation summaries.
Graphic output may be in standard form, a partition (zoomed-in) or extrapolated (zoomed-out) graph, or it may be of residuals alone.
Graphic output, printed reports and files of many different types can be produced for use with other programs such as Jandel Scientific's own SigmaPlot.
But output can also be used with Lotus 1-2-3 in all versions up to 3.0, Symphony, Quattro and Harvard Graphics.
PCX format files can be used with Ventura, Pagemaker and other desk top publishing programs, and metafiles with WordPerfect 5.0 or Word 4.0.
Some formats then allow further resizing, editing, and enhancement of the graphic output.
Ascii output can also be requested for viewing residuals or generated X-Y data.
Avoiding poor analysis
Referred to as possible ‘pitfalls’ in the excellent manual, some of the more likely situations where erroneous deductions should be anticipated are presented.
Noise on a signal producing the data points is one, though there is a smoothing routine which effectively overcomes this — provided it is used with caution.
Ill-defined regions of the data, rational equations, unjustified polynomial fits (particularly for extrapolations), failure to examine data at low X and Y values sufficiently, reversing X and Y values, and as the manual puts it, ‘trusting numbers rather than the evidence of your own eyes’ are some others.
Most TableCurve users will already have acquired the necessary good sense to avoid such pitfalls.
Those who haven't would do well to browse the informative manual — an impressive publication liberally sprinkled with graphic illustrations of the program's functions and various points to note.
An appendix lists all 221 equations used by the program, and a redefined special functions section deals with non-linear equations which cannot normally be solved by the one-pass direct solution methods used in TableCurve.
The program implements a prescan of the data to determine parameters that cannot be resolved by the direct leastsquares method of analysis.
In this way, Gaussian, log-normal, sigmoidal, and sinewave data are treated just like the program's other functions of X. Transformed equations, power-fit, exponential, hyperbolic, and others are also discussed in an appendix.
Highly recommended
TableCurve is intended to be a fast, one-pass curve-fitting program, with minimal user input.
Data sets precluded from analysis are well documented, but the great bulk of typical experimental data will be handled.
I thoroughly recommended it.
Its price reflects the fact that it is not in the category of software expected to sell in millions of copies.
But it is undoubtedly easy to use.
I was up and running with the package within ten minutes, applying the automatic mode of analysis — and making sense out of it — without reference to the manual.
Hardware
640K allows 1500 data points 512K expanded memory can be used.
Maths coprocessor desirable but not essential.
MicroSoft-compatible mouse can be used.
Supplier details
£305 plus £10 postage and VAT.
Available in the UK from Jandel Scientific, who also provide support, at The Core Store Ltd., The Studio, Hawthorn Cottage, Marbury Road, Comberbach, Northwich, Cheshire CW9 6AU.
Tel: 0606 891980.
Confidence and prediction intervals for curve fitting evaluation are available
Result of fitting data to lower ranked equation
Zoomed-in partitions can be displayed on a single screen
Review
Hard to resist?
Invaluable time-saver for analogue design, Rescalc release 2.0 is much improved and pays for itself within hours, says Ben Duncan
Rescalc V2.0 is a nifty £10/$20 shareware program from the US, for carrying out resistor calculations.
It has been designed to streamline the nitty-gritty business of balancing accuracy against available resistor values, tolerances and cost.
Version 1 of Rescalc has been around for some years and is today regarded as rather palaeolithic.
But this new version has been re-written in Power-Basic.
Basic it still is; even so it does more than many scientific calculators, and release 2 computes faster, has a proper menu structure, EGA colour capability and is generally bullet-proof.
S for simplicity
Instal and test are painless — taking under five minutes including entry into the menu batch file — and operation is intuitive.
But for users that need more help, documentation is contained in a four-page README.DOC written in ‘engineerspeak’.
My own copy has been printed and reduced for reference.
To start, type S for series or P for parallel, and enter any desired (non standard) value in the range 100R to 1M or more.
Rescalc will respond with standard resistor values, listed by accuracy or ascending value, for series or parallel combinations.
Accuracy is cited both in percent error and as the shortfall in ohms.
For critical missions a single resistor value can be entered, and the package will suggest what value(s) to parallel if, say, a production run of PTH PCBs have been fitted with a 17.4k precision resistor which should have been 14.7k…
Resistors listed all belong to a specific tolerance set.
To repeat the calculation for another tolerance set (will a cheap 1% E24 series value give a close enough fit?), just type T and select from the E12, 24, 48 or 96 series, listed under their US aliases, eg 0.1% MIL is E96, and 5% alternative commercial is actually 1% E24.
Dividing with E48 into X
Dividers and padded dividers or trimmers are handled: enter input and output voltage, and the upper and lower arm values are listed.
Typing S (sort) toggles the list on any screen; order of tolerance puts + and — % ranging either side of zero, otherwise values are listed in ascending numeric order.
Percent error is listed for output voltage vs target and sensitivity of output voltage vs resistor values.
Fig. 1.
screen dump demonstrates biasing a comparator with 240mV for a 2.45V reference source, using E24, alias standard 1% metal film resistors.
For padded dividers, the pot or present assumes an E6 value eg 10, 20 or 50kΩ.
In calculating dividers, all values are condensed into medium values (above 100R).
For many circuits, practical values will need to be derived by scaling: but be cautious about accuracy.
There is no problem with comparators fed from voltage references.
But in many circuits, significant source and load resistances will need factoring out from the upper and lower arm values respectively (and scaled pro-rata), otherwise the implied accuracy will be false).
Also rounding errors can affect cases where extreme accuracy is required.
It also performs Wye-Delta transformations (handy for network analysis simplification) and defined multi-tap dividers, for up to ten output voltages.
Characteristics
Speed is fine.
On an XT fitted with co-processor, calculation is under 1s for any mode.
Scientific notation is accepted (1.23e+8) but not engineering units (n, m, k).
README.DOC warns of large values — I found it would accept up to 9M in series calculations, and 1M in parallel.
Values below 10Ω are also restricted, but scaling results should hardly tax the brain — even brains without co-processors.
In divider calculations, maximum input voltage is 1kV.
As a result of these limits, the maximum divisive ratio is 80dB or 10,000:1, enough for 99% of jobs.
Printing (see print dump) is dos PRTSC: boring but reliable.
In any future release, two welcome refinements would be the simultaneous inclusion of source and load resistances in divider computations; and audio and communications designers would like to be able to toggle divider ratios and percent error, to read out in dB voltage (log20) and maybe power (log10) deciBels.
But, overall, Rescalc V2 will prove invaluable for analogue designers and equipment manufacturers.
It should quickly pay for itself by saving computational time, increasing accuracy and reducing parts costs by minimising expenditure on E48 or E96 parts where cheaper E24 values will do.
Fig. 1.
Biasing a comparator with 240mV for a 2.54V reference source.
EDN Design Spotlight
Circuits, Systems &Standards
First published in the US magazine EDN and edited here by Ian Hickman.
NOR gate controls oscillator frequency
You can make a simple, stable frequency-shift-keying (FSK) generator by adding an exclusive- OR gate to a standard cmos oscillator.
In this circuit (figure), the data input controls gate IC ID establishing positive or negative feedback around the oscillator formed by IC IA IC IB and IC IC .
When the data input goes low,IC ID enters its non-inverting mode, and R2 increases capacitor C's charging rate.
When the input returns high,IC ID inverts, and R2 reduces C's charging current, thus lowering the oscillator's frequency.
R1 and C set the oscillator's frequency range, and R2 determines the circuit's frequency shift.
To ensure frequency stability, make R3 much greater than R1 and use a high-quality feedback capacitor.
Note that the three gates constituting the oscillator itself need not be exclusive-OR types — you can use any cmos inverter.
Richard Rice 
Simple FSK generator
There is an elegance about a circuit which achieves its function with great simplicity.
This FSK generator certainly falls into that category.
Both centre frequency and shift will be subject to component tolerances, but the circuit fills the bill as is for undemanding applications; alternatively, present can be designed in to allow adjustment to a standard I.H shift such as 170 or 850 Hz.
IH 
Circuit converts voltage ratio to frequency
Eaton Corporation
The circuit accepts two positive-voltage inputs VN and VD and provides a TTL-compatible output pulse train whose repetition rate is proportional to the ratio VN/VD .
Full-scale output frequency is about 100 Hz, and linearity error is below 0.5.
The output Fo equals KV N/VD , where K=1 (4R2C1 ) and provided R1=R3 .
Op amp  IC1A alternately integrates VN/2 and -VN/2 producing a sawtooth output that ramps between the VD level and ground.
When transistor Q1 is on, for example,IC integrates -VN/2 until its output equals VD .
At that time, the IC IB comparator switches low, causing IC ID 'S bistable output to go low, which turns off Q1 .
IC 1A 's output then ramps in the negative direction.
When the output reaches 0V, the IC IC comparator switches,Q1 turns on, and the cycle repeats.
Transistor Q2 converts the IC ID output to TTL-compatible output logic levels.
Setting VD to 1.00V yields a linear V/F converter (Fo=KVN ), and setting VN to 1.00V yields a reciprocal V/F converter (Fo+K/VD ).
Find the ratio of two voltages
This circuit provided an output voltage proportional to the ratio of two steady input voltages.
The same thing could of course be done with two ADCs and software, but in a system without those facilities, this circuit provides a purely hardware implementation.
Watch out for settling time when one of the input voltages changes, though.
IH 
Diodes and capacitors: imitate transformers
The diode-capacitor network of Fig. 1 accepts low current at a high voltage and delivers higher current at a lower voltage, behaving like a step-down transformer.
You drive the circuit with a square wave input signal a shown.
When the input is at its peak voltage,Vp,current through D10, D7, D4 , and D1 charges series capacitors C4, C3, C2 , and C1 .
The voltage on each capacitor reaches approximately 1/4 (VP-4VF ), where VF is the forward-voltage drop across one diode.
However, the total output voltage doesn't equal the sum of the voltages on the four capacitors; it's less than that by two diode drops.
Consequently, the circuit is inefficient for low-amplitude drive signals (too much voltage is lost across the diodes).
For 15V and 60V p-p inputs, the circuit's corresponding outputs are approximately — 1.65V and — 12.9V, depending on the load.
An input of 28V p-p produces about — 5V.
Notice that the square-wave generator must sink more current than it sources: it charges the capacitors in series, but discharges them in parallel.
When the input terminal switches to 0V, it connects the capacitors in parallel by pulling the positive side of each capacitor near 0V.
The capacitor voltages then produce current flow that creates a negative charge across the load capacitor (CL ).
The voltages in (C3, C2, and C1 ), each charge CL through two diodes in series, but the charging path through C4 has only one diode,D11 .
This configuration results in a higher surge current through D11 and C4 and a slightly higher negative output voltage, unless you add a diode in series with D11 .
You can change the output voltage by adding or subtracting sections;C1, D1, D3 , and D2 constitute one section, for example.
Make the series capacitors equal in value and the total value of these capacitors equal to the load capacitor:CL = 1/2Vrf , where I is the load current, VR is the maximum allowed p-p ripple voltage, and f is the output frequency.
Rudy Stefenel, Luma Telecom.
DC transformer
Here is an intriguing circuit which provides a low voltage high current output from a high voltage low (mean) current source.
The nominal transformation ratio n in the instance shown is 4:1 step down, but unlike some other voltage-changing schemes using capacitors, n can be any whole number; it is not limited to powers of 2.
This advantage apart, the circuit is of limited practical use as the efficiency is not too high due to all the diode drops.
But one day someone will invent the perfect rectifier, and then.
IH 
Electronic Circuits, Systems &Standards edited by Ian Hickman, published by Butterworth Heinemann Newnes.
ISBN 0 7506 0068 3. price £20.
Since its appearance in 1956 the US-based EDN has established itself as a leader in controlled circulation electronics magazines.
Now this 'best’ of EDN — with useful information on components, equipment, circuits, systems and standards is available in a 216 page hardback publication.
Available from bookshops, or direct by postal application to EW + WW , Quadrant House, The Quadrant, Sutton Surrey SM2 5AS.
Cost £20 plus £1.50 post and packing.
An exclusive-OR gate (IC1D) turns a simple cmos oscillator into an FSK generator.
When the data input goes high, IC1D inverts, and negative feedback through R2 lowers the circuit's output frequency.
A low input results in positive feedback and a higher output frequency.
Fig 1 — This voltage-ratio/frequency converter produces a TTL-compatible output pulse train that equals KVN/VD, where VN and VD are the inputs and K=4R2C1.
Linearity error is less than 0.5.
Fig 1 — This diode-capacitor network converts an input square wave to a negative dc voltage.
Phase-modulate signals digitally
With two low-cost ICs, you can modulate a square-wave carrier's phase in direct proportion to the value of a 4-bit data input.
In the required circuit (figure, part (A)), a clock signal running at 16 times the desired carrier frequency drives the 4-binary counter IC1 .
The counter's outputs are binary numbers representing 16 equidistant, cyclically generated carrier phase angles.
IC2 , a 4-bit full adder, implements the phase modulation by summing the modulating data with each of the carrier phase angles.
The instantaneous binary representation of phase angle at the adder's output is offset from the original by a quantity proportional to the input's value (b).
Each new data input results in proportional shift in the cyclically generated phases.
The adders MSB output is the required phase-modulated square-wave carrier at frequency Fc.
If you need a sine wave rather than a square wave, you can add the shaping circuit (reference).
The table (c) shows the required data-input connections to the adder for two, four, eight and 16 phases.
Note that the circuit is not restricted to 16 phase levels.
Using an n-bit counter and an n-bit full adder, you can generate a 2n-level phase-shift-keyed (PSK) signal.
Remember, however, that the clock signal driving the binary counter must run at 2n times the required phase-modulated carrier's frequency.
For higher speeds, use high-speed logic (ECL, for example).
Reference
McGuire, Electronics, October 2, 1975, pg 104.15 Nick Boutin, University of Sherbrooke.
Digital phase modulator
A neat little circuit this, providing up to +180 degrees phase modulation in 22.5 degree steps.
If you are going to subsequently multiply, or frequency shift, the output to arrive at the final transmit frequency, then the optional sine shaper can be omitted — leaving a phase modulator which could scarcely simpler.
IH 
Phase-modulate a carrier with this simple circuit (a).
To obtain a sinusoidal output, add the optional shaping circuit: Choose CF for optimum response at the desired frequency.
Part (b) shows the phase shifts for the 16 combinations of a 4-bit data input:(C) gives the adder input connections for 2-, 4-, 8- and 16-phase resolution.
Interfacing with C
by Howard Hutchings
Interfacing with C can be obtained from Lindsey Gardner, Room L333, Quadrant House, The Quadrant, Sutton, Surrey SM5 2AS.
Please make cheques for £14.95 (which includes postage and packing) payable to Reed Business Publishing Group.
Alternatively, you can telephone your order, quoting a credit card number.
Telephone 081–661 3614 (mornings only, please).
A disk containing all the example listings used in the book is available at £25.50 + VAT.
Please specify size required.
C Here!
If you have followed our series on the use of the C programming language, then you will recognise its value to the practising engineer.
But, rather than turning up old issues of the journal to check your design for a digital filter, why not have all the articles collected together in one book, Interfacing with C?
The book is a storehouse of information that will be of lasting value to anyone involved in the design of filters, A-to-D conversion, convolution, Fourier and many other applications, with not a soldering iron in sight.
To complement the published series, Howard Hutchings has written additional chapters on D-to-A and A-to-D conversion, waveform synthesis and audio special effects, including echo and reverberation.
An appendix provides a ‘getting started’ introduction to the running of the many programs scattered throughout the book.
This is a practical guide to real-time programming, the programs provided having been tested and proved.
It is a distillation of the teaching of computer-assisted engineering at Humberside Polytechnic, at which Dr Hutchings is a senior lecturer.
Source code listings for the programs described in the book are available on disk.
Regulars
Circuit ideas
Sound sampler filter
Using an 8-bit sound sampler board with Atari, Archimedes or Amiga computers for sound analysis in physics and biology requires a sample rate of at least twice the highest signal frequency — around 20kHz.
Some samplers possess a simple low-pass filter rolling off at about 16kHz, but do not remove enough of the signal above 20kHz to allow a sample rate of 40kHz.
The diagram shows a low-pass filter to give the necessary performance using an inexpensive LC element which is intended for Nicam systems.
Buffering is provided by (formula provided) matching the output to the filter input impedance.
Resistor R3 matches the output to IC2 and, together with R4, sets the gain of the circuit, which is 0dB overall when circuit losses are taken into account, Components C1 and R1 form a high-pass filter, rolling off at 3.4Hz or 340Hz, depending on which value is used.
Prototype performance was within 1dB between 700Hz and 14.5kHz, with roll-off points at 330Hz and 15.2kHz; above 19.7kHz, signals are at least— 51dB.
The filter is a Toko part, an A258BLV-5085N , from Cirkit and Maplin.
L May Rochdale Lancashire 
Multimeter as frequency meter
A digital multimeter makes a good frequency meter with the addition of the circuit shown.
In principle, the average direct voltage of a rectangular waveform depends on the ratio of a fixed-width part of the waveform to the variable width of one cycle.
In the diagram, the input signal, after any prescaling in IC3a, b, and IC 4a, generates a very narrow pulse at the output of the second Nor, which is used to reset 14-stage binary counter IC1.
The counting then proceeds under the control of a built-in crystal-controlled oscillator until Q10 becomes 1, whereupon the oscillator is made to stop.
The crystal ensures that the interval between reset and stop is stable and accurate.
The width of the interval is (formula supplied) and the cycle time is (formula supplied), so that the average output direct voltage is (formula supplied) where Vdd is the voltage of the power rail.
Since Vdd and Tc are constants, output voltage is proportional to input frequency, R5 and C2 smoothing the square wave to give the direct voltage to drive the voltmeter.
Switch positions 1 to 4 select the divider outputs to give full-scale ranges of 19.99kHz to 19.99MHz.
To calibrate, adjust R3 to give a 1.9V direct-voltage output when the input is 19kHz.
Yongping Xia West Virginia University Morgantown WV USA 
Low-pass filter is steep enough to allow sound sample rate of twice signal frequency, with attenuation of better than 50dB at 20kHz.
This circuit, in conjunction with a digital voltmeter, gives a stable and accurate digital reading of frequency from about 50Hz to 19.99MHz.
Sine waves from a 4046 VCO
AM detection and waveform generation are two of the expanding number of uses for the phase-locked loop IC, the one in question being the 4046 cmos type.
Quadrature sine or triangular waves are derived from the VCO timing capacitor in a similar manner to that described in ref.1.
Voltage across the timing capacitor C1 is a linear differential sawtooth having a constant peak voltage with frequency.
The two fets make up a high input-impedance differential pair with unity gain and over 10MHz bandwidth, which maintains linearity at low charging currents and at high frequencies up to 18MHz.
Summing the two fet outputs in Tr3 gives a triangular wave and a small square component, which is removed by taking some of the squares at the VCO output to the summing point.
To obtain a sine output, use the two schottky diodes back to back on the summing point to round the triangle: the resulting sine is accurate to about 1% THD.
Adjusting VR2 gives a symmetrical waveform and you can do it by ear by reducing the frequency so that only harmonics can be heard.
The timing capacitor must be changed to 10nF to do this.
With C1 at 40pF and R1 at 3kΩ, the HC4046 works up to 18MHz typical, drawing around 10mA.
Values of 10nF and 1MΩ produce 20Hz.
Ian Hegglun Manawatu Polytechnic New Zealand 
Reference 1.
XR-S200 data sheet,Exar Data Book .
Motion direction detector
I suggest that a simpler approach to detecting the direction of motion of an object, one method of which was put forward by M Kumaran in July, is to use half a 74LS74 D-type flip-flop, as shown in the diagram, which performs exactly the same function.
A Clark Boldon Colliery Tyne and Wear 
D-to-A converter current booster
An increase in output current from the popular DAC 08 D-to-A converter is obtainable by means of this circuit, the current accuracy being better than 1% and offset current less than 0.1%.
Current gain is 16 precisely and output voltage lies between — 8V and +25V.
The only thing to watch is the likelihood of oddities at large transitions — all 0s to all 1s, for example.
The relatively low slewing rate of the LM308A is responsible for this and imposes a limit on clock frequency of 15kHz if such an eventuality is at all likely.
Alexandru Ciubotaru Galati Romania 
Function generator using a PLL produces square and sine or triangular waves from 20Hz to 18MHz.
A simpler motion detector
Precisely sixteen times the 2mA output current from a DAC 08, with a low current offset error
Speech compressor
An article in the July issue reported a method (Simitar) of increasing loudness of communications signals by up to 20dB, mentioning work at Swansea University about 14 years ago.
It is possible that an IC will eventually emerge to perform the function but, until then, the following description should allow people to develop their own system.
Figure 1 shows the principle.
Signal is filtered, digitised and sampled by a microprocessor at a rate of at least 8kHz, samples being processed by the micro and output via a D-to-A converter.
At the output, two D-to-As are needed, as seen in Fig.2.
Input signal from the micro goes to DAC1, the scale factor being written to DAC2 which, in this configuration, act as a divider to modify the output by the scale factor input from the micro.
Any 8-bit microprocessor will work, since no multiplication or division is necessary in the software, although it would be of advantage to use a 16-bit device to do the scaling of signals internally.
Software is shown in flowchart form in the table.
The micro stores samples as sign/magnitude values in a 64-wide array (SAMPLES) indexed by pointer (PTR).
Before doing that, it takes the old sample and writes it to DAC1, so that the array behaves as a 64-character circular buffer which merely delays the samples.
This delay allows the micro to determine the signal peak value between zero crossings, each being compared with the last one (IN0) and with a threshold value (THRESHOLD).
If greater, the sample is stored in the array PEAKS, indexed by PIN.
When a zero crossing is detected, PIN is incremented and PEAKS and IN0 reset.
When samples are retrieved they are also zero-crossing tested, the peak value being taken from PEAKS with the second pointer POUT.
The DACs give the function OUT1/SCALE to scale each half cycle to the same magnitude, which is said to provide a 20dB increase in loudness, subjectively, with none of the usual speech compression problems.
Looking at Fig. 2, The DACs are multiplying current-output types such as the AD7528 from Analog Devices.
Current I1 from the first is 
A somewhat simpler circuit in Fig. 3 eliminates the amplifier and two resistors, the gain being (formula provided) but two low-impedance references are now needed, since high-impedance ones introduce distortion.
David Gibson Leeds West Yorkshire 
Fig. 1.
Speech compressor in essence.
A sample rate of at least 8kHz is needed and the microprocessor can be any 8-bit variety with simple software.
Fig. 2.
Two separate DACs perform the functions of conversion and scaling.
Fig. 3.
Simpler embodiment of the idea with the slight drawback that two high-Z voltage references are needed.
Table 1: software flowchart for implementing David Gibson's processor proposal.
There is no connection with Dr Louis Thomas' work.
signal processsing
Building a toolbox for DSP Engineering
Allen Brown looks at how to choose the best DSP engineering tools in response to a fast-developing market
Digital signal processing (DSP) is generally regarded as one of the fastest expanding areas of electronics.
Advent of low cost processors allowing DSP algorithms to be implemented with relative ease means electronics engineers now have much greater awareness of its potential — certainly not the case five years ago.
Continuing commitment and investment by IC manufactures in developing high performance DSP chips points to a very promising future for DSP.
The four front runners with the highest market profiles are Texas Instruments (with its ever expanding TMS320 series), Motorola (DSP56000 and DSP96002), Analog Devices (the versatile family of ADSP-210x devices) and AT(the long standing DSP32C among others).
Several second row contenders such as Zoran, Thomson and Philips are struggling to capture a sizeable foot hole in the market.
But it is the Japanese manufacturers who have made a poor showing in the DSP arena — probably due in part to a lack of commitment and feeble marketing.
Other reasons for their conspicuous absence could be an apparent weakness in software, as DSP requires powerful software support tools and availability of these tools is a prime consideration when specifying a DSP for a design project.
Initially, when the market was small there were several contestants but some have subsequently fallen.
National Semiconductor made a brave attempt with its LM32900, and the take-over of Fairchild saw the demise of the FSP100.
Now it appears that the four main front runners will continue to dominate the scene though we are likely to witness some elbowing in the scrum.
PC as development tool
Despite the degree of criticism levelled at the PC, it remains the most popular tool for developing software for microprocessor and DSP systems.
Reasons are that the PC has become the conventional engineering tool; most DSP applications programs run under ms-dos; hardware interfacing of target DSP boards to the PC has become a standard practice, and the PC represents a low cost entry point into the field of DSP development.
Windows 3 or other multitasking environments have made software development a little easier because each window can feature a different software tool.
But most DSP development software is also available for workstations and Macs.
The four main processor manufacturers produce a wealth of software and hardware-development PC-compatible tools so much of the DSP development software will run under ms-dos.
Over the past couple of years we have seen application tools embrace the facilities offered by the Windows front end or graphics user interfaces (GUIs).
An example of a respectable product which embraces the full potential of GUIs is Texas Instrument's debugger for its TMS320C30 evaluation module — it has the same screen format as the simulator for the new TMS320C40).
However this has not always been the case; the first version of the Texas Instruments TMS32010 simulator had an appalling screen display which relied on scrolling instead of bit mapped refresh.
Software development products
A suite of software development tools for digital signal processor should include a cross assembler, a linker, a software simulator and a high level language (HLL) compiler which usually means a C compiler.
C has become the de facto primary HLL  now taught as a matter of course to all emerging electronics engineers and their education would be regarded as incomplete without it.
Before C, the only method of programming compilers for digital signal processors was to use assembly language, a restriction probably delaying acceptance of DSPs as viable devices.
A new assembly language entails yet another learning curve.
A language compilers allow a stepping into the shallow end with a gentle wade towards the awaiting DSP sharks in the deep end.
In practice, a mix of assembly language and C will be needed.
Real-time applications used to require coding exclusively in assembly language whereas the house keeping tasks could be coded in C. But now that the practices of software engineering are coming to bear on DSP projects, the emphasis is on a system approach ensuring quality becomes an intrinsic aspect of design.
C's attraction is that it lends itself to structured programming more readily than assembly language programming and it is within a C language shall that the assembly language modules should be inserted.
Each assembly language module can be considered as an object with a specific set of rules for data input and output, thus enabling methodical testing techniques to be used.
Suspect C compilers
Most C compilers conform to the Kernighan and Ritchie standard!, but there is a strong variation in quality of compilers for DSP devices.
Whether a compiler is a quality product soon becomes evident, and with some there is the suspicion that they were cobbled together as a response to opposition product innovations.
This has resulted in poorly-engineered products — particularly true for Motorola's C compiler, Version 1.233, for the DSP56001 which is hopelessly inefficient and inserts NOPs (no operation instructions) after every three or four compiled instructions due to pipeliners in the processor.
The assembly language code generation from the compiler fails to capitalise on the superb architecture of the chip and in many cases just does not produce executable code from a tried and tested C language source.
When the new version of the C compiler is released, I sincerely hope that Motorola does not charge for the update, I also hope it provides an adequate manual not like the scant document which accompanied version 1.233.
Analog Devices also has a tarnished image with its C compiler for the ADSP-2101; it was not until version 4 that the maths runtime library became available — surely a little late in the day for a fundamental component.
Texas Instruments, having recognised the wider implications of producing high performance processors, has attached great importance to its C compilers and produces them for their TMS320C25 and TMS320C3x.
Compiler for the TMS320C3x produces high optimised code and is supplied with an extensive range of run-time library facilities.
Documentation is well written and contains detailed information on usage of the function calls.
Of interesting recent developments in C compilers, one which deserves a mention is the XCC (eXtended C Compiler) for the ATDSP32(C) floating point processor.
Issued by Rich Software and distributed by Bores Signal Processing it contains a number of library functions which allow the DSP32(C) to access the bios of the hose PC.
If a user has an expansion card featuring the DSP32(C), then source code can contain the usual graphics and normal input/output instructions found in Microsoft C.
As a result C language programs can be run on the DSP32(C) instead of the PC's CPU, giving a quite remarkable increase in performance.
A Pascal compiler is also available.
Cross assemblers
Many real-time applications require source code written in assembler, and most assemblers have relatively steep learning curves for DSPs.
The reason for this stems from the parallel and/or pipelined architecture.
for number of DSPs a single line of assembly code may contain four or more sub-instructions and learning how to capitalise on the architecture through efficient use of the instruction set does take time.
With pipelined architecture, learning is even more difficult since the processor can be executing parts of four lines of code simultaneously.
Most assemblers have facilities for creating macros and library functions and have the appropriate facility to allow parameter passing.
To accompany the cross assembler a linker will perform the usual task of allocating addresses to the assembled code modules and attaching the run-time libraries to generate an executable program.
When programming in assembler, the role of the simulator, copying operation of the DSP chip, can be quite invaluable especially if testing performance of numerical operations.
Simulators have come a long way in recent years and today many of them use screen addressing to update the information.
An example is the Avocet Avsim321 TM32010 simulator — a good product as it gives instant visual access to all registers of the processor so users can monitor progress of the program, line by line if required.
For processors which have more complex architectures it is not always possible to have all features present on the screen simultaneously — unless you have dynamic windowing.
An example of a software product incorporating this feature is Analog Devices's  simulator for its ADSP-2101.
Nature and number of windows is defined by the user and size of the windows is defined by the user and size of the windows is adjustable.
An attractive aspect of this approach is that it enables the designer to concentrate on particular areas of the processor's operation.
Texas Instruments also has a quality simulator for the TMS320C30 processor which provides the designer with an extensive range of options evoked through the command menu.
The help file is well thought out, but not all simulators are as clean to use.
The simulator for the ATDSP32C (provided by AT) has a poor screen display which relies on scrolling refresh.
It is not user-friendly and leaves a lot to be desired for a software support product — rather surprising considering that the DSP32C is a very impressive device.
Hardware development tools
Simulators are effective for testing small segments of code.
But problems do arise when attempting to test complete programs of several segments.
They are slow and inappropriate, especially if time is at a premium as is most often the case.
In this case, the expansion card hosting the target processor comes to the fore.
Success of the PC as an engineering tool must in part be due to its expansion bus and the fact that nearly every type of facility can be found on a PC card.
Cards are by far the most effective means of investigating performance of a DSP device.
But the user must be patient when learning the ins and outs of an expansion card.
Information in the user manual is almost always inadequate leaving new users to find out for themselves how it works — although this does enhance practical knowledge of the processor.
Many companies manufacture expansion cards featuring DSP chips (for example Burr Brown, SMIS and Loughborough Sound Images) and it is true to say that every major DSP chip is available on an expansion card.
Choice of an expansion card, with a particular processor, will be determined largely by intended application for the device.
If for a general purpose need, then the board should have at least two independent analogue I/O channels, which means two separate A-to-Ds and D-to-As.
These should be a minimum of 12-bit, or 16-bit if possible with sampling rates of at least 100kHz.
Serial interfacing with appropriate amplifiers should be provided in support.
Amount of memory to be found on expansion cards will usually be small owing to cost (a minimum of 16K depth).
Fast, zero wait state sram is expensive and since many DSPs have 24-bit data buses, can prove to be quite costly.
If you are lucky you might get a codex thrown in for a good measure for telecomms application.
An attraction of the PC expansion card is the ease with which data can be transferred between the card and memory on the host PC — particularly useful for testing and debugging software.
Instead of using the traditional patch in the machine code, the user can switch to an editor in another ‘window’ to allow the changes to be made at source.
Considering the options
The PC expansion card is not the only standard to host DSPs; VME boards have also proved to be a popular choice with several manufacturers.
Of course they represent a greater investment than PC cards.
But flexibility of the VME standard does have its advantages especially if previously designed systems were based on it.
Data Beta of Theale, for example, produce a VME board hosting two Motorola 96002 floating point processors.
It is a particularly potent device for signal analysis since it can perform a 1024 point FFT in under 700µs.
Of late, stand-alone evaluation boards have fallen out of favour, mainly for two reasons; many evaluation boards use difficult to standardise RS-232 links, and the boards require a power supply with +-12V for the RS-232 link and another 5V for the power rail.
Using a PC expansion card drawing power from the PC avoids the hassle of both problems.
Their advantage is the presence of a wire wrap area found on some for user modifications.
But on the whole they are expensive and because the RS-232 link communications is slow, it is difficult to take advantage of a GUI environment.
Once a user is convinced that the DSP chip is capable of fulfilling the system requirements then it is time to start the design for  real, probably design of an embedded system.
Two main hardware tools are at the disposal of the designer, an in-circuit emulator and a logic analyser with processor's code dis-assembler.
Needless to say these represent a large investment costs at several £k for the logic analyser.
A belief held by a number of software designers is that if the software is properly designed then there should be no need for an emulator; it should work first time.
But few engineers subscribe to this theory and at the end of the day it is the hardware engineer who has to ‘get the thing working’.
Emulators will still be needed, for the foreseeable future at least, though their design is becoming increasingly more difficult with every rising clock speeds.
Emulators are available for the major DSP chips and their facilities can show considerable variation — getting inside the processor to determine its state, especially during interrupts is often a necessary requirement.
Core features
Features expected in a DSP emulator are not too different from those for general purpose micros.
In many cases they are actually simpler since a complete instruction is executed in a single machine cycle unlike a microprocessor which may make several memory access when executing a single instruction.
But what are these facilities?
Some processor memory should be available to act as external emulation memory for the processor and there should also be adequate memory to allow a trigger trace analysis to be performed.
There should be scope for at least 256 instructions traced before the trigger and possibly 256 instructions traced after the trigger.
Trigger condition should be configured from multiple logic conditions.
Ability to set multiple software and hardware break points is also necessary.
Software application products
There is a range of other software application programs on the market which may be described as design aids.
Most common category is that of digital filter design programs and there are several of these and most of them have common features.
Broadly speaking these include:
FIR filter design using the Parks McClellan and Window design methods, Differentiator and Hilbert transform design, IIR filters based on bilinear transform method, Display transfer functions, impulse response and pole/zero positions, Design filter directly from transfer function plane by specifying frequency cut-offs and attenuation levels, Quantise coefficient to a given word size, and Code generation for specified DSP device.
Most digital signal design application programs have emanated from US (reviews of a number of these products will appear in forthcoming editions of EW + WW ).
One of the first to emerge was DFDP from Atlanta Signal processing (Fig. 1), at one time marketed by Texas Instruments.
Sourced in Fortran it contains several of the features listed above.
Momentum Data Systems of Costa Mesa produces a filter design program, Filter Design &Analysis, which performs well (Fig. 2.)
Monarch of Gainesville also produces a filter design program entitled DSP Development System, containing a wide selection of filter design features).
These packages are dedicated filter design programs.
But another group of commercial programs offer filter design as a part of the package.
Hypersignal Workstation (reviewed May 1990) and DisPlay-XL (reviewed May 1991) are two such products.
Signal Technology of Goleta, produces a comprehensive application product (Integrated Laboratory Systems — ILS) having digital filter design as a part.
Although not as extensive in its filter design as some of the above, it is still an impressive product.
DSP and software engineering
Future systems which contain DSP technology will have to be properly engineered.
Since they will have a substantial amount of software embedded, then techniques of software engineering will have to be heeded in their manufacture — a necessary step in ensuring quality and verification.
Techniques such as object oriented design and structured programming will have to be followed to conform to engineering practice.
Power of DSP devices will necessitate multitasking in real-time, invariably involving operating systems with real-time kernels.
Spectron of Santa Barbara has already recognised this requirement and produced a real-time operating system for DSP applications.
Referred to as Spox, the OS can be hosted by Texas Instruments third generation products and by the Motorola DSP96002 processor.
Normal features expected in an OS are present including task scheduling, interrupt handling and resource management.
DSP based products will represent a rich blend of several engineering disciplines, signal processing, digital and analogue hardware design, systems engineering and software engineering.
Finding engineers able to master all these areas is going to be difficult.
But help is at hand.
Products mentioned all run under dos and many are available to run under other operating systems used by workstations.
A software product which runs exclusively on workstations is Signal Processing WorkSystem of SPW from Comdisco.
This is a very comprehensive development tool allowing the engineer to make an entry at virtually any level in the design phase of a DSP based product.
(A full review will be appearing in a later edition of EW + WW .
References 1.
B.W. Kernighan and D M Ritchie.
The C Programming Language , Prentice Hall, second edition (1978).
Computer General Panther with TMS320C30 processor can turn the PC into a 33Mflop workstation.
Applications can be written using the T1 software suite supplies consisting of a C compiler, assembler and linker.
Fig. 1.
Display from DFDP (the current version uses EGA) showing plot of a response of a filter design.
Fig. 2.
Momentum's filter design product performs well; characteristic plot of a filter transfer function.
Fig. 3.
Digital filter design is part of ILS's product and though not as extensive in its filter design as some of the others, it is still impressive.
Education
Lessons in applied science
King's College recently opened its doors to show off research work at the department of Electrical and Electronic Engineering.
Fibre optics, communications systems, integrated circuits and digital signal processing were all well represented and, strikingly, most of the research is aimed at practical applications.
Andy Gothard returns to the Alma Mater.
One might not expect sawing a Renault 5 in half to constitute part of an electronics research project, but that's what they call it at sub-basement level, below the Strand.
The ‘Renault 2.5’ is part of Drivage , and EC-funded project intended to assess the performance of older drivers, and work out what technical advances might be useful to older age groups.
The project has involved collaboration with the college geography department, and the traffic studies centre of the University of Groningen, in Holland.
The car is part of a driving simulator, which uses a Philips video disc player, in conjunction with an Acorn Archimedes A440 computer, to project video images onto a screen.
The screen replaces the car windscreen, and on it is shown a road scene that has been recorded on video tape, and transferred to disk.
A C program runs on the Archimedes, and controls the playback rate in response to the subject — the ‘driver’ of the car — working the brake and accelerator.
The task is to follow lead car, which also appears on the screen, and travels at a varying rate.
The subject is told to keep the lead car at a constant, safe distance.
Alarm messages appear on the screen if the chase car gets too close to, or too far from, the lead car.
System state is recorded three times a second, the data being preprocessed on the Archimedes before being passed to the college VAX cluster for statistical analysis.
Although drivers cannot ‘steer’ the car in the current simulation, they do get a convincing impression of speed and distance, as well as a scene recorded from real life.
It is also possible to set drivers a distracting task — for instance, a led on the car dashboard may light occasionally, requiring the driver to push a switch to cancel it.
The research should help quantify the differences between older and younger drivers.
So far, 150 have been tested, and the full statistical results will be available soon.
The research team's David Fraser says that preliminary results indicate that the range of driving performance is age independent, far outstripping variations between groups.
So next time you're stuck in your car behind that little old lady's Austin 1100, remember, statistically, she's probably no worse a driver than the 20-year-old in the Porsche which is about to overtake you both.
The sound of fractals
Chaos and fractal imagery have hit the technical and popular imagination over the last few years, and the King's DSP group is working on a number of applications for this new branch of mathematics.
In one of them, researcher Jonathan Mackenzie is using the ideas behind fractal geometry to synthesise sounds.
Mackenzie's technique uses a variant of Michael Barnsley's iterated function system, called a fractal interpolation function.
Taking a (sampled) waveform as its starting point, the function fills in gaps between samples with a replica of the original waveform, contracted in time.
Results are ‘self-similar’ waveforms which, like the classic fractal patterns, look the same as magnification increases.
All this is achieved using some C code, running on an IBM clone.
The sounds produced have a fractal sound quality — they sound the same whether the sample generated by the program is delivered quickly or slowly.
The most interesting noises emanating from Mackenzie's bench are quite hard to describe, being a combination of rhythm and timbre.
The next stage in the work, Mackenzie says, will be to produce a real-time version of the system, using a Texas Instruments TMS320C30 DSP chip, which will be able to deliver a short sequence of the waveform, while doing the maths to generate the next section.
This in turn will allow the generation of continuous sounds, and sounds which change with time.
It should also be possible to use the computer screen as a kind of operator panel, allowing the user of the system to modify, in real time, the interpolation points.
Mackenzie is also looking at other mathematical techniques for analysing and synthesising waveforms.
These include the wavelet transform, which can give a measure of the fractal qualities of a given sound, and something called the granular synthesis model, which considers sound as consisting of quanta localised both in time and frequency.
Gallium arsenide filter
Downstairs, under the aegis of Professor Garth Swanson, researcher Jim Luck is looking into the possibilities of designing and fabricating monolithic high-frequency filters, using gallium arsenide GaAs insulated gate field effect transistors (igfets).
Igfets have strange characteristics, which mean that they are not much use at frequencies of below a few kilohertz.
They are essentially depletion mode devices, which might suggest that they conduct electricity when no bias is applied, and conduct less under bias.
In fact, the same igfet can operate as ‘normally on’, or ‘normally off’.
The reason lies in what happens at the insulated gate itself.
The structure of the igfet leads to what are known as surface states.
This means that, where GaAs meets the insulating layer, electrons can exist at energies not usually allowable in plain old n-type GaAs.
Surface states are neither in the conduction band (i.e. free, and able to conduct electricity) nor in the valence band (busy keeping the crystal structure together), but somewhere in between.
The net result is that trying to create a region beneath the gate where there are no electrons (a depletion layer) becomes very hard work indeed.
Putting a negative bias on the gate simply frees up electrons living in the surface states, which promptly (within 1ms) undeplete the layer.
This seems to indicate that the igfet should not be of much use and, indeed, this is the case at low frequencies of operation.
But if the gate voltage is clocked very quickly (at tens of megahertz), the negative-going edges of the clock pulse produce depletion, and the positive-going edges (which restore the previous status quo) arrive before the surface charge has had a chance to confuse matters.
This is why the devices can be both normally on or normally off.
They respond to changes in gate potential, not absolute levels — switching between 0 and 3V looks the same to an igfet as switching between — 3 and 0V.
Perhaps the easiest way of looking at this is to think of the surface states as a charge store, which effectively shifts the switching level of the device.
In addition to its unusual electrical properties, the igfet is simple to make, and, because of the insulated gate structure, current into the gate is virtually eliminated.
All of this makes it very attractive for the application which Luck and Swanson have been working on — the construction of switched capacitor filter circuits.
Switched capacitor circuits normally use silicon mosfets, of GaAs mesfets.
But the former will only switch at a relatively low speed (42MHz), and the latter produce complex, power-hungry circuits.
The igfet SC implementation constructed at King's uses a chip designed at University College, and fabricated at Plessey III/V for the operational amplifier and capacitor parts of the  circuit.
King's itself made the igfets, using a gate length of 5micron, and 5micron access regions.
The resulting first-order filter circuit clocks at 25MHz, and produces a measured characteristic which is within 0.04dB of the theoretical value at the 1MHz cut-off frequency.
The DC power consumption of 190mW is entirely dissipated in the operational amplifiers and, when operating, the igfets consume around 20µW per switch.
This, says Luck, compares with 80mW per switch for a corresponding mesfet implementation.
Luck also managed to demonstrate the circuit's insensitivity to the DC level of the clock signal using a low-frequency version.
Changing the DC level by 5V produced a change in filter characteristic of less than 0.1dB.
Garth Swanson says that the existing work could be extended fairly easily to switching speeds of 625MHz by making the igfets on a 1µm process, now standard fare in the semiconductor industry.
In fact, the idea lends itself to fabrication as a single IC- clock distribution is not much of a problem, because of the insensitivity to DC level, and transistor count is low, because each of the switches is formed by a single igfet, rather than, for instance , several mesfets.
The switching elements are also micropower.
Semi chips are a gas
Swanson is also involved in studies on fabrication techniques for III/V ICs.
Part of this is Patrick Dainty's work on a technology called remote plasma enhanced chemical vapour deposition (RPECVD).
Although III/V substances such as GaAs and InGaAs are increasingly being used commercially, their usefulness is still constrained by what is known as final passivation — the last layer of the IC structure which protects the works against mechanical shocks and contamination (for instance by water), and also provides electrical isolation.
The problem is to produce a high quality insulator, without heating the substrate so much that the substrate itself suffers.
The temperature needs to be kept below 400°C, which compares with a figure of 1200°C for thermal oxidation of the other main semiconductor, silicon.
The substance of choice for passivation layers is silicon nitrite, Si3N4, and the manufacturing technology is plasma enhanced chemical vapour deposition (PECVD).
The whole growth process takes place in a plasma chamber, and therein lies the problem.
The high energy plasma can cause damage at the semiconductor/insulator interface, which in turn gives rise to leakage currents and poor isolation (this mechanism actually involves the creation of the surface states used to good effect in Jim Luck's igfets).
This, according to Dainty, causes low-frequency noise in microwave circuits, and can increase the dark-current of photodiodes.
The answer, apparently, is remote PECVD which removes the substrate from the plasma chamber.
Dainty's work has so far concentrated on using the technique to grow silicon nitride on silicon, with the hope of a later move to III/V substrates.
The way it works is by injecting nitrogen gas into a 2.45GHz microwave plasma.
This produces so-called activenitrogen, which passes out of the plasma region.
Silane, SiH4 is injected into the active nitrogen stream, and reacts with it to produce SiH.
When this jet of gas reaches the substrate, reactions with other nitrogen species produce a passivation layer.
The resulting layers have high breakdown electric fields (3MV/cm), and are highly uniform.
The layer thickness is around 0.1µm, with a tolerance across an entire wafer of +-5%.
The layer quality is assessed primarily by capacitance measurements, which can be used to demonstrate the low occurrence of surface states.
The next stage of the work, says Dainty, is to eliminate a layer phenomenon called ionic motion, which shows up as slight hysteresis in the capacitance/voltage plot of the layer.
There are also prospects for the deposition of silicon dioxide, and silicon oxy-nitride.
Measuring current with light
Another area of activity for King's is in fibre optics.
Research projects presided over by professor Alan Rogers include the identification of large molecules, such as those of interest in medical tests, the production of fibre lasers, and the design of distributed measuring systems.
One of the projects close to commercial realisation uses optical fibre to sense a current flowing in a conductor.
The interesting part is that the measuring fibre does not need to be part of the current carrying circuit, giving a high degree of electrical isolation.
The physical basis of the measurement leads to a linear response over a wide dynamic range, and reduces saturation and hysteresis effects.
The technique relies on the use of high birefringence fibre.
This supports two distinct modes of light propagation polarised at right angles to each other.
Under normal circumstances, there is no coupling between the two modes, which have different propagation constants.
However, when the fibre is put into a spatially varying magnetic field, a phenomenon called Faraday rotation comes into play.
This effectively changes the polarisation direction, and can cause coupling between the two modes.
So, if the current of interest can be made to produce a magnetic field which varies in the right way along the length of the fibre, this can be used to measure current.
Danny McStay and Welland Chu are the researchers working on the project.
They have devised several ways of producing the required magnetic field, including a slotted busbar (such as might be used in a power station) around which the fibre can be wrapped.
It should be possible to dope or magnetically insulate the fibre itself, producing an effective periodic variation in field, without any mechanical changes to the conductor itself.
Temperature dependence is an area which requires further development.
The characteristics of optical fibre change with temperature, sufficiently to change the required magnetic field period.
This problem can be eliminated by, for instance, producing a bus bar with a range of slot spacings or doping the fibre to produce a slight spread of magnetic field period.
This effectively spreads out the resonance peak (which at a steady temperature is very sharp), and produces strong coupling at a range of temperatures.
Measuring current with light: Danny McStay (left) and Welland Chu harness Faraday rotation to vary the degree of coupling between a pair of optical fibres carrying polarised light.
This has obvious implications for making floating current measurements in high voltage distribution systems.
The inset picture shows a slotted bus-bar for use with the optical sensor.
A fractal description originally developed by mathematician Michael Barnsley, has been adapted to digitally sampled audio.
The result is the unusual sound of fractal based audio.
King's Researcher Jonathan Mackenzie used Barnsley's Iterated Function System to fill in the gaps between sample points with a fractal representation of a low frequency signal.
Fig. 1 shows the waveform resulting from three iterations of a sine wave.
Note the high frequency content appearance in contrast to the squarewave which results from the addition of simple waveform harmonics.
Mackenzie went on to produce fractal sounds from sampled real world waveforms.
Figs. 2a, b and c show successive waveform expansions of fractal processed wind noise.
The visual similarity in the detailed structure is obvious as is the regular nature of the rhythm (Fig. 3 above left).
Mackenzie reports that the resulting sound is ‘disappointing’ and has a low volume ‘wind like rumble in the background with a louder whine on top’.
However, it remains exciting from the experimental point of view.
The sound remains the same when played back at half the speed but lasts for twice as long.
Galluim Arsenide fets processed with a special surface deposition exhibit indeterminate switch-on thresholds at low frequencies but work like normal transistors at higher frequencies.
In other words, they only respond to the differential of input signal waveforms to create single transistor hi-pass filters.
The circuit (Fig. 4a, middle left) shows a first order switched capacitor integrator where each switch element is implemented with a single GaAs igfet device replacing the multi-device circuits required by standard mesfet technology.
The accompanying graph (4b, bottom) shows the predicability — with 0.04dB between measured and theoretical — of the filter circuit when clocked at 25MHz.
RPECVD removes the substrate and one of the reactant gases from the plasma zone
Regulars
Applications
Amplifier for microphones and moving-coil pickups
Low-noise (formula provided), low-distortion microphone preamplifier enables phantom powering of the microphone and allows the use of a 20dB pad at the input.
The circuit at the amplifier output is an automatic DC restorer to remove DC offsets from the output, with an LF cut-off of 1.6Hz.
Gain of the INA103 is present by the connection of internal resistors to provide gain of 1 or 100, but the potentiometer (right) shows how external resistors can be used to obtain other settings; gain is determined from RG = 6/ (G-1) k, G being the gain.
Feedback resistors of 3kΩ are included in the chip but, again, external resistors can be used for other gain settings; care must be exercised here, though, to avoid instability, increased noise and reduced bandwidth.
Output voltage offset is trimmable in the usual way by a pot between null pins, although this method does have the slight disadvantage that input offset is affected: 1mV of output offset change alters input offset by about 1µV, also altering output offset drift by 3µV/°C per 1mV.
The application note provides details of other offset adjustment circuits.
Burr-Brown International Ltd, 1 Millfield House, Woodshots Meadow Watford, Hertfordshire WD1 8YX.
Telephone 923 33837.
Low-power frequency synthesizer
A cmos high-performance frequency synthesizer, the UMF1009T from Philips is intended for use in channelled VHF/UHF equipment such as portable and mobile radio and is programmable via the standard two-line serial I2C bus.
The main divider consists of a 7-bit binary counter and two rate selectors to control the external programmable prescaler and the 7-bit internal programmable divider (n2/n2+1 ).
The total division ratio from the DIV input to the input to the phase detector (Fig.2) is N = (128. n2 + nl) *A + n0 , where n0, n1 and n2 are less than 128.
If this ratio is expressed  as a binary number and A is 128, the n0 selector contains least significant bits, n1 the next seven bits and n2 the MSBs.
The output of this divider is taken to the phase detectors.
Also driving the phase detectors is the reference oscillator chain, which consists of a crystal oscillator with an external crystal resonating at up to 16.8MHz and a reference divider in two sections.
The first section is programmed to give an output at 1.2MHz and the second to produce 12.5kHz or 15kHz output to the phase detectors, a wide choice of frequency input thereby being possible.
Two phase detectors, analogue and digital, work together to produce good noise performance and fast locking.
When lock is achieved, the digital detector is switched out of circuit to avoid noise denigration in the analogue circuit.
Philips Semiconductors Mullard House Torrington Place London WC1E 7HD 071 580 6633.
Power mos electronic ignition provides an alternative to contact breakers.
As is pointed in International Rectifier's note AN-969m power mosfets possess advantages over bipolar transistors for ignition circuits, chiefly because of the high voltages then must withstand when used in place of a conventional mechanical contact breaker.
Power mosfets exhibit no secondary breakdown and can be avalanched to clamp excessive overvoltage due to leakage inductance or a disconnected HT lead.
Also, they need no base drive, being voltage controlled.
The note explains that the series resistance of the mosfet need not lead to inefficiency and a circuit performing at higher efficiency than the one using bipolars is feasible.
Modern ignition coils are of low inductance with ballast resistors for improved cold starting; inductive discharge circuits, as opposed to the older capacitive discharge types, require the discharge of typically 6A, a bipolar transistor needing safe-operating-area clamps, which increase both cost and dissipation.
Mosfets do not suffer from this limitation.
Figure 1 shows a practical ignition circuit with a built-in test oscillator R (formula provided).
Mosfet Q6 avalanches repetitively and absorbs the energy stored in the leakage inductance of the coil.
The photograph shows the waveforms of HT voltage (upper) at 5kV/div and drain/source voltage across Q6 at 100V/div.
Battery voltage is 4.5V and HT terminal unterminated.
The photograph shows conditions equivalent to an engine speed of 6000rpm for a n eight-cylinder engine.
At idling speed, increased dwell angle would increase aiming voltage to around 16kV.
International Rectifier, Hurst Green, Oxted, Surrey RH8 9BB.
Telephone 0883 713215.
Use of the INA103: this is a microphone preamplifier with the microphone phantom powered and a DC-restoration circuit at the output.
Circuit to adjust offset with no effect on input offset or input offset drift.
INA103 block diagram
Fig. 1.
Application of the UMF1009T.
The arrangement of counters allows a range of oscillator frequencies to be adopted.
Fig. 2.
Internal block diagram of Philips's UMF1009T low-power frequency synthesizer for mobile and portable VHF/UHF use.
All division ratios are programmable via a standard 12C bus
Fig. 1.
circuit diagram of electronic ignition circuit using power mosfet and low-inductance coil.
The IC is a 556 (dual 555) operating as a charge pump to provide gate drive voltage for the IRF740 power switching transistor.
The oscilloscope picture shows the plug voltage and the voltage across the mosfet switch.
The battery voltage is just 4.5V and the plug unterminated.
The spark duration is much longer than the 150µs indicated by this trace.
Applying multipliers
Accuracy normally found only in more expensive hybrid or modular designs is provided by the Analog Devices AD534 IC four-quadrant multiplier.
Internal trimming allows a claim of better than +-0.25% multiplication error with no external trim and AD says it is the first type to offer fully differential, high input-impedance working on all inputs, including the Z input.
A maximum gain of 100 and noise down to 90µV eliminate the need for a following amplifier in many cases, the gain being available in all operating modes.
There is a differential Z input and all the standard multiplication, division, squaring and square-rooting functions are possible, with no input/output polarity limitations, current output being allowed, if required.
The device uses the Gilbert technique, shown in Fig. 1, in which X, Y and Z inputs are converted to currents, trimmed for zero offset, the products of X and Y currents being multiplied using Gilbert's translinear technique which is described in the Analog Devices Non-linear Circuits Handbook.
A buried zener provides a 10V reference, so that the output voltage is XY/10-Z.
Scale factor is adjustable by an external resistor connected to SF and — Vs.
For signals applied to the Y input, with X at 10V, non-linearity is +-0.005% in full scale, although a different input arrangement with X variable leads to reduced non-linearity claims.
Figure 2 shows connection for multiplier, where the output is (formula provided).
When minimum AC feedthrough is needed in, for example, a suppressed-carrier modulator, a +-30MV trim voltage can be applied to the X or Y input.
Input Z can be used as a summing input.
When used as a squarer, X and Y are in parallel, the differential inputs giving polarity information (negative output if either of the inputs is reversed).
For low inputs, a feedback attenuator compensates for the reduced output; the difference-of-squares circuit in Fig. 3, for example, used the attenuator to cancel the factor of 2 loss in gain caused by the sum term generation.
Figure 4, an RMS-to-DC converter, also used the difference-of-squares function.
The filter is an integrator, its input being zeroed by the feedback loop when (formula provided).
As a divider (Fig.5), the Z input becomes the numerator, the denominator being at X. Gain is, again, adjustable by the use of an attenuator between output and Y2, a technique that is used to advantage in the percentage computer in Fig. 6, which gives the percentage deviation of A against B on a scale of 1 V per 1%.
AD's 1990/91 Linear Products Data Book gives more applications for the device, as well as general information on the use of this and other products.
Analog Devices Ltd, Station Avenue, Walton-on-Thames, Surrey KT12 1PF.
Telephone 932 253320.
Fig. 1.
Functions of the AD534, a four-quadrant IC multiplier with an accuracy associated with hybrid devices.
The device uses the Gilbert technique, shown in Fig. 1, in which X, Y and Z inputs are converted to currents, trimmed for zero offset, the products of X and T currents.
Fig. 2.
Connecting the AD534 as a unity-scaled multiplier.
Fig. 3.
Difference-of-squares circuit uses feedback attenuation to compensate for lost output
Fig. 4.
Wide-band RMS-to-DC converter, using difference-of-squares technique
Fig. 5.
AD534 used as a divider, providing differential operation on both numerator and denominator
Fig. 6.
Percentage computer to give percentage variation of one input against the other.
Hypothesis
Power lines, cancer and cyclotron resonance
Living close to overhead power lines may increase the risk of cancer in humans.
Harold Aspden hypothesises on the cause…and the cure.
But PowerGen and National Power won't like it.
Living close to overhead electric power lines causes health hazards.
At least this appears to be the conclusion of epidemiological studies about weak electro-magnetic exposure and cancer.
This might be attributed to cyclotron resonance induced in human body cells by weak electromagnetic fields.
This effect can be eliminated, but only if we cease to use our 50Hz or 60Hz frequencies and rely on DC exclusively or, alternatively, double the operating frequency to 100Hz or 120Hz.
A possible link between cancer and electromagnetic radiation from power lines and appliances has now been accepted by the US Environmental Protection Agency.
I suggest that the effect can be traced, experimentally, to a resonance effect known in physics as cyclotron resonance.
In a cyclotron, a modest electric field can accelerate a moving ion into enormous energy levels by a gradual resonant build up.
This is caused by energy from a weak field pulsating in harmony with the resonant frequency of ion motion in a steady magnetic field.
In the case of the human body, that steady field is the Earth's magnetic field.
The ions in motion are those chemical ions in body fluids and present within body cells.
The activating field producing resonance is the electric and magnetic field set up by an overhead power line or magnetic field producing appliance such as an electric blanket, etc.
On this matter, some point out that, assuming electromagnetic radiation does have an effect, it can only proliferate cancers already present.
This view is not convincing given that we all have a latent capacity to develop cancer cells through oncogene gene sectors; exposure to fields might reasonably be supposed to accelerate transmutations.
Hazardous frequency range
Cyclotron resonance by my theory only occurs if the hazardous frequency range lies with the exciting field.
Unfortunately, the 50Hz or 60Hz frequencies, relating to the two-pole magnetising rotor fields on synchronous alternators (running at a speed of 3,000 or 3,600 rpm), happen to be close to the frequency at which ions in bodies respond to, while in 50µT of the geomagnetic field.
The cyclotron formula requires that the angular frequency of the resonance should be simply Hq/m, where H is the strength of the geomagnetic field and q/m is the charge to mass ratio of the ion.
For a unit atomic  mass q/m is 96 million coulombs per kg.
Therefore, if H is 50µT, an ion of unit atomic mass and the normal unit charge will have a cyclotron angular frequency of about 4800 rad/s or about 760Hz.
In a separate publication1, I feel that I have shown that the inter-reaction processes involved when two dominant forms of ion are present can cause one ion form to screen the geomagnetic field effective on the other form to bring about a resonance at a frequency of up to 1.5 times higher that that for a single ion.
In effect, this occurs by what we can regard as a self-timing of the system.
This condition is limited by the higher frequency listed.
In other words, over a frequency range above that of natural resonance of the single ion form, there will be perfect resonance of one of the ion forms even though the external frequency is not exactly the frequency required for that form.
The data in Table I (drawn from work published by B R McLeod and Abraham Liboff) assumes that H is exclusively the geomagnetic field strength.
Doubling the frequency saves lives
I am aware of the extensive experimental research which has focused on ion forms involving Ca++ Na+ etc2, but what seems to have been missed in this prior research is the appreciation that water forms its own ion form in that it dissociates into hydroxyl and hydronium ions as listed on the table.
It would seem that the prevalence of such ions and their rather special relationship to the 50Hz and 60Hz frequencies, both of which they encompass by their combined effect, and as illustrated in Table I, has to give the underlying basis for field induced activity in body fluids.
However, this is not a critical point.
What is critical is the need to realize that, if one accepts that AC fields are dangerous but inescapable, then one should choose a power line frequency which won't set our ions jangling.
For instance, at a power line frequency between 100 and 120Hz, only lithium of atomic mass 7, if it were present in solution as a free single-atom ion, could come near to satisfying the resonance requirement.
In this situation, bearing in mind that the very prevalent hydroxyl and hydronium ions are probably the dominant cause in the hazard risk at 50Hz or 60Hz, it can be presumed that a hazard free situation prevails at 100Hz or 120Hz.
Therefore, as a simple expedient, the object should be to eliminate the 50Hz or 60Hz basic frequency and operate instead at twice this frequency.
Now, this may not seem to be a feasible solution, except for the odd application such as electric blanket or underfloor heating where full wave rectified unsmoothed 50 or 60Hz AC will produce heating from a mixture of 81% DC and 19% AC at 100 or 120Hz and above.
In the case of the overhead power lines, unless we think in terms of screening power lines, which seem uneconomic, or using normal closely wound cable forms, as used underground, the most appropriate, but drastic, remedy is to adopt a new standard frequency for power generation and supply, such as 100Hz.
Electric blankets or underfloor heating can be remedied by using DC power excitation.
However, the solution to be applied to overhead power lines, short of conversion to underground cable or high voltage DC transmission, is not so easy considering the very substantial voltages and currents involved and the heavy capital investment in existing structures.
The question is not so much that of understanding the cause, but rather one of eliminating the possibility of cyclotron ion resonance brought about by AC field in a domestic or normal commercial environment where the only prevalent DC magnetic field is that provided by the Earth itself.
Given that weak electromagnetic fields at the power frequencies 50Hz and 60Hz do pollute the environment in the sense that they make it hostile for the chemistry of body functions, there is no real choice other than making sure we live well removed from power lines or accept the higher incidence of cancer and leukaemia as tolerable.
We can only hope that the future will see technological advance: perhaps overhead power lines will carry DC in preference to AC to the extent that converters are used to generate AC domestically or industrially.
Ideally conversion could be to 100Hz to reduce risk.
Dr Harold Aspden works at the Department of Electrical Engineering, University of Southampton.
References
1.
H Aspden.
Cyclotron resonance in human body cells Sabberton, Southampton, p8 — 15, 1990.
2.
B R McLeod, A Liboff Dynamic characteristics of membrane ions in multifield configurations of low frequency electro-magnetic radiation Bioelectromagnetics 7: 177 — 189 (1986)
The Human Cyclotron
The cyclotron used in particle physics to speed charged particles up to very high energy levels combines the action of a powerful magnetic field and a high frequency pulsating electric field.
However, the weaker the magnetic field, the lower the frequency needed for a given ion mass.
The weaker the AC electric field, the longer it takes to build up the power of those ions, but the build-up is unavoidable if the particle mass, the magnetic field strength and the AC signal frequency satisfy the formula for resonance.
In a cyclotron the energy of the particles is deployed into an orderly spiral motion around the axis of the magnetic field and the pumping action of that weak pulsating electric field progressively adds to that energy and causes a pressure build-up at confining surfaces as the particles are driven in spirals of larger radius.
A cyclotron is fine-tuned to a resonant frequency specific to one chosen ion type.
In the blood stream, as in any ionised plasma, gaseous or liquid, there is a mix of positive and negative ions.
Cyclotron action implies that one polarity would spiral clockwise as the other spirals anticlockwise.
Collisions don't preclude the build-up of the resonance effects as the health hazard reports imply.
Indeed there is a case for saying that it is the mix of ions of opposite polarities that makes the resonance formula adaptable and actually causes a self-tuning resonance.
This explains why resonance of the ions typically in water or blood can occur either at 50Hz or 60Hz.
Can body cells be the seat of a human cyclotron effect?
The cyclotron needs a magnetic field and a transverse AC electric field to pump energy into ions in motion in that field.
There are ions activated by thermal motion in blood and these are always subject to the Earth's magnetic field.
Proximity to a weak 50Hz or 60Hz electric field therefore means that ions in a certain mass range can respond to cyclotron action.
The ions in water and blood happen to be in the critical mass range.
Table 1 shows the resonant cyclotron frequencies of single ions of different forms when subject to the steady geomagnetic field.
The lower frequency expressed in Hz is that of a single ion type.
The presence of other ions may raise the resonant frequency by up to 50%.
Table 2 shows how the power supplied to a purely resistive load is, by conventional electrical engineering analysis, divided between the lower frequency components of an unsmoothed full-wave rectified sinusoidal 50Hz voltage source.
submarine cables
Waves, wind and the teredo worm
At a time when plans for global communications seem to rest on the semantics of international standards, Greg Grant looks back to the Victorian adventurers who conquered nature to put a communication girdle around the world.
Integrated-systems digital-networks, open systems interconnections and 1992 are popular subjects of conversation at the moment.
The consensus is that such technologies will require international standardisation, long-term planning and organisational coordination.
But it has obviously been forgotten that a near-similar system was in place almost exactly a century ago — the submarine cable system, a communicative technique in which the British were supreme.
It too was digital in its way, planned with the varied organisations involved coordinating well.
Impetus behind the endeavour was the fact that ‘…use of the telegraph increased in significance, the greater the distance that had to be covered’ 1.
With the British having more distance to cover at that time than any other nation, including the Americans: ‘Cable enterprises supplanted the railways as popular investments’ 2.
First toe in the water
Britain became involved with cable early on, the first worthwhile trial being that ‘…carried out in 1838 by Brooke, an Englishman, across the river Hoohley in India’ .3 Even as early as 1840 a House of Commons committee had looked into the possibility of connecting Dover with Calais.
First attempt to link the opposite shores of a considerable stretch of water was the link between Dover and Cap Griz Nez in 1850.
It was the work of John and Jacob Brett their cable being ‘…made up merely of wire insulated by gutta percha’ 4, the whole effort being undertaken at their own expense.
The initiative failed shortly afterwards however due — it was said — to a French fisherman cutting the cable.
In the following year another cable was laid, consisting of four copper wires 1.7mm in diameter, insulated with gutta percha and tarred hemp.
Strengthening was by an armoured shield of 7.6mm iron wire and it established the standard for undersea cable design for years to come.
But two major problems were encountered; the cable's weight and the fact that it fell short of its intended destination.
However after successful joining of an additional length, the task was completed on the November 13, 1851.
It was the first truly successful undersea cable, continuing in business for the next 24 years.
The very considerable size of this achievement can be appreciated by looking at the hydrostatic pressure undersea cables may have to endure — up to 4t/in2, at a depth of three miles.
Moreover, the cable will move with the current, more violently in shallow water and close to a shoreline.
In tropical oceans, there is also the problem of the teredo worm, one of the lamellibranch family of boring molluscs, whose destructive vigour proved as deadly to submarine cables as it had previously done to ships' timbers, wharves and sea dykes.
Atlantic crossing
In May 1853 another narrow waterway was conquered by cable; that between mainland Britain and Ireland.
Earlier attempts had been made to bridge St George's Channel but had failed, largely because of the treacherous currents and deeper water.
The Magnetic Company under chief engineer Charles Bright laid a six-wire cable between Donaghadee in Ireland and Portpatrick on the Stranraer peninsula in south-west Scotland.
The operation had been far from easy, the cable being ‘…manhandled out of the hold of a steamer, over a pulley and round a drum which measured the speed, and then several times round a brake drum before passing into the sea’ .5
Elsewhere too, river estuaries and narrow seas were interconnected by cable.
It was in the course of one such exercise the British engineer Frederick Gisborne decided to span the Atlantic Ocean by submarine cable.
While seeking more financial support for the final leg of another project — that of connecting New York and Newfoundland by cable — Gisborne held discussions with American entrepreneur Cyrus Field, in the course of which the Atlantic cable was mentioned.
Field in turn, immediately sought advice on the feasibility of the concept from a US Naval hydrographer and oceanographer, Lieutenant Matthew Maury.
Maury had just completed Physical Geography of the Sea , the first textbook on oceanography published in 1855.
In it, his recently-prepared chart of the Atlantic Ocean showed that this huge stretch of water was shallower in the centre than at the edges.
He assured Field that it was possible to lay a cable on the ocean floor.
For the next 12 years, Field relentlessly pursued his project, winning support of distinguished scientists and electrical engineers such as John Brett, Charles Bright and William Thomson (later the first Lord Kelvin of Largs).
Probably the greatest scientific panjandrum of the 19th century, Thomson had a long association with transoceanic cables.
Indeed the basic telegraphic equations  were ‘…in their simplest form…first given by Lord Kelvin in 1855’ .6 Another of Kelvin's achievements was development of the concept of inductance, the practical importance of which was first brought to the attention of engineers by ‘…the problems  experienced in…the Atlantic cables’.7
Another of Thomson's originations was the siphon recorder, a device developed especially for telegraphic working over an under sea cable.
Atlantic Telegraph Company
Perhaps it was hardly surprising that the British had a near-monopoly on both cable technology and signal equipment techniques.
But dominion in the Atlantic cable ventures came about not through technological pre-eminence but via another British near-monopoly at that time: capital.
Having failed to raise sufficient funds for such an undertaking in America, Field came to Britain and, supported by Samuel Morse, their target of £350,000 was passed in two weeks; the Atlantic Telegraph Company was registered on the October 20, 1856.
Initially views differed on the technology to be employed and the cable-laying techniques.
This was to be expected given the experience and personalities involved such as Morse, Thomson, Field, Bright and Brunel.
Brunel of course was deeply involved in the completion stage of the massive ‘Great Eastern’— the largest ship the world had so far seen, and could not devote quite as much time to the project as the others.
Thompson on the other hand was keen to await the Great Eastern's launch, for he envisioned it as the ideal cable-layer.
Furthermore, he had reservations about the quality of the cable and he was not alone in this.
Bright too had misgivings in this area, particularly about size of the single conductor, and asked for it to be enlarged.
He also sought to have the cable spliced in mid-Atlantic with the freighting vessels sailing for America and Britain simultaneously.
Bright was turned down on both counts by Field, whose principal consideration was the weather.
Field already had two vessels on loan from the American and British navies and wanted cable-laying to be carried out within what we today would term the summer weather window.
As a result HMS Agamemnon and USS Niagara took their cable on board and began laying in early August 1857.
But the first break happened after only 8km and the second after nearly 612km.
So the project was called off.
The cause was mainly the cable's specific gravity, which resulted in it sinking too fast, overstretching and over-stressing the remainder.
Nor did its low tensile strength help either.
A second attempt was also a failure, this time after 257km.
But despite bitter disagreements Field convinced the company that they should try again.
This time they took up Bright's original suggestion and spliced the cable in mid-ocean on July 29 1858.
By August 5 the cable had been laid, and Thomson had sent the first two messages over it.
Technical hitches
The circuit however ‘…lasted only about a month.
The fault was in part with construction of the cable and in part with the high voltages…forced on it’ .8
Electrical and communications technology at this time was such that induction coils were thought to allow faster working than batteries.
Operators used high-voltage induction coils, with the result that the insulation deteriorated, probably causing the cable's failure.
Nevertheless, some 730 ‘cables’ had been transmitted over the line and so Field and his colleagues had proved that a transoceanic cable was not only feasible, but could also take a great deal of traffic.
In the following year another problem arose for the British; the Red Sea cable to India had failed.
The Board of Trade promptly set up an enquiry into the industry and its methods and techniques.
In 1861 it reported, having unearthed some disturbing facts.
Out of 18,000km of cable a mere 5000km — less than one third — were in working order.
As to the recent faults the committee concluded that they were due to careless handling of the cable during laying, to poor design and to bad practices in overall manufacture.
But a great deal of knowledge was gleaned concerning impurity effects on the conductivity of copper; the myriad problems of laying an undersea cable, and the terse frustrations of manufacturing one.
A new Atlantic cable was designed (Fig. 2.) consisting of seven wires of high-quality copper, insulated with four coats of gutta percha, the whole surrounded with hemp and armoured with ten iron bar-wires.
The Great Eastern was again chosen as the laying vessel, work beginning in July 1865, and the attempt got within 966km of the Newfoundland coast before the cable sheared again.
Another attempt was made in the following year between July 13 and 27 and was completely successful.
The shore ends of the cable used solid armoured wires instead of the stranded type and the cable itself was gradually reduced in size between the shore and the deep sea section in three stages — a practice still followed today.
The mighty Great Eastern not only laid a new cable; it succeeded in hauling up the earlier one splicing on a centre-piece, and lowering it again to the sea-bed.
In every way, the submarine cable was proving to be ‘…one of the great technical adventures of the 19th century, something akin to exploration of space today’ .9
Three years later, the Siemens brothers extended cable communications even further by implementing an 11,000km telegraph line between London and Calcutta, a very considerable achievement, one which had been yet another adventure, the sort that provided — like the Atlantic cable saga — the densely-packed reports and darkly realistic illustration for which the Illustrated London News had become justifiably renowned.
The cable has left Britain at Lowestoft, a North Sea cable bound for Germany.
Having crossed the Siemens' Brothers homeland, it  then clipped through a southern corner of Russia heading for Tehran, where it had its only interconnection before continuing on to the Indian sub-continent.
In the course of construction engineers and technicians survived fearful seas; had been protected by Russian soldiers; used, along with their equipment, as a form of target practice by turbulent natives and — on an individual level — had fallen victim to a positively bewildering variety of exotic diseases.
The line itself was very much state-of-the-art for the time, driven by punched paper tape and the section between London and Tehran had ten Wheatstone automatic repeaters installed as standard.
Clear signals
In the early days of cable, manual signalling was employed, using what are known as cable code signalling (Fig. 3.).
Transmission was by reverse-battery polarity, a simple yet efficient way of ensuring that dots and dashes used exactly the same voltage.
With no traffic, both stations would put their send/receive switches to receive, so as to be on ‘stand-by’ to receive any incoming signals.
Signal capacitors would be at earth potential.
On transmitting, at station A say, the send/receive switch was depressed to send, so earthing the signalling capacitor via the dot-dash keys.
Operation of the dot key earthed the battery positive terminal, putting a negative potential on the signalling capacitor.
The reverse occurred on restoration of the dot key and depression of the dash key.
Cable code signals — three-condition signals — were sent to line via the siphon recorder and its shunts SS1 and RSI giving pulse working to the distant terminal where the received signals passed through the recorder coils, the high-resistance inductive shunt RS2 to earth, via SR2.
In 1870 however, the brilliant Sir Charles Wheatstone invented the first, effective automatic telegraph equipment.
The message to be sent was transposed — corresponding to the dot/dash Morse code of the message — into perforations on a paper tape (Fig. 4).
The tape was then passed through an automatic transmitter, turning individual perforations into dot/dash signals for the line and considerably improving speed of the telegraph.
The first Atlantic cable operated at a speed of around 3 words/min, equating to a bandwidth of about 1.5Hz/s.
Cable code was faster, operating at about 54 words/min, or 20BAud.
Wheatstone's invention was almost twice as fast at 100 words/min (see box).
But this was only one of three such communicative tracks.
The second cable again crossed Europe.
Its route took it to Constantinople prior to passing through the southern part of the sprawling Ottoman empire to the Persian Gulf there to revert to a submarine route again before continuing to Karachi.
Unfortunately its path was not so very efficient ‘…because of the murky inconsistencies of Turkish administration’ .10
Source of Alice Springs
In 1870, the Victorians created a third electric avenue to the Orient, another submarine cable routed via Gibraltar, Malta, Alexandria, Suez and Aden, terminating at Bombay.
It was the sort of route the British had long been anxious to create, an all-red one, passing through either British-owned, British-run or British-controlled territory.
Security of these communication pathways meant headaches for the British — some real but more frequently imagined.
Keeping them functioning, not to mention well-maintained, was a formidable task, as not all routes could traverse the world on purely British turf.
The East and West African cables for example had to cross Portuguese territory, as did the South America cable which used the island of Madeira as a way-station.
No less tortuous was the route chosen for the Australian cable, crossing the island of Java, at that time a Dutch possession.
The British insisted that it be worked by British operators — reluctantly agreed to — and it finally entered Australia in the Northern Territories.
3220km and 36000 telegraph poles later it terminated at Adelaide.
The communications link had taken two years to pioneer, in situ long before a road or railway.
Its central way-station was located at one of the most remote places in the British empire; Alice Springs.
Indeed the telegraph was the sole reason for the existence of this huddle of buildings named after the wife of the cable station's chief engineer.
By 1894 British cable connections were branching out again, this time from Singapore to Hong Kong via the British possession of Labuan.
But the Hong Kong to Shanghai leg was another matter.
The last thing Britain was prepared to entertain was insecurity of relay stations on the Chinese mainland.
So an elderly hulk was stationed in the Min river estuary as an interconnection at once secure and free from prying.
Myriad troubles from which cable systems could suffer included ‘…silt, uncharted currents…winds — during the monsoon no Indian Ocean cable could be mended at all.
Even the webs of the more portly tropical spiders could interrupt an imperial dispatch’ .11
Electronic and electrical knowledge was advancing apace however, eg the discovery that signal attenuation was proportional to square root of the signal frequency.
Thanks also to the mathematical work of Oliver Heaviside, transmission techniques were steadily improved.
He realised that the ratio R/L = G/C give the necessary condition for distortionless transmission, where R = resistance; L = inductance; C = capacitance and G = leakage (conductance).
From this ratio, Heaviside established that attenuation constant a =  So distortionless conditions could only be attained with a very large increase in either the leakance or the inductance.
Increasing leakance had the disadvantage of increasing attenuation.
Increased inductance was usually adopted, using a technique known as ‘loading’, involving wrapping ‘a continuous nickel-iron wire…around the copper conductor…to neutralise the harmful effect of copper capacitance’ .12
This work led to an equation that determined the maximum frequency of signalling:
This work led to an equation that determined the maximum frequency of signalling:
Looking back to the ‘global village’
The reality of world-wide communications was indicated by Queen Victoria's Diamond Jubilee celebrations.
On June 22 1897, the Queen, in the telegraph room at Buckingham Palace, pressed the button activating the Central Telegraph Office's equipment in St. Martin's Le Grand.
It sent her jubilee greeting throughout the empire in a powerful demonstration that the global village was a possibility, some 14 years before the birth of its future prophet Marshall McLuhan.
It is doubtful if anything remotely striking will be forthcoming as regards either ISDN or OSI in the two years remaining before dawning of the brave new communicative continent in 1992.
A quick glance at the seven-layer decision-making ladder of the OSI, makes plain the potential for querulous technical in-fighting over a near-interminable period.
Where is the adventure in grappling with RS232, network layers and Hayes compatibilities, however necessary?
That sort of activity is light years removed from the drama of the Russian steppe, the pioneering excitement of the outback or the storm-lashed sense of achievement that taming the Atlantic brought.
But this is what it comes down to.
Any high drama that remains is found deep in technical working party country.
Yet there is still hope.
Some half dozen new submarine cables have been laid annually since 1945, the longest of which runs for 15,032km.
The Commonwealth Pacific cable, Compac, runs from Alberni in Canada to Sydney, Australia, interconnecting at Hawaii, Fiji and Norfolk Island, almost mimicking the cherished all-red routes of the Victorians.
The UK remains a major influence in the communications business and is the undoubted European leader in electronics information systems with 15% of world markets, second only to the US.
At the same time Russia and Eastern Europe are discarding the shackles of the last 40-odd years.
If that market open up, it could require nothing less that a Sovpac.
Now that would be an adventure.
References
1.
Science &: Western Domination , Kurt Mendelssohn.
Thames &Hudson, London.
1976, p. 176.
2.
A Social History of Engineering .
WHG.
Armytage.
Faber &Faber, London.
1976, p. 36.
3.
A Century of Technology 1851–1951 .
Ed Percy Dunsheath.
Hutchinson's Scientific &Technical Publications, London.
1951, p. 272.
4.
Ibid , p. 273.
5.
Pioneers.
Dr WA Atherton,EW + WW , August 1989, p. 811.
6.
Telegraphy &Telephony.
E Mallett.
Chapman &Hall Ltd.
London.
1941.
Page 66.
7.
Syntony &Spark —The Origins of Radio .
HGJ Aitkin.
John Wiley &Sons, New York.
1976.
Page 87.
8.
From Falling Bodies to Radio Waves .
Emilio Segre.
WH Freeman &Co, New York, 1984, p. 209.
9.
From Compass to Computer .
WA Atherton.
McMillan Publishers Ltd, Basingstoke, 1985, p. 92.
10.
Pax Britannica: The Climax of an Empire .
James Morris.
Faber &Faber, London, 1968, p. 62. 11.
Ibid , p. 62.
12.
Telecommunications Principles .
RN Renton.
Sir Isaac Pitman &Sons Ltd, London, 1950, p. 134.
Undersea technology — by gum!
In 1842 gutta percha was introduced into Great Britain — the coagulated latex of the gutta and other trees such as the bassia pallida found mainly in Malaysia and the East Indies.
Its ability to repel water along with its excellent insulating properties were immediately noted by leading engineers and scientists of the day such as Faraday, Siemens and Wheatstone.
Having a resistivity of some 10MΩ and a permittivity of 3, it would remain the first choice of insulation for undersea cables until development of para gutta, composed of wax and deproteinised rubber.
This too would retain its pre-eminent position as an insulator until replaced around 1933 by Polyethylene developed by ICI.
Thomson's siphon recorder
An operation, a moving coil D'Arsonval galvanometer connected to the line was arranged so that coil deflections, brought about by an incoming signal, controlled movement of a writing head across a moving paper tape.
The tape was driven by either an electric or clockwork motor and the writing head was supplied with ink from a reservoir by siphon action, as the tape was being driven.
Writing head deflections were proportional to the received signal current, and so the graph produced indicated magnitude of the line current.
Dots and dashes could therefore be identified by the upward and downward movements of the current curve.
Wheatstone's 100 words/min
If a transmitter operates at 6m of perforated tape/min and there are ten centre holes per 2.54 cm of tape, this would amount to 40 centre holes per second or, put another way, 40 dots/s.
Since each dot is followed by a spacing period equal to the dot duration, the dot signal length — shortest element in the code — is 1/80th of a second and the telegraphic speed is 80 Baud.
Morse code is unequal in length so conversion of the above information into words per minute requires knowledge of the average number of unit elements, equal to a five-letter word plus one word space.
In plain English this is 51.6.
For telegraphic traffic however it is taken as 48.
Therefore a telegraph modulation rate of 80 Baud equates to 80/48 x 60; 100 words per minute.
The falling price of technology
Technical advances and the increasing demand for long distance communication meant but one thing; lower prices.
In 1866 an Atlantic telegram cost £20 or $100.
Just two years later, rates had come down to $1.57/word.
A decade later, submarine cable were carrying about 27 million words per annum, their most regular and demanding customer being the governments of the day.
In 1870, for instance, the British Colonial Office's telegraph bill was around £800.
By 1897, it was £8000 annually and rising.
In the following year there were 14 cables connecting North America to Europe, 12 of which were in operation.
A further 3 would be set down before the century's end.
One result was that the British were the world's major communications operators.
HMS Agamemnon laying the Atlantic cable in 1857…but disappointment was to follow.
Second attempt to lay an Atlantic cable also failed due to the cable stretching during the operation
Fig. 1.
The siphon recorder, adapted by Lord Kelvin from the Gauss/Weber mirror galvanometer.
Writing head S is attached to lightweight plate P, which is axially supported by wire W. Plate P is capable of angular rotation about axis W, controlled by the silk fibres x and y which are attached to opposing corners of the galvanometer coil.
Coil movements produce angular movements of P, causing horizontal movements of the writing head over paper tape t.
One end of W is rapidly but delicately electrically vibrated allowing the writing head to make equally rapid, if intermittent, contact with the paper.
It also reduced friction at the point of contact.
Fig. 2.
Improved Atlantic cable consisted of several copper wires, insulated with gutta percha and surrounded with hemp and armoured with ten iron bar wires.
Laying the first cable across the English channel in 1850.
The cable hold of the Great Eastern: Siemens brothers implemented an 11,000km telegraph line between London and Calcutta.
Fig. 3.
The earliest form of submarine cable telegraphy: manual simplex working using a cable code key.
C1 and C2 are signalling capacitors; K1 and K2 sending keys and S1 and S2 siphon recorders.
SR1 and SR2 are the send/receive switches connecting the transmitting or receiving equipment to the cable, while SS1 and SS2 are variable low resistance transmitting shunts.
RS1 and RS2 conversely, are high-resistance inductive receiving shunts, adjusted for optimum received current to operate the recorders.
The transmit shunts bypass the record coils when the send/receive switch is at transmit and are adjusted such that the current through the recorder at the transmitting station is equal to that present on reception.
The battery connections are reversed at one end of the circuit, so that each recorder can operate when the station is transmitting.
Fig. 4.
Wheatstone Morse perforated tape or slip and the corresponding Morse signals.
Audio
Who designed this?
‘a manufacturer cannot contact all potentially interested parties…
’ Panasonic UK Legal Department
In September 1982, this magazine published a brilliant piece of lateral thinking in audio design, a Wheatstone bridge type circuit which should rid transistor audio amplifiers of crossover distortion originating in the output transistors.
The designer, Aubrey Sandman, believes that a student working for Matsushita devised a circuit arrangement based on his own, later patented by the Japanese electronics giant.
Dr Sandman presents his evidence.
Technical literature is full of unpatented though useful information.
But it is essential that the originators receive due recognition.
The importance of this cannot be overstated if this flow of information is to continue.
In September 1982, I published an original article which suggested the use of a subsidiary power amplifier to make the load seen by a low power main voltage amplifier seem very much greater than it was in reality.
This arrangement greatly reduced both crossover and other distortions.
It also did away with the need for bias adjustment on the output devices — a most desirable quality.
I called it Class S .
Based on the simple concept of maintaining balance in a bridge, it managed this without being intrinsically sensitive to loudspeaker load impedance variations.
In other words it rendered, at a stroke, conventional Class B circuits obsolete for serious audio systems.
Indeed there is a strong case for using it generally, even among lower quality systems when the lack of setting up is taken into account.
In 1983, the international electronics company Technics, a subsidiary of Matsushita, patented a circuit which it called Class AA , a strikingly similar design to my Class S arrangement shown in Fig. 1.
The Technics arrangement is shown in Fig. 2 for comparison.
The Japanese patenting of the Technics Class AA circuit was quite unknown to me, not having associated their advertisements for Class AA with anything I may have done until I read John Linsley Hood's article in this journal for December 1989.
Since then I have been trying to gain recognition from Technics.
The following letter from Panasonic (UK), the Technics holding company, present Matsushita's position:
‘We can be certain that the designer of Class AA (an undergraduate student) did not read your article in Wireless World unless it was translated into Japanese; he cannot, read, write or speak English.
‘As far as the involvement of one of his tutors is concerned, this is purely surmise on his part, and was always expressed as such.
Basically he felt that as Class AA was his freshman project, any exposure to your article would have had to have been indirect via his teachers.
However, he has never stated that this was in fact the case.
‘I would also state my belief that in relation to Class AA, our parent company has behaved entirely properly; the process of applying for a patent necessarily entails wide publication and ample opportunity for opposition.
I willingly accept that an inventor cannot scrutinise every patent application; by the same token a manufacturer cannot contact all potentially interested parties.’
My view about the Matsushita response is simply that the Japanese higher education system generally requires knowledge of English, which seems at odds with the Technics explanation.
The basic variation in the Technics circuit  arrangement has necessitated a set-up procedure, not present in my circuit.
Furthermore, my own analysis of the Technics arrangement suggest that it is sensitive to loudspeaker impedance variations; under some conditions the performance is worse with the subsidiary amplifier than it would be without it.
At the same time, even when the circuit performs better at some loudspeaker impedances than the traditional Class B circuit, it still has worse performance than that of Class S because of the low, yet varying impedance seen by the main voltage amplifier for all loudspeaker impedances bar one.
Analysis
Both circuits attempt to tackle the problem of crossover distortion in a basically identical manner: they try to make the load impedance, RL, appear to be very much higher to the main amplifier than it really is.
Class S works on the clear principle that the inputs to A2 must both be driven to the same potential regardless of the input voltage to AL, ie, a virtual earth.
In other words if the non-inverting input is at +3V that will also be the value of the inverting input.
References
1.
Sandman, A.M.,Wireless World , September 1982, pp. 38–39.
2.
Linsley Hood, J.L. Electronics World , December 1989 pp 1164–1168.
Pictures from the past
Sounds of the century
Ever since the first distant sounds of music and odd snatches of alien speech began interrupting the 1920s' world of Morse, better (and louder) reproduction of audio has been at the forefront of development priorities for engineers.
It all became feasible with the thermionic valve, introduced during WW1, which rapidly and irrevocably supplanted all other methods of audio amplification, including the electromechanical types (one of which used compressed air controlled by an electromagnetic air valve).
Wireless World almost created civil war in the audio industry at about this time over its publication of one of the first RC-coupled amplifiers.
Transformer coupling was then unchallenged and the journal's championing of the RC technique caused rioting among the transformer makers (and there were many), with threats of withdrawal of advertising and worse.
A devious editorial calmed things down and peace reigned again.
The culprit is shown in the picture — the RC-coupled amplifier itself, published in the issue for October 10, 1923, with its complement of ‘R’ valves.
At about the same time, 1926 to be exact, Walter Cocking produced what became known as Everyman Four receiver, originally called Everyman's Four-valve, one of which is now in the Science Museum.
It appeared in WW for July 28 and August 4, 1926 and owed it considerable success and subsequent fame to the new technique of neutralising the RF stage triode and to the use of an RF coil design by Cocking.
The picture shows the construction standard of the day, with 90-degree bends in all wiring and metalwork and woodwork that should be in the Guggenheim Museum.
The circuit shows a neutralised triode RF, anode-bend detector, transformer-coupled gain and triode output producing around 150mW!
Going back to amplifiers, the third picture is of Cocking's Push-pull Quality Amplifier, published by WW in 1934.
There were two MHL4s driving a pair of PX4s in push-pull, the MHL4s deriving the double-ended drive from a concertina phase splitter.
It gave 4W output at a ‘low’ distortion.
Some are still around and, although they do not have the power, compare reasonably well in quality with modern wonders.
Moving on to speakers, there were many methods of moving a mass of air.
Moving-iron and reed types were produced in dozens of variants, in addition to the odd-ball kinds  like the compressed-air type, but the breakthrough came with the moving coil, which had been modernised from its Siemens 1874 patent and Lodge's 1898 one by Rice and Kellogg in 1924.
The fourth picture shows a magnificent affair made by Paul Voigt, which had twin diaphragms and 20kilogauss field magnets.
It was possibly the first time the BBC had had to take other equipment out of service because someone (Voigt) complained of its bad performance, which had shown up on this speaker.
If proof were needed of the new-found fascination with sound reproduction in the 1950s, the last picture should supply it.
It is of one of the unforgettable demonstrations by G.A.
Briggs of Wharfedale, who challenged his audience to distinguish between live performers and loudspeakers.
Thousands rallied to the call, as the picture shows, and many were sadder and wiser after the event.
Push-pull Quality Amplifier, again be Cocking.
It gave 4W at a lowish distortion and worked in ‘nearly Class A’.
Loudspeaker by P.G.A.H. Voigt, which sported a 20 kilogauss magnet and twin diaphragms.
It also brought a delicate blush to the BBC cheek.
First RC-coupled amplifier, constructional details of which were published by WW in 1923, brought murder to the hearts of transformer makers in the land.
Walter Cocking's Everyman Four TRF receiver, which set a standard of performance for the rest to try to emulate.
One is now in the Science Museum.
Everyman Four and (below) the original circuit diagram
Broadcast
Radio with vision
‘Arguably the most important development in radio since the invention of the transistor’.
This was the assessment of RDS by Johnny Beerling, controller of BBC Radio 1 and chairman of the EBU Programme Experts Group in an introduction to a special issue of the EBU Review — Technical devoted to radio data system (RDS).
In his world, the ‘Utopian world of RDS’, everyone is equipped with the new technology — in cars, at home and on foot.
When that day arrives, in the not-too-distant future, Beerling says listeners to the radio will find themselves ‘better-served by their equipment than at any time since the wireless was first invented by Marconi in 1895’.
Strong words, but certainly European broadcasters have shown remarkable unanimity in rapidly implementing at least some basic RDS facilities on their national and local services.
Yet RDS remains low on the list of consumer priorities.
Even when aware of its existence, more important to most is the question of security, making digital VHF car radios less vulnerable to theft.
Second generation impetus
Benefits of RDS as a source of traffic information have penetrated the consciousness of only the relative few making top-of-the-range car-radio purchases.
This may change later this year with the first ‘second-generation’ RDS set incorporating RDS-EON (enhanced other networks) function.
This feature can be used to update information stored in a receiver about programme services other than the one to which the set is tuned.
So a driver could listen, for example, to one of the BBC national networks, while the set automatically switches temporarily to a local station making a traffic announcement.
BBC has been testing EON on its local radio traffic experiment in south-east England and the Midlands for about two years, and several other European countries are already, or soon will be, implementing EON, although none of the 50 or so existing RDS models is suitable for this function.
BBC claims that it would not have decided to go ahead with EON on a country-wide basis unless it had received assurances that other firms will have suitable second generation RDS receivers on the market soon.
DoT's rosy future
In an IEE lecture: ‘Broadcasting traffic information’, Simon Shute (BBC) and Alistair Robertson (Dept of Transport) presented a generally rosy view of the future of RDS.
The DoT believes that traffic information broadcasting, and in particular RDS, will have an important role to play in reducing delays and driver frustrations.
Better network management has, as a prime objective, constant awareness of network conditions, providing accurate and timely information to permit the network capacity to be used to greater effect.
Similarly, Shute emphasises that traffic broadcasts must be timely, relevant, accurate and sufficient.
He believes that EON will much improve the service.
It was perhaps a coincidence that chairman of the IEE meeting was RS Sandell who in the 1970s, with SM Edwardson, developed the BBC's medium-wave Carfax system that was later aborted.
Carfax was, compared to RDS, a low-technology system based on a dedicated network of synchronised, time-multiplexed, low-power (0.5kW) transmitters to which the car radio (or player) was switched automatically when a local traffic announcement was made.
It was estimated that a lattice network of sone 80 transmitters would give nationwide coverage, with more than 80% of road traffic covered from only 20 stations.
For several years, the BBC enthusiastically promoted the low user costs of Carfax and at Warc 1979 even succeeded in obtaining a special footnote to ITU Radio Regulations permitting establishment in the UK of a ‘public utility information service’ between 519.5 to 526.5kHz, just below the lower-frequency edge of the broadcast band.
But the footnote did not apply to other European countries.
In effect it gave the UK all the frequencies needed for a nationwide Carfax service on a dedicated lattice.
Indeed, in 1978, the BBC concluded: ‘the only really satisfactory solution for the broadcasting of traffic information is to provide a separate, dedicated service’.
Official abandonment
Officially, Carfax was abandoned on the grounds that ‘frequencies were not available’.
In view of the Warc footnote, this statement seems to have been economical with the truth.
At the time, most observers believed that the real reason was that the BBC had been unable to persuade the newly-elected Conservative government to contribute towards the cost of setting up (about £3million at 1977 prices) or running a dedicated traffic service.
Despite the far greater sophistication (and cost to users) of RDS, Carfax appears to remain the superior approach, permitting local traffic announcements to be made as soon as information is available to broadcasters.
This is a significant advantage over RDS-EON which has to be slotted into programmes intended also for domestic and ‘walking’ listeners.
Such listeners would soon be annoyed were announcements to be made too frequently, at intervals of less than say 20–30 minutes, or if programmes were rudely interrupted.
Traffic message control
Traffic message control (TMC), under development as one of the European Drive projects, may bring RDS a little nearer to Carfax.
This is seen as using RDS as a data channel of very limited capacity in conjunction with a speech synthesis chip or possibly some form of printer in the car (it could provide automatic translation when travelling abroad) as a means of sending ‘traffic telegrams’ to cars without interrupting the radio programmes.
But TMC seems bound to add much extra intricacy to the already complicated RDS receiver.
D Kopitz (EBU) in discussing  development of TMC admits that: ‘The TMC device will probably be too complex to permit its incorporation within a standard car radio case.
It will also require it own RF tuner (VHF) to permit the car radio listener a free choice of listening entertainment, without obligation to tune to the particular programme service which carries TMC information.
So a complex — and still rather expensive — receiving device is needed, in addition to a car radio (which may, or may not, be equipped for conventional RDS decoding).
Presently-available RDS radios cannot be used for TMC, and cannot be adapted.
Ergonomic features and acceptability of TMC functions by the driver, have not yet been assessed.
After a standard for TMC has been agreed, perhaps in 1991 or 1992, it will take many years, perhaps five to ten, until RDS-TMC receivers first reach the market.
This means that conventional spoken broadcast traffic messages will have to be continued for many years.
Indeed, they will remain the major source of information for car drivers for the foreseeable future’.
Restrained consumer response
Although broadcasters may feel that RDS is user-friendly the consumer-industry is only gradually responding to the growing dislike of over-complex and often confusing controls; c.f. video recorders easily set to record the wrong programme on the wrong channel at the wrong time, and teletext units that remain under-used.
A typical consumer reaction is summed up by a correspondent writing earlier this year in the New Scientist concerning trends in digital car radio design: ‘Previously it was simple, while driving to cope with two big knobs and six decent-sized push buttons to get a useful selection of stations.
Now one is faced with several, barely identifiable minuscule buttons packed side by side or on top of one another’.
The simplicity and low user cost of Carfax with its medium-wave capabilities seem more attractive than ever.
Admittedly, RDS, unlike Carfax, was designed to have many functions for home as well as in-car listeners.
For example station identification — particularly useful in countries such as Holland where a large number of FM broadcasts from neighbouring countries can be received.
Programme related announcements, claimed as one of the attractions of RDS, even when scrolled require displays with more alphanumeric elements than are usually fitted at present.
Potentially attractive is pin (programme item number), enabling receivers and recorders to respond to particular programme items preselected by a user.
The code contains the scheduled time of the start of the programme, plus day of the month.
It's useful, but there are already available on the market audio recorders with conventional time switches.
Several countries, but not the UK have radio-paging systems based on RDS which provides a user-attractive function for car radios.
But all in all, it seems likely to be a long time before we reach that ‘Utopian world of RDS’ for all listeners in cars, at home or on foot!
Promising start
Some nine or ten firms are manufacturing the special RDS transmission equipment (VG electronics in the UK).
Rhode &Schwarz take space to congratulate EBU on ‘the great success achieved with RDS adding, not too disinterestedly, ‘the fact that our equipment is supporting RDS in more than a thousand FM radio stations worldwide makes us rather proud’.
The BBC alone has RDS going out on over 700 FM transmitters.
More than 20 receiver firms have RDS models in their ranges, mostly car radios, with about five firms offering domestic audio tuners with RDS, though still no portable sets.
This must seem a promising start for RDS for which first receivers appeared only in 1987.
Consumers wary of poor performance
Broadcasters have been none too happy with performance of some of the earlier sets.
Mark Saunders (BBC Radio) in a personal review of design requirements for RDS car radios points out that one of the problems that RDS poses for both broadcaster and receiver manufacturer is that, with the exception of obvious features such as the display of the programme service name, an RDS receiver is not apparently different in performance, or looks, when observed in the static environment of the dealer's showroom.
So from the point of view of the consumer, there are few tests that can be done prior to purchase, to ascertain how well a product will perform when fitted into the car.
He says that it is a sad fact that many early RDS receivers failed to perform even basic tasks adequately, and many people have been put off RDS for life because of: ‘experiences with receivers that performed inadequately and which failed to live up to the promises the broadcasters made for them’.
Mr Saunders goes on to admit that broadcasters are not entirely blameless in this respect: ‘More should no doubt have been done at an earlier stage in development of RDS to give the receiver manufacturing industry some guidelines setting out the minimum levels of RDS performance that should be achieved’.
His belief is that it is preferable if the RF tuner and RDS processing stages of the receiver are continuously active whenever the vehicle ignition is switched on.
The on/off switch on the front panel should only control the power amplifiers and display functions.
Advantage of powering the receiver all the time the vehicle is moving is that, even though the driver is not listening to the receiver, it is continuously being updated by RDS data, in the same way as if the driver were listening to the CD player.
If the vehicle has an electrically powered antenna, arrangements should be made to ensure that it is raised whenever the vehicle is moving — with an over-ride switch allowing the driver to avoid damage when entering a garage or car park.
Mark Saunders admits that car receivers should not rely exclusively on reception of the CT (clock) and date information but use the RDS to set and correct the displayed time.
Otherwise the clock would stop if the set were tuned to a transmitter not transmitting CT or if the set were switched off.
Furthermore it cannot be assumed that all RDS information will be received accurately at all times.
In areas of multipath, several minutes may elapse between successive occurrences of successful reception and decoding of the groups carrying the CT data.
Fig. 1.
Three different communications protocols for the broadcasting of spoken and coded traffic messages on RDS.
Fig. 2.
RDS interface scenarios in car radios.
Fig. 3.
Basic features of an integrated Carfax receiver for 1977 trials on 534kHz.
Carfax adaptors were designed for use with existing car radios.
Index to advertisers
Abracadabra Electronics
791
Adept
IFC
Adept
754
American Automation
787
Anchor Surplus
789
Antex Electronics
756
Audio Electronics
746
Audio Visual Research
770
Autona
797
A.O.R. (UK)
746
Billington Valves
799
Chelmer Valve Company
766
Citadel Products
OBC
Dewsbury
Electronics794
Digitask
714
Display
Electronics719
Halcyon
Electronics721
Hoka Electronics
737
Icom (UK)
763
Institute of
Electrical
Engineers
735
IPK
721
John's Radio
782
J. and N. Bull
Electrical751
J.B. Designs
794
Kestrel
Electronic
Components
756
Langrex Supplies
756
Lloyd Research
791
Matmos
800
Micro Amps
758
MQP Electronics
758
M. and B. Electrical
733
Number One Systems
723
Research Communications
721
R. Henson
763
Sherwood Data Systems
766
Signals Research
763
Smart Communications
770
Stewarts of Reading
737
Surrey Electronics
737
Talent Computers
787
Taylor Brothers
IBC
Those Engineers
746
Thurlby Thandar
758
Thurlby Thandar
766
Tsien (UK)
769
Wood and Douglas
776
